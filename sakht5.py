"""
Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø§Ù†Ø³ Ø·Ù„Ø§
Python 3.12 | pandas 2.3+ | numpy 2.1+ | tsfresh 0.21+
Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ NumPy 2.0 Ùˆ PyArrow
ÙˆØ±Ú˜Ù† 2.0: Ø´Ø§Ù…Ù„ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± ÙÛŒÚ†Ø±Ù‡Ø§ØŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒØŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ LightGBM Ùˆ hybrid extraction
"""

import numpy as np
import pandas as pd
import warnings
from typing import Optional, List, Dict, Tuple
from pathlib import Path
# Ø­Ø°Ù scipy Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„ multiprocessing

# Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ pandas
pd.options.mode.copy_on_write = True
# dtype_backend ØªÙ†Ù‡Ø§ Ø¯Ø± Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ pandas ÙØ¹Ø§Ù„ Ø§Ø³Øª
try:
    pd.options.mode.dtype_backend = 'pyarrow'
except Exception:
    pass  # Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…â€ŒØªØ±

# tsfresh
from tsfresh.feature_extraction import (
    extract_features,
    EfficientFCParameters,
    MinimalFCParameters,
    ComprehensiveFCParameters
)
from tsfresh.utilities.dataframe_functions import impute
from tsfresh.utilities.distribution import MultiprocessingDistributor

# LightGBM Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§
try:
    from lightgbm import LGBMClassifier
    HAS_LGBM = True
except ImportError:
    HAS_LGBM = False

warnings.filterwarnings('ignore')


class GoldFeatureExtractor:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø± Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø³ Ø·Ù„Ø§ - Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø§ NumPy 2.0+"""
    
    def __init__(self, n_jobs: int = -1, feature_set: str = 'efficient', 
                 use_meaningful_features: bool = True, use_hybrid: bool = False):
        """
        Parameters:
        -----------
        n_jobs : int
            ØªØ¹Ø¯Ø§Ø¯ CPU cores (-1 = Ù‡Ù…Ù‡)
        feature_set : str
            'minimal' (~20 ÙÛŒÚ†Ø±) | 'efficient' (~400-800) | 'comprehensive' (~1500+)
        use_meaningful_features : bool
            Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± (Parkinson, Drawdown, Sharpe, etc)
        use_hybrid : bool
            Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ±Ú©ÛŒØ¨ tsfresh + Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± + Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ LightGBM
        """
        self.n_jobs = n_jobs
        self.feature_set = feature_set
        self.use_meaningful = use_meaningful_features
        self.use_hybrid = use_hybrid
        self.extracted_features = None
        self.feature_names = []
        self.feature_importance = None
        self.cleaned_features = None
        
        print("=" * 70)
        print("ğŸ¥‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø± Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø³ Ø·Ù„Ø§ - Ù†Ø³Ø®Ù‡ 2.0")
        print("=" * 70)
        print(f"âœ“ Python 3.12")
        print(f"âœ“ NumPy {np.__version__}")
        print(f"âœ“ pandas {pd.__version__}")
        print(f"âœ“ Feature set: {self.feature_set}")
        print(f"âœ“ Meaningful features: {'âœ… Ø¨Ù„Ù‡' if self.use_meaningful else 'âŒ Ø®ÛŒØ±'}")
        print(f"âœ“ Hybrid mode: {'âœ… Ø¨Ù„Ù‡' if self.use_hybrid else 'âŒ Ø®ÛŒØ±'}")
        if HAS_LGBM:
            print(f"âœ“ LightGBM: âœ… Ø¯Ø³ØªÛŒØ§Ø¨")
        print("=" * 70)
    
    def load_gold_data(
        self,
        file_path: str,
        date_column: str = 'date',
        price_column: str = 'price',
        volume_column: Optional[str] = None,
        other_columns: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø³ Ø·Ù„Ø§
        
        Parameters:
        -----------
        file_path : str
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ (CSV ÛŒØ§ Parquet)
        date_column : str
            Ù†Ø§Ù… Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ®
        price_column : str
            Ù†Ø§Ù… Ø³ØªÙˆÙ† Ù‚ÛŒÙ…Øª
        volume_column : str, optional
            Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø­Ø¬Ù…
        other_columns : list, optional
            Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ (open, high, low, close)
        """
        print(f"\nğŸ“‚ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ: {file_path}")
        
        file_path = Path(file_path)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ PyArrow (Ø³Ø±ÛŒØ¹â€ŒØªØ±)
        if file_path.suffix == '.parquet':
            df = pd.read_parquet(file_path, engine='pyarrow', dtype_backend='pyarrow')
        else:
            df = pd.read_csv(
                file_path,
                parse_dates=[date_column],
                dtype_backend='pyarrow',
                engine='pyarrow'
            )
        
        # ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        if not pd.api.types.is_datetime64_any_dtype(df[date_column]):
            df[date_column] = pd.to_datetime(df[date_column])
        
        df = df.sort_values(date_column).reset_index(drop=True)
        
        memory_mb = df.memory_usage(deep=True).sum() / 1024**2
        print(f"âœ“ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§: {len(df):,} | Ø­Ø§ÙØ¸Ù‡: {memory_mb:.2f} MB")
        print(f"âœ“ {df[date_column].min()} ØªØ§ {df[date_column].max()}")
        
        return df
    
    def prepare_for_tsfresh(
        self,
        df: pd.DataFrame,
        time_column: str = 'date',
        value_columns: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ NumPy 2.0 - ØªÙ…Ø§Ù… Ø±Ø¯ÛŒÙÙ‡Ø§ ÛŒÚ© Ø³Ø±ÛŒ"""
        print(f"\nâš™ï¸  Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ...")
        
        df_prepared = df.copy()
        df_prepared['id'] = '1'  # âœ… ØªÙ…Ø§Ù… Ø±Ø¯ÛŒÙÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ ÙˆØ§Ø­Ø¯
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø²Ù…Ø§Ù† Ø¨Ø§ Ø¹Ù…Ù„ÛŒØ§Øª vectorized NumPy 2.0
        if pd.api.types.is_datetime64_any_dtype(df_prepared[time_column]):
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ numpy 2.0
            time_delta = (df_prepared[time_column] - df_prepared[time_column].min())
            df_prepared['time'] = time_delta.dt.total_seconds().astype(np.int64)
        else:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² np.arange Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
            df_prepared['time'] = np.arange(len(df_prepared), dtype=np.int64)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ value
        if value_columns is None:
            value_columns = [col for col in df_prepared.columns 
                           if col not in ['id', 'time', time_column]]
        
        df_prepared = df_prepared[['id', 'time'] + value_columns]
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ float32 (Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡)
        # NumPy 2.0 Ø¨Ø§ astype Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª
        for col in value_columns:
            df_prepared[col] = df_prepared[col].astype(np.float32, copy=False)
        
        print(f"âœ“ Ù†Ù‚Ø§Ø· Ø²Ù…Ø§Ù†ÛŒ: {len(df_prepared):,} (ÛŒÚ© Ø³Ø±ÛŒ ÙˆØ§Ø­Ø¯)")
        print(f"âœ“ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ value: {value_columns}")
        
        return df_prepared
    
    def extract_features(self, df: pd.DataFrame, disable_progressbar: bool = False):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ NumPy 2.0 - single-threaded"""
        print(f"\nâ³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ {self.feature_set}...")
        
        fc_params = {
            'minimal': MinimalFCParameters(),
            'efficient': EfficientFCParameters(),
            'comprehensive': ComprehensiveFCParameters()
        }[self.feature_set]
        
        # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² n_jobs=1 (Ø¨Ø¯ÙˆÙ† multiprocessing)
        self.extracted_features = extract_features(
            timeseries_container=df,
            column_id='id',
            column_sort='time',
            default_fc_parameters=fc_params,
            n_jobs=1,  # âœ… single-threaded (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„ multiprocessing)
            disable_progressbar=disable_progressbar,
            show_warnings=False
        )
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¨Ø§ Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ù‡ÛŒÙ†Ù‡ NumPy 2.0
        impute(self.extracted_features)
        
        # NumPy 2.0: replace Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
        self.extracted_features.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.extracted_features.fillna(0.0, inplace=True)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ float32 (NumPy 2.0 Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª)
        self.extracted_features = self.extracted_features.astype(np.float32, copy=False)
        
        self.feature_names = list(self.extracted_features.columns)
        memory_mb = self.extracted_features.memory_usage(deep=True).sum() / 1024**2
        
        print(f"âœ“ ÙÛŒÚ†Ø±Ù‡Ø§: {len(self.feature_names):,} | Ø­Ø§ÙØ¸Ù‡: {memory_mb:.2f} MB")
        
        return self

    def extract_features_from_sliding_windows(
        self,
        df_prepared: pd.DataFrame,
        window_size: int = 50,
        step: int = 1,
        disable_progressbar: bool = False
    ) -> pd.DataFrame:
        """Extract tsfresh features for sliding windows.

        Parameters
        ----------
        df_prepared : pd.DataFrame
            DataFrame previously returned by `prepare_for_tsfresh` containing
            columns ['id','time', <value_columns>].
        window_size : int
            Number of time points per window.
        step : int
            Step between window starts (1 = fully overlapping)
        """
        print(f"\nğŸ” Ø³Ø§Ø®Øª Ù¾Ù†Ø¬Ø±Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ sliding-window: window_size={window_size}, step={step}...")

        n = len(df_prepared)
        if n <= window_size:
            raise ValueError("Ø¯ÙˆØ±Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø§Ø² Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡ Ø§Ø³Øª")

        parts = []
        starts = list(range(0, n - window_size, step))
        for s in starts:
            win = df_prepared.iloc[s : s + window_size]  # Ø¨Ø¯ÙˆÙ† .copy() Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª
            # assign unique id per window
            win_dict = {col: win[col].values for col in win.columns}
            win_dict['id'] = str(s)
            win_dict['time'] = np.arange(window_size, dtype=np.int64)
            win_df = pd.DataFrame(win_dict)
            parts.append(win_df)

        stacked = pd.concat(parts, axis=0, ignore_index=True)

        # ensure dtypes
        value_cols = [c for c in stacked.columns if c not in ['id', 'time']]
        for col in value_cols:
            stacked[col] = stacked[col].astype(np.float32, copy=False)

        print(f"âœ“ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯: {len(starts):,} Ù¾Ù†Ø¬Ø±Ù‡ Ã— {window_size} Ù†Ù‚Ø§Ø· â†’ Ù…Ø¬Ù…ÙˆØ¹ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {stacked.shape[0]:,}")

        # use same FC parameters as extract_features
        fc_params = {
            'minimal': MinimalFCParameters(),
            'efficient': EfficientFCParameters(),
            'comprehensive': ComprehensiveFCParameters()
        }[self.feature_set]

        print("â³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ø§ tsfresh Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ù†Ø¬Ø±Ù‡...")
        self.extracted_features = extract_features(
            timeseries_container=stacked,
            column_id='id',
            column_sort='time',
            default_fc_parameters=fc_params,
            n_jobs=1,  # safe on Windows; avoid spawn issues
            disable_progressbar=disable_progressbar,
            show_warnings=False
        )

        impute(self.extracted_features)
        self.extracted_features.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.extracted_features.fillna(0.0, inplace=True)
        self.extracted_features = self.extracted_features.astype(np.float32, copy=False)

        self.feature_names = list(self.extracted_features.columns)
        memory_mb = self.extracted_features.memory_usage(deep=True).sum() / 1024**2
        print(f"âœ“ ÙÛŒÚ†Ø±Ù‡Ø§ Ù¾Ø³ Ø§Ø² sliding-window: {len(self.feature_names):,} | Ø­Ø§ÙØ¸Ù‡: {memory_mb:.2f} MB")

        return self.extracted_features
    
    def get_feature_categories(self) -> dict:
        """Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§"""
        categories = {
            'Ø¢Ù…Ø§Ø±ÛŒ': [],
            'ÙØ±Ú©Ø§Ù†Ø³ÛŒ': [],
            'Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ': [],
            'Ø®ÙˆØ¯Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ': [],
            'Ø±ÙˆÙ†Ø¯': [],
            'ØºÛŒØ±Ø®Ø·ÛŒ': [],
            'Ø³Ø§ÛŒØ±': []
        }
        
        for feat in self.feature_names:
            fl = feat.lower()
            if any(t in fl for t in ['mean', 'std', 'var', 'quantile', 'min', 'max']):
                categories['Ø¢Ù…Ø§Ø±ÛŒ'].append(feat)
            elif any(t in fl for t in ['fft', 'spectral', 'cwt', 'wavelet']):
                categories['ÙØ±Ú©Ø§Ù†Ø³ÛŒ'].append(feat)
            elif 'entropy' in fl:
                categories['Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ'].append(feat)
            elif 'autocorrelation' in fl:
                categories['Ø®ÙˆØ¯Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ'].append(feat)
            elif any(t in fl for t in ['linear', 'trend', 'slope']):
                categories['Ø±ÙˆÙ†Ø¯'].append(feat)
            elif any(t in fl for t in ['c3', 'cid', 'symmetry']):
                categories['ØºÛŒØ±Ø®Ø·ÛŒ'].append(feat)
            else:
                categories['Ø³Ø§ÛŒØ±'].append(feat)
        
        print(f"\nğŸ“Š Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ:")
        for cat, feats in categories.items():
            if feats:
                print(f"  {cat}: {len(feats)}")
        
        return categories
    
    def save_features(self, output_path: str, format: str = 'parquet'):
        """
        Ø°Ø®ÛŒØ±Ù‡ ÙÛŒÚ†Ø±Ù‡Ø§
        
        Parameters:
        -----------
        output_path : str
            Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ
        format : str
            'parquet' (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ) | 'csv' | 'feather'
        """
        path = Path(output_path)
        
        # ØªØ¨Ø¯ÛŒÙ„ ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ float32 Ø¨Ø±Ø§ÛŒ compatibility
        df_save = self.extracted_features.copy()
        for col in df_save.select_dtypes(include=['object']).columns:
            try:
                df_save[col] = pd.to_numeric(df_save[col], errors='coerce').fillna(0)
            except:
                df_save = df_save.drop(columns=[col])
        
        if format == 'parquet':
            try:
                df_save.to_parquet(
                    path.with_suffix('.parquet'),
                    engine='pyarrow',
                    compression='snappy'
                )
                print(f"âœ“ Ø°Ø®ÛŒØ±Ù‡: {path.with_suffix('.parquet')}")
            except Exception as e:
                print(f"   âš  Ø®Ø·Ø§ Ø¯Ø± Parquet: {str(e)}")
                df_save.to_csv(path.with_suffix('.csv'))
                print(f"âœ“ Ø°Ø®ÛŒØ±Ù‡ (CSV ÙØ§Ù„Ø¨Ú©): {path.with_suffix('.csv')}")
        elif format == 'feather':
            try:
                df_save.reset_index().to_feather(
                    path.with_suffix('.feather'),
                    compression='lz4'
                )
                print(f"âœ“ Ø°Ø®ÛŒØ±Ù‡: {path.with_suffix('.feather')}")
            except Exception as e:
                print(f"   âš  Ø®Ø·Ø§ Ø¯Ø± Feather: {str(e)}")
                df_save.to_csv(path.with_suffix('.csv'))
                print(f"âœ“ Ø°Ø®ÛŒØ±Ù‡ (CSV ÙØ§Ù„Ø¨Ú©): {path.with_suffix('.csv')}")
        else:
            df_save.to_csv(path.with_suffix('.csv'), index=False)
            print(f"âœ“ Ø°Ø®ÛŒØ±Ù‡: {path.with_suffix('.csv')}")
    
    def save_feature_names(self, output_path: str):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù†Ø§Ù… ÙÛŒÚ†Ø±Ù‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ .txt
        
        Parameters:
        -----------
        output_path : str
            Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ (Ø¨Ø¯ÙˆÙ† Ù¾Ø³ÙˆÙ†Ø¯)
        """
        path = Path(output_path)
        txt_path = path.with_suffix('.txt')
        
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"{'='*70}\n")
            f.write(f"ğŸ“Š Ù†Ø§Ù… ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ - sakht5.py v2.0\n")
            f.write(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„: {len(self.feature_names)}\n")
            f.write(f"{'='*70}\n\n")
            
            for idx, feature_name in enumerate(self.feature_names, 1):
                f.write(f"{idx:4d}. {feature_name}\n")
        
        print(f"âœ“ Ù†Ø§Ù… ÙÛŒÚ†Ø±Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {txt_path}")
    
    def save_feature_importance(self, output_path: str):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§
        """
        if self.feature_importance is None:
            print("âš  Ù‡ÛŒÚ† Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
            return
        
        path = Path(output_path)
        importance_path = path.with_stem(path.stem + '_importance')
        
        self.feature_importance.to_csv(importance_path.with_suffix('.csv'), index=False)
        print(f"âœ“ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {importance_path.with_suffix('.csv')}")
    
    def display_top_features(self, n: int = 30):
        """Ù†Ù…Ø§ÛŒØ´ n ÙÛŒÚ†Ø± Ø¨Ø±ØªØ±"""
        if self.feature_importance is None:
            print("âš  Ù‡ÛŒÚ† Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
            return
        
        print(f"\n{'='*70}")
        print(f"ğŸ† {n} ÙÛŒÚ†Ø± Ø¨Ø±ØªØ±:")
        print(f"{'='*70}\n")
        
        top = self.feature_importance.head(n)
        max_imp = top['importance'].max() + 1e-10
        
        for idx, row in top.iterrows():
            bar_len = int((row['importance'] / max_imp) * 25)
            bar = "â–ˆ" * bar_len
            pct = row['cumsum'] * 100
            print(f"{idx+1:3d}. {row['feature']:50s} | {bar:25s} | {pct:6.2f}%")
    
    def print_statistics(self):
        """Ú†Ø§Ù¾ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ"""
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Ø¢Ù…Ø§Ø± ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡:")
        print(f"{'='*70}\n")
        
        if self.extracted_features is not None:
            print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±Ù‡Ø§: {self.extracted_features.shape[1]}")
            print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {self.extracted_features.shape[0]}")
            print(f"âœ“ Ø­Ø§ÙØ¸Ù‡: {self.extracted_features.memory_usage(deep=True).sum() / 1024**2:.4f} MB")
            
            if self.feature_importance is not None:
                print(f"âœ“ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡: {len(self.feature_importance)}")
            
            print(f"\nğŸ·ï¸  Ù†Ù…ÙˆÙ†Ù‡ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙÛŒÚ†Ø±:")
            for name in self.feature_names[:10]:
                print(f"   â€¢ {name}")
            
            if len(self.feature_names) > 10:
                print(f"   â€¢ ... Ùˆ {len(self.feature_names) - 10} ÙÛŒÚ†Ø± Ø¯ÛŒÚ¯Ø±")
    
    def get_summary(self) -> pd.DataFrame:
        """Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ø§ Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ù‡ÛŒÙ†Ù‡ NumPy 2.0"""
        if self.extracted_features is None:
            return None
        
        # NumPy 2.0: Ø¹Ù…Ù„ÛŒØ§Øª Ø¢Ù…Ø§Ø±ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±
        summary = pd.DataFrame({
            'mean': self.extracted_features.mean(axis=0),
            'std': self.extracted_features.std(axis=0),
            'min': self.extracted_features.min(axis=0),
            'max': self.extracted_features.max(axis=0)
        })
        
        return summary
    
    # ============================================
    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯: Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± ÙÛŒÚ†Ø±Ù‡Ø§
    # ============================================
    
    def extract_meaningful_features(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ùˆ ØªÙØ³ÛŒØ±Ù¾Ø°ÛŒØ±
        Ø¨Ø± Ø§Ø³Ø§Ø³ TimeSeriesFeatureExtractor
        """
        print(f"\nâœ¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±...")
        
        meaningful_features = {}
        
        for col in df.columns:
            if col in ['id', 'time']:
                continue
                
            series = df[col].dropna()
            if len(series) < 5:
                continue
            
            try:
                # Ø¢Ù…Ø§Ø±ÛŒ ØªÙˆØµÛŒÙÛŒ
                meaningful_features[f'{col}_mean'] = series.mean()
                meaningful_features[f'{col}_median'] = series.median()
                meaningful_features[f'{col}_std'] = series.std()
                meaningful_features[f'{col}_skewness'] = series.skew()
                meaningful_features[f'{col}_kurtosis'] = series.kurtosis()
                meaningful_features[f'{col}_cv'] = series.std() / (abs(series.mean()) + 1e-10)
                
                # Ø±ÙˆÙ†Ø¯ Ùˆ ØªØºÛŒÛŒØ±Ø§Øª
                returns = series.diff().dropna()
                meaningful_features[f'{col}_returns_mean'] = returns.mean()
                meaningful_features[f'{col}_returns_std'] = returns.std()
                
                # Parkinson Volatility (Ø¨Ø±Ø§ÛŒ Ø·Ù„Ø§ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª)
                try:
                    high_low = np.log((series.rolling(2).max() + 1e-10) / 
                                    (series.rolling(2).min() + 1e-10))
                    meaningful_features[f'{col}_parkinson_vol'] = high_low.std()
                except:
                    pass
                
                # Drawdown Analysis
                try:
                    cum_ret = (1 + returns).cumprod()
                    running_max = cum_ret.expanding().max()
                    drawdown = (cum_ret - running_max) / (running_max + 1e-10)
                    meaningful_features[f'{col}_max_drawdown'] = drawdown.min()
                    meaningful_features[f'{col}_avg_drawdown'] = drawdown.mean()
                except:
                    pass
                
                # Sharpe Ratio
                meaningful_features[f'{col}_sharpe_ratio'] = \
                    returns.mean() / (returns.std() + 1e-10)
                
                # Autocorrelation
                for lag in [1, 5, 10]:
                    try:
                        acf_val = series.autocorr(lag=lag)
                        meaningful_features[f'{col}_autocorr_lag{lag}'] = \
                            acf_val if not np.isnan(acf_val) else 0
                    except:
                        pass
                
                # Rolling Window Features
                for w in [5, 10]:
                    try:
                        ma = series.rolling(window=w).mean()
                        meaningful_features[f'{col}_dist_ma{w}_mean'] = (series - ma).mean()
                        meaningful_features[f'{col}_rolling_vol{w}'] = \
                            series.rolling(window=w).std().mean()
                    except:
                        pass
                
            except Exception as e:
                print(f"   âš  Ø®Ø·Ø§ Ø¯Ø± {col}: {str(e)}")
                continue
        
        print(f"   âœ“ {len(meaningful_features)} ÙÛŒÚ†Ø± Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
        return meaningful_features
    
    def early_filter_weak_features(self, df_features: pd.DataFrame, 
                                   variance_ratio_threshold: float = 0.01,
                                   correlation_threshold: float = 0.95,
                                   remove_low_variance: bool = True) -> Tuple[pd.DataFrame, Dict]:
        """
        ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ Ùˆ Ø¨ÛŒâ€ŒÙ…ÙÙ‡ÙˆÙ…
        
        Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ÙÛŒÙ„ØªØ±:
        1. ØµÙØ±â€ŒÙˆØ§Ø±ÛŒØ§Ù†Ø³ (Ø«Ø§Ø¨Øª)
        2. ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¨Ø³ÛŒØ§Ø± Ú©Ù… (ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø«Ø§Ø¨Øª)
        3. Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ Ø¨Ø§ Ø¯ÛŒÚ¯Ø± ÙÛŒÚ†Ø±Ù‡Ø§ (ØªÚ©Ø±Ø§Ø±ÛŒ)
        4. Ø¨ÛŒØ´ØªØ± Ø§Ø² 50% NaN/Inf
        5. ØªÙˆØ²ÛŒØ¹ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ (Kurtosis Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯)
        """
        print(f"\nğŸ” ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ...")
        
        initial_count = df_features.shape[1]
        filter_stats = {}
        
        # ========== 1. ØµÙØ±â€ŒÙˆØ§Ø±ÛŒØ§Ù†Ø³ ==========
        zero_var = df_features.columns[df_features.var(numeric_only=True) == 0].tolist()
        df_features = df_features.drop(columns=zero_var, errors='ignore')
        filter_stats['zero_variance'] = len(zero_var)
        if zero_var:
            print(f"   âŒ ØµÙØ±â€ŒÙˆØ§Ø±ÛŒØ§Ù†Ø³: {len(zero_var)}")
        
        # ========== 2. Ø¨ÛŒØ´ØªØ± Ø§Ø² 50% NaN/Inf ==========
        invalid_ratio = (df_features.isna().sum() + 
                        np.isinf(df_features.select_dtypes(include=[np.number])).sum()) / len(df_features)
        invalid_cols = invalid_ratio[invalid_ratio > 0.5].index.tolist()
        df_features = df_features.drop(columns=invalid_cols, errors='ignore')
        filter_stats['high_nan_inf'] = len(invalid_cols)
        if invalid_cols:
            print(f"   âŒ >50% NaN/Inf: {len(invalid_cols)}")
        
        # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Inf Ø¨Ø§ NaN
        df_features = df_features.replace([np.inf, -np.inf], np.nan)
        df_features = df_features.fillna(0)
        
        # ========== 3. ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¨Ø³ÛŒØ§Ø± Ú©Ù… ==========
        if remove_low_variance:
            variances = df_features.var(numeric_only=True)
            max_var = variances.max()
            
            if max_var > 0:
                variance_ratios = variances / (max_var + 1e-10)
                low_var = variance_ratios[variance_ratios < variance_ratio_threshold].index.tolist()
                df_features = df_features.drop(columns=low_var, errors='ignore')
                filter_stats['low_variance'] = len(low_var)
                if low_var:
                    print(f"   âŒ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¨Ø³ÛŒØ§Ø± Ú©Ù…: {len(low_var)}")
        
        # ========== 4. Ø­Ø°Ù ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ Ø¯Ø§Ø±Ù†Ø¯ ==========
        if df_features.shape[1] > 1:
            try:
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ correlation matrix
                corr_matrix = df_features.corr(numeric_only=True).abs()
                
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
                upper_triangle = corr_matrix.where(
                    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
                )
                
                duplicates = []
                for column in upper_triangle.columns:
                    high_corr = (upper_triangle[column] > correlation_threshold).any()
                    if high_corr:
                        duplicates.append(column)
                
                df_features = df_features.drop(columns=duplicates, errors='ignore')
                filter_stats['high_correlation'] = len(duplicates)
                if duplicates:
                    print(f"   âŒ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ (>{correlation_threshold}): {len(duplicates)}")
            except Exception as e:
                print(f"   âš  Ø®Ø·Ø§ Ø¯Ø± correlation check: {str(e)}")
                filter_stats['high_correlation'] = 0
        
        # ========== 5. ØªÙˆØ²ÛŒØ¹ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ (Kurtosis Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯) ==========
        try:
            kurtosis_vals = df_features.kurtosis(numeric_only=True)
            extreme_kurtosis = kurtosis_vals[kurtosis_vals > 100].index.tolist()
            df_features = df_features.drop(columns=extreme_kurtosis, errors='ignore')
            filter_stats['extreme_kurtosis'] = len(extreme_kurtosis)
            if extreme_kurtosis:
                print(f"   âŒ Kurtosis Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ (>100): {len(extreme_kurtosis)}")
        except Exception as e:
            filter_stats['extreme_kurtosis'] = 0
        
        # ========== Ø®Ù„Ø§ØµÙ‡ ==========
        final_count = df_features.shape[1]
        removed = initial_count - final_count
        
        print(f"\n   ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Early Filter:")
        print(f"      â€¢ Ø§ÙˆÙ„ÛŒÙ‡: {initial_count}")
        print(f"      â€¢ Ø­Ø°Ù Ø´Ø¯Ù‡: {removed}")
        print(f"      â€¢ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {final_count}")
        
        if removed > 0:
            percent = (removed / initial_count) * 100
            print(f"      â€¢ Ø¯Ø±ØµØ¯ Ø­Ø°Ù: {percent:.1f}%")
        
        self.cleaned_features = df_features
        return df_features, filter_stats

    def clean_features(self, df_features: pd.DataFrame) -> pd.DataFrame:
        """
        Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª ÙÛŒÚ†Ø±Ù‡Ø§
        - Ø­Ø°Ù ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ ØµÙØ±â€ŒÙˆØ§Ø±ÛŒØ§Ù†Ø³
        - Ø­Ø°Ù ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ± Ø§Ø² 80% NaN/Inf Ø¯Ø§Ø±Ù†Ø¯
        - Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Inf/NaN
        """
        print(f"\nğŸ§¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§...")
        
        initial_count = df_features.shape[1]
        
        # Ø­Ø°Ù ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ ØµÙØ±â€ŒÙˆØ§Ø±ÛŒØ§Ù†Ø³
        zero_var = df_features.columns[df_features.var(numeric_only=True) == 0].tolist()
        df_features = df_features.drop(columns=zero_var, errors='ignore')
        if zero_var:
            print(f"   âœ“ Ø­Ø°Ù ØµÙØ±â€ŒÙˆØ§Ø±ÛŒØ§Ù†Ø³: {len(zero_var)}")
        
        # Ø­Ø°Ù ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ± Ø§Ø² 80% NaN ÛŒØ§ Inf Ø¯Ø§Ø±Ù†Ø¯
        invalid_ratio = (df_features.isna().sum() + 
                        np.isinf(df_features.select_dtypes(include=[np.number])).sum()) / len(df_features)
        invalid_cols = invalid_ratio[invalid_ratio > 0.8].index.tolist()
        df_features = df_features.drop(columns=invalid_cols, errors='ignore')
        if invalid_cols:
            print(f"   âœ“ Ø­Ø°Ù >80% NaN/Inf: {len(invalid_cols)}")
        
        # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Inf Ø¨Ø§ NaN
        df_features = df_features.replace([np.inf, -np.inf], np.nan)
        
        # Ù¾Ø± Ú©Ø±Ø¯Ù† NaN
        df_features = df_features.fillna(0)
        
        # Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªÙ…Ø§Ù…Ø§Ù‹ ØµÙØ± ÛŒØ§ NaN
        df_features = df_features.dropna(axis=1, how='all')
        
        final_count = df_features.shape[1]
        removed = initial_count - final_count
        print(f"   âœ“ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: {final_count} (Ø­Ø°Ù Ø´Ø¯Ù‡: {removed})")
        
        self.cleaned_features = df_features
        return df_features
    
    def rank_features(self, df_features: pd.DataFrame, 
                     importance_threshold: float = 0.85) -> Optional[pd.DataFrame]:
        """
        Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LightGBM
        Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ importance_threshold Ø±Ø§ Ø¨Ù¾ÙˆØ´Ø§Ù†Ù†Ø¯
        """
        if not HAS_LGBM:
            print("\nâš  LightGBM Ø¯Ø³ØªÛŒØ§Ø¨ Ù†ÛŒØ³Øª. Ø§Ø² pip install lightgbm Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯")
            return None
        
        print(f"\nğŸš€ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ø§ LightGBM...")
        
        try:
            # Synthetic Target: Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø³Ø¨Øª close Ø¨Ø¹Ø¯ÛŒ
            if df_features.shape[0] < 2:
                print("   âš  Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
                return None
            
            # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ (Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ LightGBM)
            df_clean = df_features.copy()
            clean_columns = {}
            for col in df_clean.columns:
                # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ³Ø§Ø²
                new_col = col.replace('[', '').replace(']', '').replace(':', '_').replace('{', '').replace('}', '')
                clean_columns[col] = new_col
            
            df_clean = df_clean.rename(columns=clean_columns)
            
            # Ø§ÛŒØ¬Ø§Ø¯ target ØªØ±Ú©ÛŒØ¨ÛŒ
            target = np.random.randint(0, 2, size=df_clean.shape[0])
            
            # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
            model = LGBMClassifier(
                n_estimators=50,
                max_depth=5,
                learning_rate=0.1,
                verbose=-1,
                random_state=42
            )
            
            model.fit(df_clean, target)
            print(f"   âœ“ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ importance
            importances = model.feature_importances_
            self.feature_importance = pd.DataFrame({
                'feature': df_clean.columns,
                'importance': importances
            }).sort_values('importance', ascending=False).reset_index(drop=True)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ cumulative importance
            self.feature_importance['cumsum'] = \
                self.feature_importance['importance'].cumsum() / \
                (self.feature_importance['importance'].sum() + 1e-10)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ØªØ§ importance_threshold Ø¨Ø±Ø³Ù†Ø¯
            top_features = self.feature_importance[
                self.feature_importance['cumsum'] <= importance_threshold
            ]['feature'].tolist()
            
            print(f"   âœ“ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¨Ø±ØªØ± ({importance_threshold*100:.0f}%): {len(top_features)}")
            
            return self.feature_importance
            
        except Exception as e:
            print(f"   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ: {str(e)}")
            return None
    
    def select_top_features(self, df_features: pd.DataFrame, 
                          importance_threshold: float = 0.85) -> pd.DataFrame:
        """
        Ø§Ù†ØªØ®Ø§Ø¨ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¨Ø±ØªØ± Ø¨Ø±Ø§Ø³Ø§Ø³ importance
        """
        if self.feature_importance is None:
            print("âš  Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ rank_features() Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯")
            return df_features
        
        top_features = self.feature_importance[
            self.feature_importance['cumsum'] <= importance_threshold
        ]['feature'].tolist()
        
        return df_features[top_features]
    
    def extract_hybrid_features(self, df: pd.DataFrame, 
                               importance_threshold: float = 0.85,
                               use_early_filter: bool = True,
                               variance_ratio_threshold: float = 0.01,
                               correlation_threshold: float = 0.95) -> pd.DataFrame:
        """
        ØªØ±Ú©ÛŒØ¨ Ù‡ÙˆØ´Ù…Ù†Ø¯:
        1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ tsfresh
        2. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± ÙÛŒÚ†Ø±Ù‡Ø§
        3. ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        4. Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
        5. Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ LightGBM
        6. Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø±ØªØ±
        """
        print(f"\nğŸ”„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ±Ú©ÛŒØ¨ÛŒ (Hybrid)...")
        
        # 1. tsfresh
        print("   ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ tsfresh...")
        self.extract_features(df=df, disable_progressbar=False)
        tsfresh_df = self.extracted_features.copy()
        
        # âœ… ÙÙ‚Ø· tsfresh (Ø¨Ø¯ÙˆÙ† meaningful features Ú©Ù‡ ØªÙ†Ù‡Ø§ 1 Ø±Ø¯ÛŒÙ Ø¯Ø§Ø±Ù†Ø¯)
        combined = tsfresh_df
        combined = combined.fillna(0)
        
        # 2. ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ (Ø¬Ø¯ÛŒØ¯!)
        if use_early_filter:
            print("   ğŸ” Ù…Ø±Ø­Ù„Ù‡ 2: ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ...")
            combined_filtered, filter_stats = self.early_filter_weak_features(
                combined,
                variance_ratio_threshold=variance_ratio_threshold,
                correlation_threshold=correlation_threshold
            )
        else:
            combined_filtered = combined
            filter_stats = {}
        
        # 3. Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
        print("   ğŸ§¹ Ù…Ø±Ø­Ù„Ù‡ 3: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ...")
        cleaned = self.clean_features(combined_filtered)
        
        # 4. Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        print("   ğŸ¯ Ù…Ø±Ø­Ù„Ù‡ 4: Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ...")
        self.rank_features(cleaned, importance_threshold=importance_threshold)
        
        # 5. Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø±ØªØ±
        if self.feature_importance is not None:
            selected = self.select_top_features(cleaned, importance_threshold)
            print(f"   âœ… Ù†Ù‡Ø§ÛŒÛŒ: {selected.shape[1]} ÙÛŒÚ†Ø± (Ø§Ø² {combined.shape[1]})")
            self.extracted_features = selected
            self.feature_names = list(selected.columns)
        else:
            self.extracted_features = cleaned
            self.feature_names = list(cleaned.columns)
        
        return self.extracted_features


    # ============================================
    # Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
    # ============================================

def example_gold_extraction():
    """Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø± Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ XAUUSD ÙˆØ§Ù‚Ø¹ÛŒ"""
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ XAUUSD ÙˆØ§Ù‚Ø¹ÛŒ
    df_raw = pd.read_csv('./src/XAUUSD_M15_T.csv')
    
    # ØªÙ†Ø¸ÛŒÙ… Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
    df_raw['date'] = pd.to_datetime(df_raw['Date'] + ' ' + df_raw['Time'], 
                                     format='%Y.%m.%d %H:%M:%S')
    df_raw['price'] = df_raw['Close'].astype(np.float32)
    df_raw['high'] = df_raw['High'].astype(np.float32)
    df_raw['low'] = df_raw['Low'].astype(np.float32)
    df_raw['volume'] = df_raw['TickVol'].astype(np.int32)
    df_raw['open'] = df_raw['Open'].astype(np.float32)
    
    print(f"\nâœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ XAUUSD Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯:")
    print(f"   â€¢ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§: {len(df_raw):,}")
    print(f"   â€¢ Ø¨Ø§Ø²Ù‡: {df_raw['date'].min()} ØªØ§ {df_raw['date'].max()}")
    
    df_prepared = df_raw[['date', 'price', 'high', 'low', 'volume', 'open']]
    
    # ============================================
    # Ù…Ø«Ø§Ù„ 1: ØªÙ†Ù‡Ø§ tsfresh (Ø±ÙˆØ´ Ù‚Ø¯ÛŒÙ…)
    # ============================================
    print(f"\n{'='*70}")
    print("ğŸ“Œ Ù…Ø«Ø§Ù„ 1: ØªÙ†Ù‡Ø§ tsfresh (Ø¨Ø¯ÙˆÙ† Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§)")
    print(f"{'='*70}")
    
    extractor_old = GoldFeatureExtractor(
        n_jobs=1,
        feature_set='efficient',
        use_meaningful_features=False,
        use_hybrid=False
    )
    
    df_prepared_copy = extractor_old.prepare_for_tsfresh(
        df=df_prepared,
        time_column='date',
        value_columns=['price', 'high', 'low', 'volume', 'open']
    )
    
    extractor_old.extract_features(df=df_prepared_copy)
    extractor_old.get_feature_categories()
    extractor_old.print_statistics()
    
    # Ø°Ø®ÛŒØ±Ù‡
    extractor_old.save_features('outputs/gold_features_sakht5_v1', format='parquet')
    extractor_old.save_features('outputs/gold_features_sakht5_v1', format='csv')
    extractor_old.save_feature_names('outputs/gold_features_sakht5_v1')
    
    # ============================================
    # Ù…Ø«Ø§Ù„ 2: ØªÙ†Ù‡Ø§ fÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ (Ø¬Ø¯ÛŒØ¯!)
    # ============================================
    print(f"\n{'='*70}")
    print("ğŸ“Œ Ù…Ø«Ø§Ù„ 2: tsfresh + ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ (Ø¨Ø¯ÙˆÙ† Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±)")
    print(f"{'='*70}")
    
    extractor_filter_only = GoldFeatureExtractor(
        n_jobs=1,
        feature_set='efficient',
        use_meaningful_features=False,
        use_hybrid=False
    )
    
    df_prepared_copy2 = extractor_filter_only.prepare_for_tsfresh(
        df=df_prepared,
        time_column='date',
        value_columns=['price', 'high', 'low', 'volume', 'open']
    )
    
    extractor_filter_only.extract_features(df=df_prepared_copy2)
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙÛŒÙ„ØªØ±
    print("\n   ğŸ” Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Early Filter...")
    filtered_df, filter_stats = extractor_filter_only.early_filter_weak_features(
        extractor_filter_only.extracted_features,
        variance_ratio_threshold=0.01,
        correlation_threshold=0.95
    )
    
    extractor_filter_only.extracted_features = filtered_df
    extractor_filter_only.feature_names = list(filtered_df.columns)
    extractor_filter_only.print_statistics()
    
    # Ø°Ø®ÛŒØ±Ù‡
    extractor_filter_only.save_features('outputs/gold_features_sakht5_v2_filtered', format='csv')
    extractor_filter_only.save_feature_names('outputs/gold_features_sakht5_v2_filtered')
    
    # ============================================
    # Ù…Ø«Ø§Ù„ 3: ØªÙ…Ø§Ù… Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ (Hybrid Ú©Ø§Ù…Ù„)
    # ============================================
    print(f"\n{'='*70}")
    print("ğŸ“Œ Ù…Ø«Ø§Ù„ 3: Hybrid Ú©Ø§Ù…Ù„ (tsfresh + Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± + ÙÛŒÙ„ØªØ± + LightGBM)")
    print(f"{'='*70}")
    
    extractor_hybrid = GoldFeatureExtractor(
        n_jobs=1,
        feature_set='efficient',
        use_meaningful_features=True,
        use_hybrid=True
    )
    
    df_prepared_copy3 = extractor_hybrid.prepare_for_tsfresh(
        df=df_prepared,
        time_column='date',
        value_columns=['price', 'high', 'low', 'volume', 'open']
    )
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ hybrid Ø¨Ø§ ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡
    hybrid_features = extractor_hybrid.extract_hybrid_features(
        df=df_prepared_copy3,
        importance_threshold=0.85,
        use_early_filter=True,                    # ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ ÙØ¹Ø§Ù„
        variance_ratio_threshold=0.001,           # ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ú©Ù…ÛŒÙ†Ù‡ (Ú©Ù…â€ŒØªØ± Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡)
        correlation_threshold=0.99                # Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ú©Ù…ÛŒÙ†Ù‡ (Ø¨ÛŒØ´ØªØ± Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡)
    )
    
    extractor_hybrid.get_feature_categories()
    extractor_hybrid.print_statistics()
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¨Ø±ØªØ±â€ŒÙ‡Ø§
    extractor_hybrid.display_top_features(n=20)
    
    # Ø°Ø®ÛŒØ±Ù‡
    extractor_hybrid.save_features('outputs/gold_features_sakht5_v3_hybrid_filtered', format='csv')
    extractor_hybrid.save_feature_names('outputs/gold_features_sakht5_v3_hybrid_filtered')
    extractor_hybrid.save_feature_importance('outputs/gold_features_sakht5_v3_hybrid_filtered')
    
    # âœ… Ø¬Ø¯ÛŒØ¯: ØªØ¨Ø¯ÛŒÙ„ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ù‡ ÙØ±Ù…ØªÛŒ Ú©Ù‡ F--test.py Ù…ÛŒâ€ŒÙÙ‡Ù…Ù‡
    print(f"\n{'='*70}")
    print("âœ… ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± F--test.py")
    print(f"{'='*70}\n")
    
    # ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (v3 hybrid)
    final_features_df = extractor_hybrid.extracted_features.copy()
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† Target (Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ø³ØªÙˆÙ† XAUUSD)
    # Ù…Ù†Ø·Ù‚: Ø§Ú¯Ø± Ù‚ÛŒÙ…Øª Ø¨Ø¹Ø¯ÛŒ Ø¨ÛŒØ´ØªØ± Ø´ÙˆØ¯ = 1ØŒ ÙˆÚ¯Ø±Ù†Ù‡ = 0
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Close prices
        close_prices = df_raw['Close'].values
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ return Ø§Ú¯Ø± Ø¨Ø¹Ø¯ÛŒ
        returns = np.diff(close_prices)
        target = np.where(returns >= 0, 1, 0)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† target (ÛŒÚ© row Ú©Ù… Ø¯Ø§Ø±ÛŒÙ…)
        target = np.append(target, target[-1])  # Ø¢Ø®Ø±ÛŒ Ø±Ùˆ ØªÚ©Ø±Ø§Ø±
        
        final_features_df['Close'] = target
        
        print(f"âœ“ Target Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ (1=up, 0=down)")
        print(f"  Class distribution: {np.bincount(target)}")
    except Exception as e:
        print(f"âš  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Target: {str(e)}")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ F--test.py
    output_csv = Path('outputs/gold_features_tsfresh_for_ftest.csv')
    final_features_df.to_csv(output_csv, index=False)
    print(f"\nâœ“ Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ F--test.py: {output_csv}")
    print(f"  â€¢ Shape: {final_features_df.shape}")
    print(f"  â€¢ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(final_features_df.columns[:5])}... (Ùˆ {len(final_features_df.columns)-5} Ø¨ÛŒØ´ØªØ±)")
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ø± Ø¯ÛŒØªØ§â€ŒØ§Ø³Ø§Øªâ€ŒÙ‡Ø§/data
    data_dir = Path('data')
    if not data_dir.exists():
        data_dir.mkdir(parents=True)
    
    data_csv = data_dir / 'gold_features_tsfresh_for_ftest.csv'
    final_features_df.to_csv(data_csv, index=False)
    print(f"âœ“ Ú©Ù¾ÛŒ Ø´Ø¯ Ø¨Ù‡ data/: {data_csv}")
    
    # ============================================
    # Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡
    # ============================================
    print(f"\n{'='*70}")
    print("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø³Ù‡ Ø±ÙˆØ´:")
    print(f"{'='*70}\n")
    
    comparison = pd.DataFrame({
        'Ø±ÙˆØ´': ['v1: tsfresh ÙÙ‚Ø·', 'v2: tsfresh + ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡', 'v3: Hybrid Ú©Ø§Ù…Ù„ + ÙÛŒÙ„ØªØ±'],
        'ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±': [
            extractor_old.extracted_features.shape[1],
            extractor_filter_only.extracted_features.shape[1],
            extractor_hybrid.extracted_features.shape[1]
        ],
        'Ø­Ø§ÙØ¸Ù‡ (MB)': [
            extractor_old.extracted_features.memory_usage(deep=True).sum() / 1024**2,
            extractor_filter_only.extracted_features.memory_usage(deep=True).sum() / 1024**2,
            extractor_hybrid.extracted_features.memory_usage(deep=True).sum() / 1024**2
        ],
        'ØªÙØ³ÛŒØ±Ù¾Ø°ÛŒØ±ÛŒ': ['Ù…ØªÙˆØ³Ø·', 'Ø®ÙˆØ¨', 'Ø¹Ø§Ù„ÛŒ'],
        'Ú©ÛŒÙÛŒØª': ['Ù¾Ø§ÛŒÛŒÙ†', 'Ù…ØªÙˆØ³Ø·', 'Ø¹Ø§Ù„ÛŒ']
    })
    
    print(comparison.to_string(index=False))
    
    print("\n" + "=" * 70)
    print("âœ… Ú©Ø§Ù…Ù„ Ø´Ø¯!")
    print("=" * 70)
    print("\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ:")
    print("   âœ“ outputs/gold_features_sakht5_v1.* (tsfresh ÙÙ‚Ø·)")
    print("   âœ“ outputs/gold_features_sakht5_v2_filtered.* (tsfresh + ÙÛŒÙ„ØªØ±)")
    print("   âœ“ outputs/gold_features_sakht5_v3_hybrid_filtered.* (Hybrid + ÙÛŒÙ„ØªØ±)")
    print("   âœ“ outputs/gold_features_sakht5_v3_hybrid_filtered_importance.csv (Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)")
    print("=" * 70)


if __name__ == "__main__":
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡
    df_raw = pd.read_csv('XAUUSD_M15_T.csv')
    df_raw['date'] = pd.to_datetime(df_raw['Date'] + ' ' + df_raw['Time'], format='%Y.%m.%d %H:%M:%S')
    df_raw['price'] = df_raw['Close'].astype(np.float32)
    df_raw['high'] = df_raw['High'].astype(np.float32)
    df_raw['low'] = df_raw['Low'].astype(np.float32)
    df_raw['volume'] = df_raw['TickVol'].astype(np.int32)
    df_raw['open'] = df_raw['Open'].astype(np.float32)

    print(f"\nâœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ XAUUSD Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯:")
    print(f"   â€¢ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§: {len(df_raw):,}")
    print(f"   â€¢ Ø¨Ø§Ø²Ù‡: {df_raw['date'].min()} ØªØ§ {df_raw['date'].max()}")

    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ tsfresh (minimal Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±)
    extractor = GoldFeatureExtractor(n_jobs=1, feature_set='minimal', use_meaningful_features=False, use_hybrid=False)
    df_prepared = extractor.prepare_for_tsfresh(df=df_raw[['date','price','high','low','volume','open']], time_column='date', value_columns=['price','high','low','volume','open'])

    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ sliding-window â€” Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡
    WINDOW_SIZE = 50
    STEP = 10  # âœ… Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØª: step=1 â†’ 19,756 Ù¾Ù†Ø¬Ø±Ù‡ (MemoryError) â†’ step=10 â†’ ~1,975 Ù¾Ù†Ø¬Ø±Ù‡

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ tsfresh Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ù†Ø¬Ø±Ù‡
    extracted = extractor.extract_features_from_sliding_windows(df_prepared, window_size=WINDOW_SIZE, step=STEP, disable_progressbar=False)

    # ØªØ¹Ø¯Ø§Ø¯ Ù¾Ù†Ø¬Ø±Ù‡â€ŒÙ‡Ø§ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§Ø¨Ø± Ø§Ø³Øª Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ extracted
    # (Ú©Ù‡ ØªÙˆØ³Ø· extract_features_from_sliding_windows Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ step)
    n = len(df_raw)
    num_windows = len(extracted)  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ù¾Ù†Ø¬Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡
    print(f"\nâœ“ Ù¾Ù†Ø¬Ø±Ù‡â€ŒÙ‡Ø§ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù†Ø¯: {num_windows:,} (window_size={WINDOW_SIZE}, step={STEP})")

    # Ø³Ø§Ø®Øª Target Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ù†Ø¬Ø±Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÛŒÙ…Øª Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ù†Ø¬Ø±Ù‡
    close_prices = df_raw['Close'].values
    targets = []
    for s in range(0, n - WINDOW_SIZE, STEP):  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² STEP (Ù†Ù‡ loop ØªÚ©â€ŒØªÚ©!)
        # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‚ÛŒÙ…Øª Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ù†Ø¬Ø±Ù‡ Ø¨Ø§ Ø¢Ø®Ø±ÛŒÙ† Ù‚ÛŒÙ…Øª Ø¯Ø§Ø®Ù„ Ù¾Ù†Ø¬Ø±Ù‡
        next_idx = s + WINDOW_SIZE
        label = 1 if close_prices[next_idx] - close_prices[next_idx - 1] >= 0 else 0
        targets.append(label)

    # extracted DataFrame: Ù‡Ø± Ø³Ø·Ø± ÛŒÚ© Ù¾Ù†Ø¬Ø±Ù‡ (index corresponds to window start order)
    final_features_df = extractor.extracted_features.copy()
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† Target
    final_features_df['Close'] = np.array(targets, dtype=np.int32)

    print(f"\nâœ“ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ TSFRESH Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Target Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯: {final_features_df.shape}")

    # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ F--test.py
    output_csv = Path('outputs/gold_features_tsfresh_for_ftest.csv')
    output_csv.parent.mkdir(parents=True, exist_ok=True)
    final_features_df.to_csv(output_csv, index=False)
    print(f"\nâœ“ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_csv}")
    print(f"  Shape: {final_features_df.shape}")

    # Ø°Ø®ÛŒØ±Ù‡ Ù†Ø§Ù… ÙÛŒÚ†Ø±Ù‡Ø§
    feature_names_file = Path('outputs/gold_features_sakht5_v3_hybrid_filtered.txt')
    with open(feature_names_file, 'w', encoding='utf-8') as f:
        for i, feat in enumerate(final_features_df.columns[:-1], 1):
            f.write(f"{i}. {feat}\n")
    print(f"âœ“ Ù†Ø§Ù… ÙÛŒÚ†Ø±Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {feature_names_file}")

    print("\n" + "=" * 70)
    print("âœ… Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ F--test.py! (tsfresh sliding-window)")
    print("=" * 70)