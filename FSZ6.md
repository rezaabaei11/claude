# ğŸ”¬ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ù…Ù…ÛŒØ²ÛŒ Ø±Ø¨Ø§Øª ØªØ³Øª ÙÛŒÚ†Ø±Ù‡Ø§ (FSZ6.py)
## Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ 4 Ø¯ÙˆØ± ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¹Ù…ÛŒÙ‚

**ØªØ§Ø±ÛŒØ® Ø¨Ø±Ø±Ø³ÛŒ:** 18 Ù†ÙˆØ§Ù…Ø¨Ø± 2025  
**Ù†Ø³Ø®Ù‡ Ú©Ø¯:** FSZ6.py  
**Ø¯ÙˆØ±Ù‡ ØªØ­Ù‚ÛŒÙ‚Ø§Øª:** 4 Ø¯ÙˆØ± Ú©Ø§Ù…Ù„ - Ø§Ø² Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ ØªØ§ Ø¹Ù…ÛŒÙ‚â€ŒØªØ±ÛŒÙ† Ø¬Ø²Ø¦ÛŒØ§Øª  
**ÙˆØ¶Ø¹ÛŒØª:** Ø¨Ø­Ø±Ø§Ù†ÛŒ - Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ù‚Ø¯Ø§Ù… ÙÙˆØ±ÛŒ

---

## ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ

Ø§ÛŒÙ† Ø±Ø¨Ø§Øª Ø¨Ø±Ø§ÛŒ **ØªØ³Øª Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§** Ø¯Ø± forex trading Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù¾Ø³ Ø§Ø² 4 Ø¯ÙˆØ± ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¹Ù…ÛŒÙ‚ØŒ **20 Ù…Ø´Ú©Ù„ Ø¨Ø­Ø±Ø§Ù†ÛŒ** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù…Ù†Ø¬Ø± Ø¨Ù‡:

- âŒ **False positive rate Ø¨Ø§Ù„Ø§** (ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù‚ÙˆÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´ÙˆÙ†Ø¯)
- âŒ **Overoptimistic performance estimates** (Ø¯Ù‚Øª 90%+ Ø¯Ø± backtest â†’ Ø¶Ø±Ø± 100% Ø¯Ø± live)
- âŒ **Data leakage Ø§Ø² Ø¢ÛŒÙ†Ø¯Ù‡** (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚ÛŒÙ…Øª ÙØ±Ø¯Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù…Ø±ÙˆØ²)
- âŒ **Backtest overfitting** (Ù…Ø¯Ù„ ÙÙ‚Ø· Ø±ÙˆÛŒ ÛŒÚ© Ù…Ø³ÛŒØ± ØªØ§Ø±ÛŒØ®ÛŒ Ø®Ø§Øµ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯)

### ğŸ¯ Ù†ØªÛŒØ¬Ù‡ Ú©Ù„ÛŒØ¯ÛŒ:

> **"Ú©Ø¯ ÙØ¹Ù„ÛŒ Ø¨Ø±Ø§ÛŒ production trading Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†ÛŒØ³Øª. Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø¨Ø§ÛŒØ¯ Ø­Ø¯Ø§Ù‚Ù„ 10 Ù…Ø´Ú©Ù„ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø§ÙˆÙ„ Ø±ÙØ¹ Ø´ÙˆÙ†Ø¯."**

---

## ğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ 4 Ø¯ÙˆØ± ØªØ­Ù‚ÛŒÙ‚Ø§Øª

| Ø¯ÙˆØ± | ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ú©Ù„Ø§Øª ÛŒØ§ÙØªÙ‡ Ø´Ø¯Ù‡ | Ø³Ø·Ø­ Ø®Ø·Ø± | Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ Ø±ÙØ¹ |
|-----|----------------------|---------|-----------------|
| **Ø¯ÙˆØ± 1** | 8 Ù…Ø´Ú©Ù„ | Critical: 5, High: 3 | 8-10 Ø³Ø§Ø¹Øª |
| **Ø¯ÙˆØ± 2** | +3 Ù…Ø´Ú©Ù„ | Critical: 3 | 2-3 Ø³Ø§Ø¹Øª |
| **Ø¯ÙˆØ± 3** | +6 Ù…Ø´Ú©Ù„ | Critical: 6 | 4-5 Ø³Ø§Ø¹Øª |
| **Ø¯ÙˆØ± 4** | +3 Ù…Ø´Ú©Ù„ | Critical: 3 | 2 Ø³Ø§Ø¹Øª |
| **Ø¬Ù…Ø¹ Ú©Ù„** | **20 Ù…Ø´Ú©Ù„** | **Critical: 17** | **16-20 Ø³Ø§Ø¹Øª** |

---

## ğŸš¨ 20 Ù…Ø´Ú©Ù„ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡ (Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡)

### ÙÙˆØ±ÛŒâ€ŒØªØ±ÛŒÙ† (TOP 5 - Ø¨Ø§ÛŒØ¯ Ø§Ù…Ø±ÙˆØ² Ø±ÙØ¹ Ø´ÙˆÙ†Ø¯!)

#### 1. **Lookahead Bias Ø¯Ø± Feature Engineering** âš¡ Ø®Ø·Ø±Ù†Ø§Ú©â€ŒØªØ±ÛŒÙ†!

**Ù…Ø´Ú©Ù„:**
```python
# Ø¯Ø± preprocessing ÛŒØ§ feature creation:
X['future_return'] = X['close'].pct_change(5).shift(-5)  # âŒ
X.fillna(method='bfill')  # âŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢ÛŒÙ†Ø¯Ù‡!
X_normalized = (X - X.mean()) / X.std()  # âŒ global statistics Ø´Ø§Ù…Ù„ test!
```

**Ú†Ø±Ø§ ÙØ§Ø¬Ø¹Ù‡â€ŒØ¨Ø§Ø± Ø§Ø³Øª:**
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚ÛŒÙ…Øª **ÙØ±Ø¯Ø§** Ø¨Ø±Ø§ÛŒ predict **Ø§Ù…Ø±ÙˆØ²**
- Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ 90%+ accuracy Ú©Ø§Ø°Ø¨ Ù…Ù†Ø¬Ø± Ø´ÙˆØ¯
- Ø¯Ø± live trading: **Ø¶Ø±Ø± 100%** ØªØ¶Ù…ÛŒÙ†ÛŒ!

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def validate_no_lookahead_bias(X: pd.DataFrame):
    """ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± lookahead bias"""
    warnings_found = []
    
    # 1. Ø¨Ø±Ø±Ø³ÛŒ feature names
    suspicious_keywords = ['future', 'next', 'forward', 'ahead', 'lead']
    for col in X.columns:
        if any(kw in col.lower() for kw in suspicious_keywords):
            warnings_found.append(f"Suspicious name: {col}")
    
    # 2. Ø¨Ø±Ø±Ø³ÛŒ trailing NaNs (Ù†Ø´Ø§Ù†Ù‡ shift(-n))
    for col in X.select_dtypes(include=[np.number]).columns:
        if X[col].isna().any():
            trailing_nans = X[col].iloc[::-1].isna().cumsum().iloc[::-1].iloc[-1]
            if trailing_nans > 0:
                warnings_found.append(f"{col} has {trailing_nans} trailing NaNs - possible future shift!")
    
    if warnings_found:
        raise ValueError(f"Lookahead bias detected: {warnings_found}")
    
    return True

# Ø§Ø³ØªÙØ§Ø¯Ù‡:
X_safe = create_safe_features(df)
validate_no_lookahead_bias(X_safe)
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- "Look-ahead Bias & How To Prevent It" (2022)
- "3 Common Time Series Modeling Mistakes" (TDS 2025)

---

#### 2. **Forward/Backward Fill Leakage** âš¡

**Ù…Ø´Ú©Ù„:**
```python
def preprocess_features(self, X, y):
    for col in missing_numeric:
        X[col] = X[col].fillna(method='bfill')  # âŒ CRITICAL!
```

**Ú†Ø±Ø§ Ø®Ø·Ø±Ù†Ø§Ú©:**
- `bfill()` Ø§Ø² **Ø¢ÛŒÙ†Ø¯Ù‡** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø±Ø§ÛŒ fill Ú©Ø±Ø¯Ù† Ú¯Ø°Ø´ØªÙ‡
- Ø¯Ø± time series: Ø§ÛŒÙ† Ù…Ø¹Ø§Ø¯Ù„ **cheating** Ø§Ø³Øª!

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def safe_fill_time_series(X: pd.DataFrame):
    """Fill Ø¨Ø¯ÙˆÙ† leakage"""
    X_filled = X.copy()
    
    # âœ… ÙÙ‚Ø· forward fill (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø°Ø´ØªÙ‡)
    X_filled = X_filled.fillna(method='ffill')
    
    # âœ… ÛŒØ§ median/mean Ø§Ø² train set (Ù†Ù‡ Ú©Ù„ dataset)
    # Ø§ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¯Ø± fit_preprocessors Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯
    
    # âŒ Ù‡Ø±Ú¯Ø² bfill Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯!
    return X_filled
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- "Data Leakage in Pandas: The Perils of Forward and Back Fill" (2023)
- "A Prediction Method with Data Leakage Suppression" (MDPI 2022)

---

#### 3. **Nested CV Feature Selection Leakage** âš¡

**Ù…Ø´Ú©Ù„:**
```python
# âŒ Ø§Ø´ØªØ¨Ø§Ù‡ ÙØ¹Ù„ÛŒ:
X_train_filtered = self.quick_prefilter(X_train, y_train)  # Ø±ÙˆÛŒ Ú©Ù„ train
nested_cv_results = self.nested_cross_validation(X_train_filtered, y_train)
```

**Ú†Ø±Ø§ Ø¨Ø­Ø±Ø§Ù†ÛŒ:**
- Feature selection Ø±ÙˆÛŒ **Ú©Ù„ train set** Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ø³Ù¾Ø³ nested CV Ø±ÙˆÛŒ Ù‡Ù…Ø§Ù† features
- Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ: Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² validation folds Ø¯Ø± feature selection leak Ø´Ø¯Ù‡!
- Bias ØªØ§ **5-15%** Ø¯Ø± performance estimates

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def nested_cv_proper(X, y, n_outer=5):
    """Feature selection INSIDE each fold"""
    
    for outer_fold, (train_idx, val_idx) in enumerate(cv_outer.split(X, y)):
        X_train_outer = X.iloc[train_idx]
        y_train_outer = y.iloc[train_idx]
        
        # âœ… Feature selection ÙÙ‚Ø· Ø±ÙˆÛŒ Ø§ÛŒÙ† fold
        X_train_filtered, _ = quick_prefilter(X_train_outer, y_train_outer)
        
        # Inner CV Ø¨Ø±Ø§ÛŒ hyperparameter tuning
        # ... (Ø±ÙˆÛŒ X_train_filtered)
        
        # Train final model Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† fold
        # ...
    
    return results
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- "nestedcv: an R package" (PMC 2023) - specifically designed for this!
- "Measuring the bias of incorrect application of feature selection" (PMC 2021)
- "Feature Selection without Label or Feature Leakage" (arXiv 2024)

---

#### 4. **Temporal Split Ø¨Ø¯ÙˆÙ† Gap** âš¡

**Ù…Ø´Ú©Ù„:**
```python
def temporal_split(self, X, y):
    split_idx = int(len(X) * 0.8)
    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]  # âŒ Ù‡ÛŒÚ† gap Ø§ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!
```

**Ú†Ø±Ø§ Ø®Ø·Ø±Ù†Ø§Ú©:**
- Ø¢Ø®Ø±ÛŒÙ† sample train = sample Ù‚Ø¨Ù„ Ø§Ø² test
- Features Ø¨Ø§ autocorrelation (Ù…Ø«Ù„ SMA, EMA) leak Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
- Label overlapping Ø¯Ø± forex (Ù…Ø«Ù„Ø§Ù‹ label = return 5 bars ahead)

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def temporal_split_with_gap(X, y, test_size=0.2, gap=24):
    """
    Gap Ø¨Ø±Ø§ÛŒ forex: Ø­Ø¯Ø§Ù‚Ù„ 24 (ÛŒÚ© Ø±ÙˆØ² Ú©Ø§Ù…Ù„)
    """
    n = len(X)
    n_test = int(n * test_size)
    n_train = n - n_test - gap
    
    X_train = X.iloc[:n_train].copy()
    y_train = y.iloc[:n_train].copy()
    
    # Ø­Ø°Ù gap samples
    X_test = X.iloc[n_train + gap:].copy()
    y_test = y.iloc[n_train + gap:].copy()
    
    logging.info(f"Split: train={n_train}, gap={gap}, test={len(X_test)}")
    
    return X_train, X_test, y_train, y_test
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- "Cross Validation in Finance: Purging, Embargoing" (QuantInsti 2025)
- LÃ³pez de Prado (2018). "Advances in Financial Machine Learning"

---

#### 5. **Ø¹Ø¯Ù… Test Set Validation** âš¡

**Ù…Ø´Ú©Ù„:**
```python
# Ø¨Ø¹Ø¯ Ø§Ø² feature selection:
nested_cv_results = self.nested_cross_validation(X_train, y_train)
# âŒ X_test Ø§ØµÙ„Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯!
```

**Ú†Ø±Ø§ ÙØ§Ø¬Ø¹Ù‡â€ŒØ¨Ø§Ø±:**
- **Ù†Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒØ¯ ÙÛŒÚ†Ø±Ù‡Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ú†Ù‚Ø¯Ø± Ø®ÙˆØ¨ Ù‡Ø³ØªÙ†Ø¯!**
- Ù…Ù…Ú©Ù† Ø§Ø³Øª overfitting Ø´Ø¯ÛŒØ¯ Ø¨Ù‡ train set Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
- Ø¨Ø±Ø§ÛŒ trading: Ø§ÛŒÙ† Ù…Ø¹Ø§Ø¯Ù„ **Ø¹Ø¯Ù… ØªØ³Øª Ø¯Ø± Ù…Ø­ÛŒØ· ÙˆØ§Ù‚Ø¹ÛŒ** Ø§Ø³Øª

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def process_batch_with_test_validation(X_train, X_test, y_train, y_test):
    # 1. Feature selection Ø±ÙˆÛŒ train
    selected_features = feature_selection(X_train, y_train)
    
    # 2. Nested CV Ø±ÙˆÛŒ train
    train_performance = nested_cv(X_train[selected_features], y_train)
    
    # 3. âœ… VALIDATION Ø±ÙˆÛŒ TEST SET
    final_model = train_final_model(X_train[selected_features], y_train)
    test_performance = evaluate(final_model, X_test[selected_features], y_test)
    
    # 4. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ùˆ ØªØ´Ø®ÛŒØµ overfitting
    gap = train_performance - test_performance
    if gap > 0.05:  # 5% threshold
        logging.warning(f"âš ï¸ OVERFITTING: gap={gap:.4f}")
    
    return {
        'train': train_performance,
        'test': test_performance,
        'gap': gap,
        'selected_features': selected_features
    }
```

---

### Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø§Ù…Ø§ Ú©Ù…ÛŒ Ú©Ù…â€ŒØ§ÙˆÙ„ÙˆÛŒØªâ€ŒØªØ± (6-10)

#### 6. **COMBINATORIAL PURGED CV - Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ 2025**

**Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ:
TimeSeriesSplit(n_splits=3)  # âŒ ÙÙ‚Ø· ÛŒÚ© Ù…Ø³ÛŒØ±!
```

**Ú†Ø±Ø§ Ù…Ù‡Ù…:**
- Walk-forward ÙÙ‚Ø· **ÛŒÚ© Ù…Ø³ÛŒØ± ØªØ§Ø±ÛŒØ®ÛŒ** Ø±Ø§ test Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- High variance Ø¯Ø± performance
- CPCV = Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ finance 2024-2025

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
class CombinatorialPurgedCV:
    """
    Multiple paths + Purging + Embargo
    
    Ù…Ø«Ø§Ù„: Ø§Ø² 10 folds, test 2 folds â†’ C(10,2) = 45 paths
    """
    
    def __init__(self, n_splits=10, n_test_splits=2, embargo_pct=0.01):
        self.n_splits = n_splits
        self.n_test_splits = n_test_splits
        self.embargo_pct = embargo_pct
    
    def split(self, X, y):
        from itertools import combinations
        
        # Split Ø¨Ù‡ groups
        groups = self._create_groups(X, self.n_splits)
        
        # Ù‡Ù…Ù‡ combinations Ø¨Ø±Ø§ÛŒ test
        test_combos = combinations(range(self.n_splits), self.n_test_splits)
        
        for test_groups in test_combos:
            train_idx = self._get_train_indices(groups, test_groups)
            test_idx = self._get_test_indices(groups, test_groups)
            
            # Apply purging & embargo
            train_idx = self._purge_and_embargo(train_idx, test_idx, len(X))
            
            yield train_idx, test_idx
```

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… 45 paths Ø¨Ù‡ Ø¬Ø§ÛŒ 1 path
- âœ… Distribution of performance
- âœ… Robust statistical inference
- âœ… Purging Ø¨Ø±Ø§ÛŒ overlapping labels
- âœ… Embargo Ø¨Ø±Ø§ÛŒ autocorrelated features

**Ù…Ù†Ø§Ø¨Ø¹:**
- "Backtest Overfitting in the Machine Learning Era" (2024)
- LÃ³pez de Prado (2018)
- Wikipedia: "Purged cross-validation" (2025)

---

#### 7. **Data Leakage Ø¯Ø± Preprocessing**

**Ù…Ø´Ú©Ù„:**
```python
def preprocess_features(self, X_train, y_train):
    # âœ… ÙÙ‚Ø· Ø±ÙˆÛŒ train fit Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø®ÙˆØ¨ Ø§Ø³Øª)
    # âŒ Ø§Ù…Ø§ transform Ø¨Ø±Ø§ÛŒ test Ú†Ø·ÙˆØ±ØŸ
```

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
class FeatureSelector:
    def __init__(self):
        self.fitted_preprocessors_ = {}
    
    def fit_preprocessors(self, X_train):
        """Fit ÙÙ‚Ø· Ø±ÙˆÛŒ train"""
        from sklearn.preprocessing import StandardScaler
        from sklearn.impute import SimpleImputer
        
        self.fitted_preprocessors_['imputer'] = SimpleImputer(strategy='median')
        self.fitted_preprocessors_['imputer'].fit(X_train)
        
        X_imputed = self.fitted_preprocessors_['imputer'].transform(X_train)
        
        self.fitted_preprocessors_['scaler'] = StandardScaler()
        self.fitted_preprocessors_['scaler'].fit(X_imputed)
        
        return self
    
    def transform_safe(self, X):
        """Transform Ø¨Ø§ fitted preprocessors"""
        X_transformed = self.fitted_preprocessors_['imputer'].transform(X)
        X_transformed = self.fitted_preprocessors_['scaler'].transform(X_transformed)
        return X_transformed

# Ø§Ø³ØªÙØ§Ø¯Ù‡:
selector.fit_preprocessors(X_train)
X_train_transformed = selector.transform_safe(X_train)
X_test_transformed = selector.transform_safe(X_test)  # âœ… No leakage!
```

---

#### 8. **Overfitting Ø¯Ø± Stability Selection**

**Ù…Ø´Ú©Ù„:**
```python
stability_selection(n_iterations=100, sample_fraction=0.5)  # âŒ Ø®ÛŒÙ„ÛŒ Ú©Ù…!
```

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def stability_selection_improved(X, y, 
                                 n_iterations=100,
                                 sample_fraction=0.7,  # âœ… Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ù‡ 70%
                                 stratify=True):
    """
    Ø¨Ø§ bootstrap (replacement=True) Ùˆ stratification
    """
    
    for iteration in range(n_iterations):
        # Bootstrap sampling
        if stratify and is_classification:
            # Stratified sampling
            sample_idx = stratified_sample(y, sample_fraction)
        else:
            sample_idx = rng.choice(len(X), size=int(len(X)*sample_fraction), 
                                   replace=True)  # âœ… bootstrap
        
        X_boot = X.iloc[sample_idx]
        y_boot = y.iloc[sample_idx]
        
        # Train & select
        model = train_model(X_boot, y_boot)
        selected_features = get_top_features(model)
        selection_counts[selected_features] += 1
    
    # Adaptive threshold Ø¨Ø§ FDR control
    selection_prob = selection_counts / n_iterations
    threshold = adaptive_threshold(n_features, n_iterations, target_fdr=0.05)
    
    stable_features = selection_prob >= threshold
    
    # FDR estimation
    expected_fdr = estimate_fdr(selection_prob, threshold, n_features)
    
    return {
        'stable_features': stable_features,
        'selection_prob': selection_prob,
        'expected_fdr': expected_fdr
    }
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- Meinshausen & BÃ¼hlmann (2010). "Stability selection"
- Shah & Samworth (2013). "Variable selection with error control"

---

#### 9. **SHAP Ø¨Ø§ Multicollinearity**

**Ù…Ø´Ú©Ù„:**
```python
explainer = shap.TreeExplainer(model, feature_perturbation='tree_path_dependent')
# âŒ Ø¨Ø±Ø§ÛŒ correlated features Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª!
```

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def shap_analysis_robust(X, y):
    """SHAP Ø¨Ø§ multicollinearity awareness"""
    
    # 1. ØªØ´Ø®ÛŒØµ correlation
    corr_matrix = X.corr().abs()
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > 0.8:
                high_corr_pairs.append((corr_matrix.columns[i], 
                                       corr_matrix.columns[j],
                                       corr_matrix.iloc[i, j]))
    
    # 2. Ø§Ù†ØªØ®Ø§Ø¨ method Ù…Ù†Ø§Ø³Ø¨
    if len(high_corr_pairs) > 0:
        logging.warning(f"High correlation detected: {len(high_corr_pairs)} pairs")
        feature_perturbation = 'interventional'  # âœ… Ø¨Ø±Ø§ÛŒ correlated
    else:
        feature_perturbation = 'tree_path_dependent'
    
    # 3. Ù…Ø­Ø§Ø³Ø¨Ù‡ SHAP Ø¨Ø§ multiple runs
    shap_values_list = []
    
    for run in range(10):
        model = train_model_bootstrap(X, y, seed=run)
        explainer = shap.TreeExplainer(
            model,
            data=X,  # background
            feature_perturbation=feature_perturbation
        )
        shap_values = explainer.shap_values(X)
        shap_values_list.append(np.abs(shap_values))
    
    # 4. Aggregation
    shap_mean = np.mean(np.mean(shap_values_list, axis=1), axis=0)
    shap_std = np.std(np.mean(shap_values_list, axis=1), axis=0)
    shap_cv = shap_std / (shap_mean + 1e-6)  # coefficient of variation
    
    return {
        'shap_mean': shap_mean,
        'shap_cv': shap_cv,  # stability metric
        'high_corr_detected': len(high_corr_pairs) > 0
    }
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions"
- Aas et al. (2021). "Explaining predictions when features are dependent"

---

#### 10. **Early Stopping Leakage**

**Ù…Ø´Ú©Ù„:**
```python
# Validation set Ø§Ø² Ø¢Ø®Ø± train
n_val = int(0.15 * len(X_train))
X_val = X_train.iloc[-n_val:]  # âŒ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ test!

model = lgb.train(valid_sets=[val_data], 
                 callbacks=[lgb.early_stopping(50)])  # âŒ
```

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def train_with_proper_early_stopping(X_train, y_train, gap=24):
    """Strategy 1: Time-based split Ø¨Ø§ gap"""
    
    n_total = len(X_train)
    n_tr = int(n_total * 0.7)
    
    X_tr = X_train.iloc[:n_tr]
    y_tr = y_train.iloc[:n_tr]
    
    # Gap
    val_start = n_tr + gap
    X_val = X_train.iloc[val_start:]
    y_val = y_train.iloc[val_start:]
    
    train_data = lgb.Dataset(X_tr, label=y_tr)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    
    model = lgb.train(
        params, train_data,
        num_boost_round=1000,
        valid_sets=[val_data],
        callbacks=[lgb.early_stopping(50)]
    )
    
    return model

# ÛŒØ§ Strategy 2: Inner CV Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† optimal iterations
def train_with_cv_iterations(X_train, y_train):
    """Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² inner CV"""
    
    best_iterations = []
    
    for train_idx, val_idx in TimeSeriesSplit(3).split(X_train):
        X_tr = X_train.iloc[train_idx]
        y_tr = y_train.iloc[train_idx]
        X_val = X_train.iloc[val_idx]
        y_val = y_train.iloc[val_idx]
        
        model_cv = lgb.train(..., early_stopping(50))
        best_iterations.append(model_cv.best_iteration)
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² median
    optimal_iter = int(np.median(best_iterations))
    
    # Train Ø±ÙˆÛŒ Ú©Ù„ train Ø¨Ø§ iterations Ø«Ø§Ø¨Øª
    model = lgb.train(params, train_data, num_boost_round=optimal_iter)
    
    return model
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- "Data leakage by early stopping" (Reddit ML 2024)
- "mlr3 book: Validation and Internal Tuning" (2023)

---

### Ù…Ù‡Ù… Ø§Ù…Ø§ Ú©Ù…ØªØ± Ø¨Ø­Ø±Ø§Ù†ÛŒ (11-15)

#### 11. **ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ Ø¯Ø± Ensemble**

**Ù…Ø´Ú©Ù„:**
```python
weights = {
    'null_z': 0.08,
    'shap': 0.08,
    # ... Ø¯Ù„Ø¨Ø®ÙˆØ§Ù‡ÛŒ!
}
```

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def ensemble_ranking_adaptive(feature_names, **importance_dicts):
    """ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ data-driven"""
    
    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ metrics
    df_metrics = pd.DataFrame({
        f"{name}_{key}": normalize(values)
        for name, imp_dict in importance_dicts.items()
        for key, values in imp_dict.items()
    }, index=feature_names)
    
    # Ø­Ø°Ù highly correlated metrics
    corr_matrix = df_metrics.corr().abs()
    redundant = set()
    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix)):
            if corr_matrix.iloc[i, j] > 0.95:
                # Ø­Ø°Ù Ø¨Ø§ variance Ú©Ù…ØªØ±
                var_i = df_metrics.iloc[:, i].var()
                var_j = df_metrics.iloc[:, j].var()
                redundant.add(corr_matrix.columns[i] if var_i < var_j 
                             else corr_matrix.columns[j])
    
    df_metrics = df_metrics.drop(columns=list(redundant))
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ variance Ùˆ discrimination
    metric_weights = {}
    for col in df_metrics.columns:
        variance_score = df_metrics[col].var()
        q75 = df_metrics[col].quantile(0.75)
        q25 = df_metrics[col].quantile(0.25)
        discrimination_score = q75 - q25
        
        metric_weights[col] = variance_score * discrimination_score
    
    # Normalize
    total = sum(metric_weights.values())
    metric_weights = {k: v/total for k, v in metric_weights.items()}
    
    # Final score
    final_scores = sum(df_metrics[col] * weight 
                      for col, weight in metric_weights.items())
    
    return pd.DataFrame({
        'feature': feature_names,
        'final_score': final_scores
    }).sort_values('final_score', ascending=False)
```

---

#### 12. **Hyperparameter Tuning Ù†Ø§Ú©Ø§ÙÛŒ**

**Ø±Ø§Ù‡Ú©Ø§Ø± Ø¨Ø§ Optuna:**
```python
def hyperparameter_tuning_optuna(X_train, y_train, n_trials=50):
    """Automated hyperparameter tuning"""
    import optuna
    
    def objective(trial):
        params = {
            'num_leaves': trial.suggest_int('num_leaves', 20, 150),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),
            'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),
            'lambda_l2': trial.suggest_float('lambda_l2', 0, 10)
        }
        
        # TimeSeriesSplit CV
        scores = []
        for train_idx, val_idx in TimeSeriesSplit(3).split(X_train):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
            
            model = lgb.train(params, lgb.Dataset(X_tr, y_tr), num_boost_round=300)
            y_pred = model.predict(X_val)
            score = roc_auc_score(y_val, y_pred)
            scores.append(score)
        
        return np.mean(scores)
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)
    
    logging.info(f"Best params: {study.best_params}")
    logging.info(f"Best score: {study.best_value:.4f}")
    
    return study.best_params
```

---

#### 13. **Group-Based Splitting Ø¨Ø±Ø§ÛŒ Autocorrelation**

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def detect_autocorrelation(X, acf_threshold=0.3):
    """ØªØ´Ø®ÛŒØµ features Ø¨Ø§ high ACF"""
    from statsmodels.tsa.stattools import acf
    
    high_acf_features = []
    acf_values = {}
    
    for col in X.columns:
        try:
            acf_result = acf(X[col].dropna(), nlags=50, fft=True)
            max_acf = np.max(np.abs(acf_result[1:]))
            acf_values[col] = max_acf
            
            if max_acf > acf_threshold:
                high_acf_features.append(col)
        except:
            continue
    
    if len(high_acf_features) > 0:
        logging.warning(
            f"âš ï¸ {len(high_acf_features)} features have high autocorrelation. "
            f"Consider larger gaps in CV."
        )
    
    return high_acf_features, acf_values

def recommend_gap_size(X, y, acf_threshold=0.3):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ gap Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø± Ø§Ø³Ø§Ø³ ACF"""
    
    high_acf_features, _ = detect_autocorrelation(X, acf_threshold)
    
    if len(high_acf_features) == 0:
        return 24  # default
    
    # ÛŒØ§ÙØªÙ† max lag Ø¨Ø§ ACF > threshold
    max_lags = []
    for feat in high_acf_features[:10]:
        acf_result = acf(X[feat].dropna(), nlags=100, fft=True)
        lags_above = np.where(np.abs(acf_result[1:]) > acf_threshold)[0]
        if len(lags_above) > 0:
            max_lags.append(lags_above[-1] + 1)
    
    if max_lags:
        recommended_gap = int(np.max(max_lags) * 2)  # 2x safety
        logging.info(f"Recommended gap: {recommended_gap}")
        return recommended_gap
    
    return 24
```

---

#### 14. **Sample Weights Leakage**

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def cv_with_proper_sample_weights(X, y, n_splits=5):
    """Sample weights per fold"""
    
    cpcv = CombinatorialPurgedCV(n_splits=n_splits)
    scores = []
    
    for train_idx, val_idx in cpcv.split(X, y):
        X_train_fold = X.iloc[train_idx]
        y_train_fold = y.iloc[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_val_fold = y.iloc[val_idx]
        
        # âœ… Ù…Ø­Ø§Ø³Ø¨Ù‡ weights ÙÙ‚Ø· Ø§Ø² Ø§ÛŒÙ† fold
        from sklearn.utils.class_weight import compute_sample_weight
        sample_weights = compute_sample_weight('balanced', y=y_train_fold)
        
        # Train Ø¨Ø§ weights Ø§ÛŒÙ† fold
        train_data = lgb.Dataset(X_train_fold, y_train_fold, weight=sample_weights)
        model = lgb.train(params, train_data, num_boost_round=300)
        
        # Evaluate
        y_pred = model.predict(X_val_fold)
        score = roc_auc_score(y_val_fold, y_pred)
        scores.append(score)
    
    return np.mean(scores), np.std(scores)
```

---

#### 15. **Ù†Ø¨ÙˆØ¯ Statistical Testing**

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def statistical_significance_testing(X, y, feature_pairs, n_bootstrap=1000):
    """Bootstrap hypothesis testing Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ features"""
    
    results = []
    
    for feat1, feat2 in feature_pairs:
        performance_diff = []
        
        for b in range(n_bootstrap):
            # Bootstrap sample
            boot_idx = rng.choice(len(X), size=len(X), replace=True)
            oob_idx = np.setdiff1d(np.arange(len(X)), boot_idx)
            
            # Train models
            model1 = train_model(X[[feat1]].iloc[boot_idx], y.iloc[boot_idx])
            model2 = train_model(X[[feat2]].iloc[boot_idx], y.iloc[boot_idx])
            
            # Evaluate on OOB
            score1 = evaluate(model1, X[[feat1]].iloc[oob_idx], y.iloc[oob_idx])
            score2 = evaluate(model2, X[[feat2]].iloc[oob_idx], y.iloc[oob_idx])
            
            performance_diff.append(score1 - score2)
        
        # Statistical test
        mean_diff = np.mean(performance_diff)
        ci_lower = np.percentile(performance_diff, 2.5)
        ci_upper = np.percentile(performance_diff, 97.5)
        p_value = 2 * min(np.mean(performance_diff >= 0), 
                         np.mean(performance_diff <= 0))
        
        results.append({
            'feature1': feat1,
            'feature2': feat2,
            'mean_diff': mean_diff,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'p_value': p_value,
            'is_significant': p_value < 0.05
        })
    
    return pd.DataFrame(results)
```

---

### Ù…Ø´Ú©Ù„Ø§Øª Ø¯ÙˆØ± Ú†Ù‡Ø§Ø±Ù… (16-20)

#### 16. **Multiple Testing Correction - FDR Control**

**Ù…Ø´Ú©Ù„:**
- ÙˆÙ‚ØªÛŒ 1000 feature test Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ Ø¨Ø§ Î±=0.05
- Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø±ÛŒØ¯ **50 feature** Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ significant Ø¨Ø§Ø´Ù†Ø¯!
- Ø§ÛŒÙ† "feature selection by chance" Ø§Ø³Øª

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def feature_selection_with_fdr_control(X, y, method='BH', target_fdr=0.05):
    """
    Feature selection Ø¨Ø§ False Discovery Rate control
    
    Methods:
    - 'bonferroni': Ø®ÛŒÙ„ÛŒ conservative
    - 'BH': Benjamini-Hochberg (recommended)
    - 'BY': Benjamini-Yekutieli (Ø¨Ø±Ø§ÛŒ dependent tests)
    """
    from statsmodels.stats.multitest import multipletests
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ p-value Ø¨Ø±Ø§ÛŒ Ù‡Ø± feature
    p_values = []
    
    for col in X.columns:
        # Test feature importance (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø§ permutation test)
        _, p_val = permutation_test_feature(X[col], y)
        p_values.append(p_val)
    
    # FDR correction
    reject, pvals_corrected, _, _ = multipletests(
        p_values,
        alpha=target_fdr,
        method=method  # 'bonferroni' or 'fdr_bh' or 'fdr_by'
    )
    
    selected_features = X.columns[reject].tolist()
    
    # Expected FDR
    n_selected = sum(reject)
    expected_false_discoveries = n_selected * target_fdr
    
    logging.info(f"FDR control ({method}):")
    logging.info(f"  Selected: {n_selected}/{len(X.columns)}")
    logging.info(f"  Expected false discoveries: {expected_false_discoveries:.1f}")
    logging.info(f"  FDR: {expected_false_discoveries/max(n_selected,1):.2%}")
    
    return {
        'selected_features': selected_features,
        'p_values': p_values,
        'p_values_corrected': pvals_corrected,
        'expected_fdr': expected_false_discoveries / max(n_selected, 1)
    }

def permutation_test_feature(X_feature, y, n_permutations=100):
    """Permutation test Ø¨Ø±Ø§ÛŒ feature importance"""
    
    # Train model Ø¨Ø§ feature
    model = train_simple_model(X_feature.values.reshape(-1, 1), y)
    original_score = evaluate_model(model, X_feature.values.reshape(-1, 1), y)
    
    # Permutation scores
    perm_scores = []
    for _ in range(n_permutations):
        y_perm = np.random.permutation(y)
        model_perm = train_simple_model(X_feature.values.reshape(-1, 1), y_perm)
        perm_score = evaluate_model(model_perm, X_feature.values.reshape(-1, 1), y_perm)
        perm_scores.append(perm_score)
    
    # p-value
    p_value = np.mean([s >= original_score for s in perm_scores])
    
    return original_score, p_value
```

**Ø§Ù‡Ù…ÛŒØª:**
- Ø¨Ø¯ÙˆÙ† FDR controlØŒ Ø¯Ø± 1000 features:
  - Ø¨Ø§ Î±=0.05 â†’ expect 50 false positives!
  - Ø¨Ø§ Bonferroni: Î±_corrected = 0.05/1000 = 0.00005 (Ø®ÛŒÙ„ÛŒ conservative)
  - Ø¨Ø§ BH (FDR): balance Ø¨ÛŒÙ† power Ùˆ false positives

**Ù…Ù†Ø§Ø¨Ø¹:**
- Benjamini & Hochberg (1995). "Controlling the false discovery rate"
- "Bon-EV: improved multiple testing for FDR" (PMC 2017)
- "MultipleTesting.com" (PMC 2021)

---

#### 17. **Data Snooping Bias & Probability of Backtest Overfitting**

**Ù…Ø´Ú©Ù„:**
- Ø´Ù…Ø§ 100 Ù…Ø¯Ù„ Ù…Ø®ØªÙ„Ù test Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
- Ø¨Ù‡ØªØ±ÛŒÙ† Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
- Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ **by luck** Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³ØªØŒ Ù†Ù‡ **by skill**!

**Probability of Backtest Overfitting (PBO):**

```python
def calculate_pbo(strategies_performance):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ PBO Ø·Ø¨Ù‚ Bailey & LÃ³pez de Prado (2015)
    
    Args:
        strategies_performance: dict with keys 'IS' (in-sample) and 'OOS' (out-of-sample)
                               each containing performance of N strategies
    
    Returns:
        pbo: Ø§Ø­ØªÙ…Ø§Ù„ overfitting (0 to 1)
             PBO > 0.5 â†’ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ overfit Ø´Ø¯Ù‡!
    """
    
    IS_performance = np.array(strategies_performance['IS'])
    OOS_performance = np.array(strategies_performance['OOS'])
    
    N = len(IS_performance)
    
    # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ strategies Ø¨Ø± Ø§Ø³Ø§Ø³ IS
    IS_ranks = np.argsort(IS_performance)[::-1]  # descending
    
    # Ø¨Ù‡ØªØ±ÛŒÙ† strategy Ø¯Ø± IS
    best_IS_idx = IS_ranks[0]
    
    # OOS performance Ø§ÛŒÙ† strategy
    best_IS_OOS = OOS_performance[best_IS_idx]
    
    # Median OOS performance
    median_OOS = np.median(OOS_performance)
    
    # PBO = Ø§Ø­ØªÙ…Ø§Ù„ Ø§ÛŒÙ†Ú©Ù‡ best IS strategy < median OOS
    # (ÛŒØ¹Ù†ÛŒ overfitting Ø¯Ø§Ø±ÛŒÙ…)
    
    # Ø¨Ø±Ø§ÛŒ robust estimation: CSCV
    # (Combinatorially Symmetric Cross-Validation)
    
    pbo = calculate_pbo_cscv(IS_performance, OOS_performance)
    
    return pbo

def calculate_pbo_cscv(IS_perf, OOS_perf):
    """
    CSCV: ØªÙ…Ø§Ù… combinations Ø§Ø² splits Ø±Ø§ test Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    
    Ù…Ø«Ù„Ø§Ù‹: split data to S=16 groups
    Ø¨Ø±Ø§ÛŒ Ù‡Ø± combination Ø§Ø² 8 groups:
        - 8 groups = train (IS)
        - 8 groups = test (OOS)
    
    ØªØ¹Ø¯Ø§Ø¯ combinations: C(16, 8) = 12,870
    """
    from scipy.special import comb
    
    N = len(IS_perf)
    
    # Count: Ú†Ù†Ø¯ Ø¨Ø§Ø± best IS strategy < median OOS
    count_overfit = 0
    
    # Ø§ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø³Ù†Ú¯ÛŒÙ† Ù‡Ø³ØªÙ†Ø¯
    # Ø¯Ø± Ø¹Ù…Ù„: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² sampling Ø§Ø² combinations
    
    n_samples = min(1000, int(comb(N, N//2)))
    
    for _ in range(n_samples):
        # Random split
        indices = np.arange(N)
        np.random.shuffle(indices)
        IS_idx = indices[:N//2]
        OOS_idx = indices[N//2:]
        
        # Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯Ø± IS
        best_IS_in_this_split = IS_idx[np.argmax(IS_perf[IS_idx])]
        
        # OOS performance Ø§ÛŒÙ† strategy
        oos_of_best_is = OOS_perf[best_IS_in_this_split]
        
        # Median OOS
        median_oos = np.median(OOS_perf[OOS_idx])
        
        # Check overfitting
        if oos_of_best_is < median_oos:
            count_overfit += 1
    
    pbo = count_overfit / n_samples
    
    return pbo

# Ø§Ø³ØªÙØ§Ø¯Ù‡:
strategies_results = {
    'IS': [0.8, 0.7, 0.9, 0.6, ...],  # in-sample Sharpe ratios
    'OOS': [0.3, 0.5, 0.2, 0.4, ...]  # out-of-sample
}

pbo = calculate_pbo(strategies_results)

if pbo > 0.5:
    logging.error(f"âš ï¸ HIGH OVERFITTING RISK: PBO={pbo:.2f}")
    logging.error("The selected strategy likely won due to LUCK, not SKILL!")
elif pbo > 0.3:
    logging.warning(f"âš ï¸ MODERATE OVERFITTING: PBO={pbo:.2f}")
else:
    logging.info(f"âœ“ Low overfitting risk: PBO={pbo:.2f}")
```

**Deflated Sharpe Ratio (DSR):**

```python
def deflated_sharpe_ratio(estimated_sr, n_samples, n_trials, skewness=0, kurtosis=3):
    """
    DSR: Sharpe Ratio ØªØ¹Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ multiple testing
    
    Args:
        estimated_sr: Sharpe ratio Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´Ø¯Ù‡
        n_samples: ØªØ¹Ø¯Ø§Ø¯ samples (Ù…Ø«Ù„Ø§Ù‹ returns)
        n_trials: ØªØ¹Ø¯Ø§Ø¯ strategies test Ø´Ø¯Ù‡
        skewness: skewness of returns
        kurtosis: excess kurtosis of returns
    
    Returns:
        dsr: Deflated Sharpe Ratio
        psr: Probabilistic Sharpe Ratio
    """
    from scipy.stats import norm
    
    # Variance of Sharpe Ratio estimate
    var_sr = (1 + 0.5 * estimated_sr**2 - skewness * estimated_sr + 
             (kurtosis - 1) / 4 * estimated_sr**2) / n_samples
    
    # Adjustment Ø¨Ø±Ø§ÛŒ multiple testing
    # Ù…Ø·Ø§Ø¨Ù‚ Bailey & LÃ³pez de Prado (2014)
    
    euler_mascheroni = 0.5772156649
    
    # SR_0^star: threshold Ø¨Ø±Ø§ÛŒ multiple testing
    sr_star = np.sqrt(var_sr) * (
        (1 - euler_mascheroni) * norm.ppf(1 - 1/n_trials) + 
        euler_mascheroni * norm.ppf(1 - 1/(n_trials * np.e))
    )
    
    # Deflated Sharpe Ratio
    dsr = (estimated_sr - sr_star) / np.sqrt(var_sr)
    
    # Probabilistic Sharpe Ratio
    # Ø§Ø­ØªÙ…Ø§Ù„ Ø§ÛŒÙ†Ú©Ù‡ true SR > 0
    psr = norm.cdf(dsr)
    
    logging.info(f"Sharpe Ratio Analysis (n_trials={n_trials}):")
    logging.info(f"  Estimated SR: {estimated_sr:.4f}")
    logging.info(f"  SR threshold (adjusted): {sr_star:.4f}")
    logging.info(f"  Deflated SR: {dsr:.4f}")
    logging.info(f"  Probabilistic SR: {psr:.2%}")
    
    if psr < 0.95:
        logging.warning(f"âš ï¸ PSR < 95%: likely not significant after multiple testing")
    
    return {
        'deflated_sr': dsr,
        'probabilistic_sr': psr,
        'sr_threshold': sr_star,
        'var_sr': var_sr
    }

# Ù…Ø«Ø§Ù„:
# Ø´Ù…Ø§ 100 strategy test Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯
# Ø¨Ù‡ØªØ±ÛŒÙ† ÛŒÚ©ÛŒ SR=1.5 Ø¯Ø§Ø±Ø¯ Ø¨Ø§ 1000 returns

result = deflated_sharpe_ratio(
    estimated_sr=1.5,
    n_samples=1000,
    n_trials=100,
    skewness=-0.5,
    kurtosis=3.0
)

# Ø§Ú¯Ø± PSR < 95% â†’ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø®Ø§Ø·Ø± multiple testing Ø§Ø³Øª!
```

**Ø§Ù‡Ù…ÛŒØª:**
- PBO: Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ overfitting Ø±Ø§ Ù…ÛŒâ€ŒØ³Ù†Ø¬Ø¯
- DSR: Sharpe ratio Ø±Ø§ Ø¨Ø±Ø§ÛŒ multiple testing adjust Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- Ø§ÛŒÙ† Ø¯Ùˆ metric Ø¨Ø§ÛŒØ¯ **Ù‚Ø¨Ù„ Ø§Ø² production deployment** Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆÙ†Ø¯!

**Ù…Ù†Ø§Ø¨Ø¹:**
- Bailey & LÃ³pez de Prado (2015). "The Probability of Backtest Overfitting"
- Bailey & LÃ³pez de Prado (2014). "The Deflated Sharpe Ratio"
- "Overfitting & Data-Snooping in Backtests" (Surmount.ai 2025)

---

#### 18. **Adversarial Validation Ø¨Ø±Ø§ÛŒ Dataset Shift**

**Ù…Ø´Ú©Ù„:**
- Ø¢ÛŒØ§ train set Ùˆ test set Ø§Ø² **Ù‡Ù…Ø§Ù† distribution** Ù‡Ø³ØªÙ†Ø¯ØŸ
- Ø§Ú¯Ø± Ù†Ù‡ (dataset shift) â†’ Ù…Ø¯Ù„ fail Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø±Ø¯!

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def adversarial_validation(X_train, X_test):
    """
    ØªØ´Ø®ÛŒØµ dataset shift Ø¨Ø§ adversarial validation
    
    Ø§ÛŒØ¯Ù‡:
    - Label train=0, test=1
    - Train classifier
    - AUC â‰ˆ 0.5 â†’ similar distributions âœ“
    - AUC > 0.7 â†’ significant shift âš ï¸
    """
    
    # Label datasets
    X_train_labeled = X_train.copy()
    X_train_labeled['source'] = 0
    
    X_test_labeled = X_test.copy()
    X_test_labeled['source'] = 1
    
    # Combine
    X_combined = pd.concat([X_train_labeled, X_test_labeled], axis=0)
    y_source = X_combined['source']
    X_combined = X_combined.drop(columns=['source'])
    
    # Train classifier
    from sklearn.model_selection import cross_val_score
    from sklearn.ensemble import RandomForestClassifier
    
    clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
    
    # 5-fold CV
    auc_scores = cross_val_score(
        clf, X_combined, y_source,
        cv=5, scoring='roc_auc'
    )
    
    mean_auc = np.mean(auc_scores)
    
    # Interpretation
    if mean_auc < 0.55:
        status = "âœ“ EXCELLENT: Distributions very similar"
        color = "green"
    elif mean_auc < 0.65:
        status = "âœ“ GOOD: Mild shift, acceptable"
        color = "yellow"
    elif mean_auc < 0.75:
        status = "âš ï¸ WARNING: Moderate shift detected"
        color = "orange"
    else:
        status = "âŒ CRITICAL: Severe shift - model will likely fail!"
        color = "red"
    
    logging.info(f"Adversarial Validation AUC: {mean_auc:.4f} - {status}")
    
    # Feature importance: which features shifted most?
    clf.fit(X_combined, y_source)
    feature_importances = pd.Series(
        clf.feature_importances_,
        index=X_combined.columns
    ).sort_values(ascending=False)
    
    top_shifted_features = feature_importances.head(10)
    
    logging.info(f"Top 10 shifted features:")
    for feat, imp in top_shifted_features.items():
        logging.info(f"  {feat}: {imp:.4f}")
    
    return {
        'auc': mean_auc,
        'status': status,
        'shifted_features': top_shifted_features.to_dict(),
        'recommendation': 'RETRAIN' if mean_auc > 0.75 else 'OK'
    }

# Ø§Ø³ØªÙØ§Ø¯Ù‡:
result = adversarial_validation(X_train, X_test)

if result['recommendation'] == 'RETRAIN':
    logging.error("âŒ Train/Test distributions too different!")
    logging.error("Consider:")
    logging.error("  1. Using more recent training data")
    logging.error("  2. Re-sampling train set to match test")
    logging.error("  3. Feature engineering to reduce shift")
```

**Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§:**
1. **Ù‚Ø¨Ù„ Ø§Ø² training:** Ø¨Ø±Ø±Ø³ÛŒ train/test similarity
2. **Ø¯Ø± production:** monitoring Ø¨Ø±Ø§ÛŒ drift detection
3. **Feature debugging:** ÛŒØ§ÙØªÙ† features Ø¨Ø§ shift

**Ù…Ù†Ø§Ø¨Ø¹:**
- "Using Adversarial Validation for Drift Assessment" (APXML 2025)
- "Managing dataset shift by adversarial validation" (arXiv 2021)
- "Adversarial Learning for Feature Shift Detection" (NeurIPS 2023)

---

#### 19. **Label Leakage & Overlapping Labels**

**Ù…Ø´Ú©Ù„ Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ Time Series:**

```python
# Ù…Ø«Ø§Ù„: Label = return 5 bars ahead
df['label'] = df['close'].pct_change(5).shift(-5)

# Ø³Ø§Ø¹Øª 10:00 â†’ label based on price at 10:05
# Ø³Ø§Ø¹Øª 10:01 â†’ label based on price at 10:06
# ...

# Ø¯Ø± temporal CV:
# Train: 09:00-09:59
# Test:  10:00-10:05

# Ù…Ø´Ú©Ù„:
# - Label Ø¨Ø±Ø§ÛŒ 09:55 depends on price 10:00 (test set!)
# - Ø§ÛŒÙ† LEAKAGE Ø§Ø³Øª!
```

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def create_labels_with_awareness(df, label_horizon=5):
    """
    Ø³Ø§Ø®Øª labels Ø¨Ø§ awareness Ø§Ø² overlap
    
    Args:
        label_horizon: ØªØ¹Ø¯Ø§Ø¯ bars Ø¨Ø±Ø§ÛŒ forward return
    
    Returns:
        df: Ø¨Ø§ label Ùˆ metadata
        embargo_size: ØªØ¹Ø¯Ø§Ø¯ bars Ø¨Ø±Ø§ÛŒ embargo
    """
    
    # Create label
    df['label'] = df['close'].pct_change(label_horizon).shift(-label_horizon)
    
    # Metadata Ø¨Ø±Ø§ÛŒ purging/embargo
    df['label_start_time'] = df.index
    df['label_end_time'] = df.index.shift(-label_horizon)
    
    # Embargo size = label_horizon
    embargo_size = label_horizon
    
    logging.info(f"Labels created with horizon={label_horizon}")
    logging.info(f"âš ï¸ Embargo size should be at least {embargo_size} bars")
    
    return df, embargo_size

def temporal_split_with_label_awareness(X, y, df_meta, test_size=0.2, embargo_bars=0):
    """
    Split Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† overlapping labels
    """
    
    n = len(X)
    n_test = int(n * test_size)
    n_train = n - n_test - embargo_bars
    
    # Train
    X_train = X.iloc[:n_train].copy()
    y_train = y.iloc[:n_train].copy()
    
    # Embargo gap
    # (Ø­Ø°Ù samples Ú©Ù‡ label Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ test overlap Ø¯Ø§Ø±Ø¯)
    
    # Test
    X_test = X.iloc[n_train + embargo_bars:].copy()
    y_test = y.iloc[n_train + embargo_bars:].copy()
    
    # Check: Ø¢ÛŒØ§ Ù‡ÛŒÚ† label Ø§Ø² train Ø¨Ù‡ test overlap Ù†Ø¯Ø§Ø±Ø¯ØŸ
    last_train_time = df_meta.iloc[n_train - 1]['label_end_time']
    first_test_time = df_meta.iloc[n_train + embargo_bars]['label_start_time']
    
    if last_train_time >= first_test_time:
        logging.error(f"âš ï¸ LABEL OVERLAP: last train label ends at {last_train_time}, "
                     f"but first test starts at {first_test_time}")
        logging.error(f"Increase embargo_bars to at least {embargo_bars + 10}")
    else:
        logging.info(f"âœ“ No label overlap: gap = {(first_test_time - last_train_time).total_seconds() / 3600:.1f} hours")
    
    return X_train, X_test, y_train, y_test
```

**Ù…Ù†Ø§Ø¨Ø¹:**
- LÃ³pez de Prado (2018). "Advances in Financial Machine Learning" - Chapter 7
- "Don't Push the Button! Exploring Data Leakage" (arXiv 2024)
- "Date Train Test Leakage Overlap" (Deepchecks 2021)

---

#### 20. **Minimum Track Record Length (MinTRL)**

**Ù…Ø´Ú©Ù„:**
- ÛŒÚ© strategy Ø¨Ø§ SR=2.0 Ø¯Ø± 100 trades
- Ø¢ÛŒØ§ Ø§ÛŒÙ† **statistically significant** Ø§Ø³ØªØŸ
- ÛŒØ§ ÙÙ‚Ø· **luck**ØŸ

**Ø±Ø§Ù‡Ú©Ø§Ø±:**
```python
def minimum_track_record_length(estimated_sr, target_sr=0, prob=0.95, 
                                skewness=0, kurtosis=3):
    """
    MinTRL: Ø­Ø¯Ø§Ù‚Ù„ ØªØ¹Ø¯Ø§Ø¯ samples Ø¨Ø±Ø§ÛŒ Ø§Ø«Ø¨Ø§Øª SR > target_SR
    
    Ù…Ø·Ø§Ø¨Ù‚ Bailey & LÃ³pez de Prado (2012)
    
    Args:
        estimated_sr: Sharpe ratio Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´Ø¯Ù‡
        target_sr: threshold Ø¨Ø±Ø§ÛŒ comparison (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 0)
        prob: Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 0.95)
        skewness: skewness of returns
        kurtosis: excess kurtosis
    
    Returns:
        min_trl: Ø­Ø¯Ø§Ù‚Ù„ ØªØ¹Ø¯Ø§Ø¯ samples Ù„Ø§Ø²Ù…
    """
    from scipy.stats import norm
    
    # Variance of SR under non-normal returns
    var_sr = (1 + 0.5 * estimated_sr**2 - skewness * estimated_sr + 
             (kurtosis - 1) / 4 * estimated_sr**2)
    
    # MinTRL formula
    z_score = norm.ppf(prob)
    
    min_trl = var_sr * (z_score / (estimated_sr - target_sr))**2
    
    logging.info(f"Minimum Track Record Length Analysis:")
    logging.info(f"  Estimated SR: {estimated_sr:.4f}")
    logging.info(f"  Target SR: {target_sr:.4f}")
    logging.info(f"  Confidence: {prob:.1%}")
    logging.info(f"  MinTRL: {min_trl:.0f} samples")
    
    return {
        'min_trl': min_trl,
        'var_sr': var_sr
    }

# Ù…Ø«Ø§Ù„:
# Ø´Ù…Ø§ SR=1.5 Ø¯Ø§Ø±ÛŒØ¯ Ø¨Ø§ 500 returns
# Ø¢ÛŒØ§ Ú©Ø§ÙÛŒ Ø§Ø³ØªØŸ

result = minimum_track_record_length(
    estimated_sr=1.5,
    target_sr=0.0,
    prob=0.95,
    skewness=-0.3,
    kurtosis=2.0
)

n_samples_available = 500

if n_samples_available >= result['min_trl']:
    logging.info(f"âœ“ Track record sufficient: {n_samples_available} >= {result['min_trl']:.0f}")
else:
    deficit = result['min_trl'] - n_samples_available
    logging.warning(f"âš ï¸ Track record insufficient: need {deficit:.0f} more samples")
    logging.warning(f"Current results may be due to LUCK, not SKILL!")
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**
- Ù‚Ø¨Ù„ Ø§Ø² deployment: Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ track record Ú©Ø§ÙÛŒ Ø§Ø³Øª
- Ø¨Ø±Ø§ÛŒ live trading: Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø­Ø¯Ø§Ù‚Ù„ X Ù…Ø§Ù‡ track record

---

## ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ù…Ù„

| # | Ù…Ø´Ú©Ù„ | Ø³Ø·Ø­ Ø®Ø·Ø± | Ø²Ù…Ø§Ù† Ø±ÙØ¹ | Ø§ÙˆÙ„ÙˆÛŒØª | Ø¯ÙˆØ± |
|---|------|---------|----------|--------|------|
| 1 | Lookahead Bias Ø¯Ø± Features | ğŸ”´ CRITICAL | 10min | 1 | 3 |
| 2 | Forward/Backward Fill Leakage | ğŸ”´ CRITICAL | 15min | 1 | 2 |
| 3 | Nested CV Feature Selection Leakage | ğŸ”´ CRITICAL | 30min | 1 | 3 |
| 4 | Temporal Split Ø¨Ø¯ÙˆÙ† Gap | ğŸ”´ CRITICAL | 15min | 1 | 1 |
| 5 | Ø¹Ø¯Ù… Test Set Validation | ğŸ”´ CRITICAL | 20min | 1 | 1 |
| 6 | Combinatorial Purged CV | ğŸ”´ CRITICAL | 2h | 2 | 3 |
| 7 | Data Leakage Ø¯Ø± Preprocessing | ğŸ”´ CRITICAL | 45min | 2 | 1 |
| 8 | Overfitting Ø¯Ø± Stability Selection | ğŸ”´ CRITICAL | 30min | 2 | 1 |
| 9 | SHAP Ø¨Ø§ Multicollinearity | ğŸ”´ CRITICAL | 1h | 2 | 1 |
| 10 | Early Stopping Leakage | ğŸ”´ CRITICAL | 30min | 2 | 3 |
| 11 | ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ Ensemble | ğŸŸ¡ HIGH | 45min | 3 | 1 |
| 12 | Hyperparameter Tuning Ù†Ø§Ú©Ø§ÙÛŒ | ğŸŸ¡ HIGH | 1h | 3 | 1 |
| 13 | Group-Based Splitting (ACF) | ğŸŸ¡ HIGH | 45min | 3 | 3 |
| 14 | Sample Weights Leakage | ğŸŸ¡ HIGH | 20min | 3 | 3 |
| 15 | Ù†Ø¨ÙˆØ¯ Statistical Testing | ğŸŸ¡ HIGH | 1h | 3 | 1 |
| 16 | Multiple Testing Correction (FDR) | ğŸ”´ CRITICAL | 30min | 2 | 4 |
| 17 | PBO & Data Snooping | ğŸ”´ CRITICAL | 1h | 2 | 4 |
| 18 | Adversarial Validation | ğŸŸ¡ HIGH | 30min | 3 | 4 |
| 19 | Label Leakage & Overlapping | ğŸ”´ CRITICAL | 45min | 2 | 4 |
| 20 | Minimum Track Record Length | ğŸŸ¢ MEDIUM | 20min | 4 | 4 |

**Ø¬Ù…Ø¹ Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ: 16-20 Ø³Ø§Ø¹Øª**

---

## ğŸ¯ Ù¾Ù„Ù† Ø§Ø¬Ø±Ø§ÛŒÛŒ (Action Plan)

### ÙØ§Ø² 1: ÙÙˆØ±ÛŒ (Ø±ÙˆØ² Ø§ÙˆÙ„ - 2 Ø³Ø§Ø¹Øª)

**Ù‡Ø¯Ù:** Ø±ÙØ¹ TOP 5 Ù…Ø´Ú©Ù„Ø§Øª Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ù‡ disaster Ù…Ù†Ø¬Ø± Ø´ÙˆÙ†Ø¯

```python
# 1. Lookahead Bias Validation (10min)
validate_no_lookahead_bias(X)

# 2. Fix Forward/Backward Fill (15min)
X_safe = X.fillna(method='ffill')  # ÙÙ‚Ø· forward

# 3. Temporal Split Ø¨Ø§ Gap (15min)
X_train, X_test, y_train, y_test = temporal_split_with_gap(X, y, gap=24)

# 4. Test Set Validation (20min)
test_performance = evaluate_on_test_set(final_model, X_test, y_test)
if train_performance - test_performance > 0.05:
    logging.error("OVERFITTING DETECTED!")

# 5. FDR Control (30min)
selected_features = feature_selection_with_fdr_control(X_train, y_train, target_fdr=0.05)
```

### ÙØ§Ø² 2: Ù…Ù‡Ù… (Ø±ÙˆØ² Ø¯ÙˆÙ… - 5 Ø³Ø§Ø¹Øª)

**Ù‡Ø¯Ù:** Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª architectural

```python
# 6. Nested CV Ø¨Ø§ Feature Selection ØµØ­ÛŒØ­ (1h)
results = nested_cv_with_proper_feature_selection(X, y)

# 7. Preprocessing Ø¨Ø¯ÙˆÙ† Leakage (45min)
selector = FeatureSelector()
selector.fit_preprocessors(X_train)
X_train_safe = selector.transform_safe(X_train)
X_test_safe = selector.transform_safe(X_test)

# 8. SHAP Ø¨Ø§ Multicollinearity (1h)
shap_results = shap_analysis_robust(X, y)

# 9. Combinatorial Purged CV (2h)
cpcv_results = nested_cv_with_cpcv(X, y, n_splits=10, embargo_pct=0.01)
```

### ÙØ§Ø² 3: Ø¨Ù‡Ø¨ÙˆØ¯ (Ø±ÙˆØ² Ø³ÙˆÙ… - 4 Ø³Ø§Ø¹Øª)

**Ù‡Ø¯Ù:** Ø§ÙØ²Ø§ÛŒØ´ reliability

```python
# 10. Stability Selection Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ (30min)
stable_features = stability_selection_improved(X, y, sample_fraction=0.7, stratify=True)

# 11. Early Stopping ØµØ­ÛŒØ­ (30min)
model = train_with_proper_early_stopping(X_train, y_train, gap=24)

# 12. Autocorrelation Detection (45min)
gap_size = recommend_gap_size(X, y)

# 13. Hyperparameter Tuning (1h)
best_params = hyperparameter_tuning_optuna(X_train, y_train, n_trials=50)

# 14. Ensemble Ranking Adaptive (45min)
df_ranking = ensemble_ranking_adaptive(feature_names, **importance_dicts)
```

### ÙØ§Ø² 4: Ø¢Ø®Ø±ÛŒÙ† Ú©Ù†ØªØ±Ù„â€ŒÙ‡Ø§ (Ø±ÙˆØ² Ú†Ù‡Ø§Ø±Ù… - 4 Ø³Ø§Ø¹Øª)

**Ù‡Ø¯Ù:** Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¢Ù…Ø§Ø¯Ú¯ÛŒ production

```python
# 15. PBO Ù…Ø­Ø§Ø³Ø¨Ù‡ (1h)
pbo = calculate_pbo(strategies_results)
if pbo > 0.5:
    raise ValueError("HIGH OVERFITTING RISK - DO NOT DEPLOY!")

# 16. Deflated Sharpe Ratio (30min)
dsr_result = deflated_sharpe_ratio(estimated_sr, n_samples, n_trials)
if dsr_result['probabilistic_sr'] < 0.95:
    logging.warning("Results may not be significant after multiple testing!")

# 17. Adversarial Validation (30min)
adv_result = adversarial_validation(X_train, X_test)
if adv_result['auc'] > 0.75:
    raise ValueError("SEVERE DATASET SHIFT - RETRAIN REQUIRED!")

# 18. Sample Weights per Fold (20min)
cv_score = cv_with_proper_sample_weights(X, y)

# 19. Label Overlap Check (45min)
df_labeled, embargo_size = create_labels_with_awareness(df, label_horizon=5)
X_train, X_test, y_train, y_test = temporal_split_with_label_awareness(
    X, y, df_labeled, embargo_bars=embargo_size
)

# 20. MinTRL Check (20min)
mintrl_result = minimum_track_record_length(estimated_sr, prob=0.95)
if n_samples < mintrl_result['min_trl']:
    logging.warning(f"Track record insufficient: need {mintrl_result['min_trl']:.0f} samples")
```

---

## ğŸ“š Ù…Ù†Ø§Ø¨Ø¹ Ø¹Ù„Ù…ÛŒ Ú©Ù„ÛŒØ¯ÛŒ (Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡)

### Data Leakage & Preprocessing:
1. Kaufman et al. (2012). "Leakage in data mining: Formulation, detection, and avoidance"
2. "Data Leakage in Pandas: The Perils of Forward and Back Fill" (2023)
3. "Don't Push the Button! Exploring Data Leakage Risks" (arXiv 2024)
4. "A Prediction Method with Data Leakage Suppression" (MDPI 2022)

### Cross-Validation Ø¨Ø±Ø§ÛŒ Finance:
5. LÃ³pez de Prado (2018). **"Advances in Financial Machine Learning"** â­ Ú©ØªØ§Ø¨ Ú©Ù„ÛŒØ¯ÛŒ
6. "Cross Validation in Finance: Purging, Embargoing" (QuantInsti 2025)
7. "Backtest Overfitting in the Machine Learning Era" (2024)
8. Wikipedia: "Purged cross-validation" (2025)

### Feature Selection:
9. Guyon & Elisseeff (2003). "An introduction to variable and feature selection"
10. Meinshausen & BÃ¼hlmann (2010). **"Stability selection"** â­
11. Shah & Samworth (2013). "Variable selection with error control"
12. "nestedcv: an R package" (PMC 2023)
13. "Feature Selection without Label or Feature Leakage" (arXiv 2024)

### Multiple Testing:
14. Benjamini & Hochberg (1995). "Controlling the false discovery rate"
15. "Bon-EV: improved multiple testing for FDR" (PMC 2017)
16. "MultipleTesting.com" (PMC 2021)

### Backtest Overfitting:
17. Bailey & LÃ³pez de Prado (2015). **"The Probability of Backtest Overfitting"** â­
18. Bailey & LÃ³pez de Prado (2014). **"The Deflated Sharpe Ratio"** â­
19. Bailey & LÃ³pez de Prado (2012). "The Sharpe Ratio Efficient Frontier"
20. "Overfitting & Data-Snooping in Backtests" (Surmount.ai 2025)

### SHAP & Interpretability:
21. Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions"
22. Aas et al. (2021). "Explaining predictions when features are dependent"

### Dataset Shift:
23. "Using Adversarial Validation for Drift Assessment" (APXML 2025)
24. "Managing dataset shift by adversarial validation" (arXiv 2021)
25. "Adversarial Learning for Feature Shift Detection" (NeurIPS 2023)

### Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:
- â­ **LÃ³pez de Prado (2018). "Advances in Financial Machine Learning"**
- **Hastie, Tibshirani & Friedman. "The Elements of Statistical Learning"**
- Kuhn & Johnson. "Applied Predictive Modeling"
- Zheng & Casari. "Feature Engineering for Machine Learning"

---

## âš ï¸ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ

### ğŸ”´ CRITICAL WARNINGS:

1. **Ù‡Ø±Ú¯Ø² Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ù†Ú©Ù†ÛŒØ¯:**
```python
# âŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢ÛŒÙ†Ø¯Ù‡
X['future'] = X['close'].shift(-5)

# âŒ Backward fill
X.fillna(method='bfill')

# âŒ Global statistics Ø´Ø§Ù…Ù„ test
X_normalized = (X - X.mean()) / X.std()

# âŒ Feature selection Ù‚Ø¨Ù„ Ø§Ø² CV
selected = select_features(X)  # Ø±ÙˆÛŒ Ú©Ù„ X!
cv_score = cross_val_score(model, X[selected], y)

# âŒ Ø¨Ø¯ÙˆÙ† gap
X_train = X[:800]
X_test = X[800:]  # Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ø¹Ø¯ Ø§Ø² train!
```

2. **Ù‡Ù…ÛŒØ´Ù‡ Ø§ÛŒÙ† Ú©Ø§Ø±Ù‡Ø§ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:**
```python
# âœ… ÙÙ‚Ø· Ø§Ø² Ú¯Ø°Ø´ØªÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
X['lag5'] = X['close'].shift(5)

# âœ… Forward fill only
X.fillna(method='ffill')

# âœ… Fit ÙÙ‚Ø· Ø±ÙˆÛŒ train
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# âœ… Feature selection INSIDE CV
for fold in cv:
    selected = select_features(X_train_fold)
    model.fit(X_train_fold[selected])

# âœ… Ø¨Ø§ gap
X_train = X[:800]
X_test = X[824:]  # gap=24
```

### ğŸ“Š Ú†Ú©â€ŒÙ„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ù‚Ø¨Ù„ Ø§Ø² Production:

- [ ] **Lookahead bias validation passed?**
- [ ] **No forward/backward fill in preprocessing?**
- [ ] **Feature selection inside CV loops?**
- [ ] **Temporal split with adequate gap (â‰¥24 for forex)?**
- [ ] **Test set validation performed?**
- [ ] **PBO < 0.5?**
- [ ] **PSR > 0.95 (after deflation)?**
- [ ] **Adversarial validation AUC < 0.7?**
- [ ] **Track record â‰¥ MinTRL?**
- [ ] **FDR controlled (< 0.05)?**
- [ ] **Gap â‰¥ 2Ã— max ACF lag?**
- [ ] **All preprocessors fitted only on train?**
- [ ] **CPCV shows consistent performance?**
- [ ] **Performance gap (train-test) < 5%?**
- [ ] **No trailing NaNs in features?**

**Ø§Ú¯Ø± Ø­ØªÛŒ ÛŒÚ© âœ— Ø¯Ø§Ø±ÛŒØ¯ â†’ DO NOT USE IN PRODUCTION!**

---

## ğŸ“ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ

> **"Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø±Ø¨Ø§Øª Ù†Ù…ÙˆÙ†Ù‡ Ú©Ù„Ø§Ø³ÛŒÚ© Ø§Ø² Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª Ø±Ø§ÛŒØ¬ Ø¯Ø± financial ML Ù‡Ø³ØªÙ†Ø¯. Ø±ÙØ¹ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„Ø§Øª Ù†Ù‡ ØªÙ†Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ØŒ Ø¨Ù„Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ø±ÙˆÚ˜Ù‡ trading/ML Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª."**
>
> **â€” Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§ÛŒ 2025 Ùˆ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Bailey, LÃ³pez de Prado, Ùˆ Ø¯ÛŒÚ¯Ø±Ø§Ù†**

### Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ:

- **20 Ù…Ø´Ú©Ù„ Ø¨Ø­Ø±Ø§Ù†ÛŒ** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯
- **17 Ù…Ø´Ú©Ù„ CRITICAL** Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ù‡ disaster Ù…Ù†Ø¬Ø± Ø´ÙˆÙ†Ø¯
- **16-20 Ø³Ø§Ø¹Øª** Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ú©Ø§Ù…Ù„
- **4 Ø¯ÙˆØ± ØªØ­Ù‚ÛŒÙ‚Ø§Øª** Ø¹Ù…ÛŒÙ‚ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯

### Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:

Ø§ÛŒÙ† Ø±Ø¨Ø§Øª **Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø¨Ø±Ø§ÛŒ production trading Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†ÛŒØ³Øª**. Ù‚Ø¨Ù„ Ø§Ø² Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ:

1. âœ… Ø­Ø¯Ø§Ù‚Ù„ TOP 10 Ù…Ø´Ú©Ù„ Ø±Ø§ Ø±ÙØ¹ Ú©Ù†ÛŒØ¯
2. âœ… ØªÙ…Ø§Ù… Ú†Ú©â€ŒÙ„ÛŒØ³Øª Ø±Ø§ verify Ú©Ù†ÛŒØ¯
3. âœ… PBO, PSR, Ùˆ Adversarial Validation Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒØ¯
4. âœ… Walk-forward validation Ø¨Ø±Ø§ÛŒ Ø­Ø¯Ø§Ù‚Ù„ 6 Ù…Ø§Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯
5. âœ… Ø¨Ø§ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø®ÛŒÙ„ÛŒ Ú©Ù… (Ù…Ø«Ù„Ø§Ù‹ $100) Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯ Ùˆ monitor Ú©Ù†ÛŒØ¯

**Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± trading Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ avoiding mistakes ÙˆØ§Ø¨Ø³ØªÙ‡ Ø§Ø³Øª ØªØ§ finding the best model!**

---

**ØªØ§Ø±ÛŒØ®:** 18 Ù†ÙˆØ§Ù…Ø¨Ø± 2025  
**Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ:** 4.0 (Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ 4 Ø¯ÙˆØ±)  
**ÙˆØ¶Ø¹ÛŒØª:** Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ

ğŸš€ **Ø§ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹â€ŒØªØ±ÛŒÙ† Ù…Ù…ÛŒØ²ÛŒ Ú©Ø¯ Ø´Ù…Ø§ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§Ø³Ø§Ø³ Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ØªØ­Ù‚ÛŒÙ‚Ø§Øª 2024-2025 ØªÙ‡ÛŒÙ‡ Ø´Ø¯Ù‡!**
