# Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ ØªØ­Ù„ÛŒÙ„ Ø±Ø¨Ø§Øª FSZ12.py
## Ù†Ø³Ø®Ù‡ 2.0 - Ø¨Ø§ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ù…Ø¬Ø¯Ø¯ Ùˆ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±

**ØªØ§Ø±ÛŒØ® ØªØ­Ù„ÛŒÙ„:** 19 Ù†ÙˆØ§Ù…Ø¨Ø± 2025  
**Ù†Ø³Ø®Ù‡ Ú©Ø¯:** FSZ12.py  
**Ù‡Ø¯Ù:** ØªØ³Øª Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ±ÛŒØ¯ ÙØ§Ø±Ú©Ø³  
**ÙˆØ¶Ø¹ÛŒØª ØªØ­Ù‚ÛŒÙ‚Ø§Øª:** âœ… ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§ Ø¨ÛŒØ´ Ø§Ø² 30 Ù…Ù†Ø¨Ø¹ Ø¹Ù„Ù…ÛŒ Ù…Ø¹ØªØ¨Ø±

---

## ğŸ”´ Ø§Ø®Ø·Ø§Ø± Ø­ÛŒØ§ØªÛŒ

âš ï¸ **Ø§ÛŒÙ† Ú©Ø¯ Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø¨Ø±Ø§ÛŒ production Ùˆ ØªØ±ÛŒØ¯ ÙˆØ§Ù‚Ø¹ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³Øª**

**Ø¯Ù„Ø§ÛŒÙ„:**
1. âŒ 5 Ù…Ø³Ø¦Ù„Ù‡ CRITICAL Ú©Ù‡ Ø¨Ø§Ø¹Ø« data leakage Ùˆ Ù†ØªØ§ÛŒØ¬ ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
2. âŒ 5 Ù…Ø³Ø¦Ù„Ù‡ HIGH PRIORITY Ú©Ù‡ Ø¯Ù‚Øª Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ù‡ Ø´Ø¯Øª Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯
3. âŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ú©Ø¯ Ø¯Ø± ØªØ±ÛŒØ¯ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒ Ø¶Ø±Ø± Ø¯Ø§Ø±Ø¯

**ØªØ£ÛŒÛŒØ¯ Ø¹Ù„Ù…ÛŒ:** Ù‡Ù…Ù‡ Ø§ÛŒØ±Ø§Ø¯Ø§Øª Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ø§Ø² Ø¬Ù…Ù„Ù‡:
- ğŸ“š Marcos Lopez de Prado: "Advances in Financial Machine Learning" (2018)
- ğŸ“š Bailey et al.: "The Probability of Backtest Overfitting" (2014-2015)
- ğŸ“š Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ù„Ù…ÛŒØŒ WikipediaØŒ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ (mlfinlab, skfolio, hudsonthames)

---

## Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ

### âœ… Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ú©Ø¯:
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Nested Cross-Validation
- ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Data Leakage Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Stability Selection
- Ù…Ø­Ø§Ø³Ø¨Ù‡ PBO (Ø§Ú¯Ø±Ú†Ù‡ Ù†Ø§Ø¯Ø±Ø³Øª)
- Walk-Forward Analysis (Ø§Ú¯Ø±Ú†Ù‡ Ø¨Ø¯ÙˆÙ† embargo)
- Logging Ùˆ monitoring Ù…Ù†Ø§Ø³Ø¨

### âŒ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ú©Ù„ÛŒØ¯ÛŒ (ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹):

**5 Ù…Ø³Ø¦Ù„Ù‡ CRITICAL:**
1. **Look-Ahead Bias Ø¯Ø± Stability Selection** [1][11][12][13][16][19]
2. **Gap Calculation Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢ÛŒÙ†Ø¯Ù‡** [11][13][16][19]
3. **PBO Implementation Ù†Ø§Ø¯Ø±Ø³Øª** [12][15][18][21][24][27]
4. **Sharpe Ratio ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ** [39][42][45][48][51][54][57]
5. **Sample Weights Ø§Ø² Ø¢ÛŒÙ†Ø¯Ù‡** [11][67][68][69][72][79]

**5 Ù…Ø³Ø¦Ù„Ù‡ HIGH PRIORITY:**
1. Walk-Forward Ø¨Ø¯ÙˆÙ† Embargo [13][16][19][28]
2. Feature Validation Layer Ù†Ø§Ù‚Øµ [11]
3. Nested CV Ø¨Ø§ splits Ú©Ù… [11][13]
4. Ensemble Weights Ø«Ø§Ø¨Øª (Ù†Ù‡ data-driven)
5. MinTRL Ø¨Ø§ Sharpe Ù†Ø§Ø¯Ø±Ø³Øª [39][42]

---

## Ø¨Ø®Ø´ 1: Ù…Ø³Ø§Ø¦Ù„ CRITICAL Ø¨Ø§ Ø´ÙˆØ§Ù‡Ø¯ Ø¹Ù„Ù…ÛŒ

### C1: Look-Ahead Bias Ø¯Ø± Stability Selection

**Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±:**
- Lopez de Prado (2018) - "Advances in Financial Machine Learning", Chapter 7 [11][17][23][26]
- Purged Cross-Validation methodology [13][16][19][22][25]
- CPCV implementations: skfolio, quantbeckman [13][16][25]

**Ù…Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ú©Ø¯:**
```python
# Ø¯Ø± nested_cross_validation
fold_stability = self.stability_selection_framework(
    X_train_outer, y_train_outer,  # â† Ø±ÙˆÛŒ Ú©Ù„ outer fold
    n_iterations=min(self.stability_selection_iterations, 20),
    sample_fraction=0.5, threshold=0.6
)
```

**Ú†Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª:**
Ø·Ø¨Ù‚ Lopez de Prado (2018), stability selection Ø¨Ø§ÛŒØ¯ ÙÙ‚Ø· Ø¯Ø± **inner CV folds** Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯ ØªØ§ Ø§Ø² information leakage Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯. Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù† Ø±ÙˆÛŒ Ú©Ù„ outer fold Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¯Ø± validation Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø´ÙˆÙ†Ø¯ØŒ Ø¯Ø± selection process Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯.

**Ø´ÙˆØ§Ù‡Ø¯ Ø¹Ù„Ù…ÛŒ:**
> "To avoid look-ahead bias, feature selection must be performed within each training fold, not on the entire dataset." - Lopez de Prado (2018) [11]

**ØªØ§Ø«ÛŒØ±:**
- ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ unstable Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ stable Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Overfitting Ø¯Ø± feature selection
- Performance metrics ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ optimistic

**Ø±Ø§Ù‡â€ŒØ­Ù„ ØµØ­ÛŒØ­:**
```python
def nested_cv_correct(self, X, y):
    """
    Stability selection ÙÙ‚Ø· Ø¯Ø± inner CV
    """
    for train_outer_idx, test_outer_idx in outer_cv.split(X):
        X_train_outer = X.iloc[train_outer_idx]
        
        # Inner CV Ø¨Ø±Ø§ÛŒ stability selection
        for train_inner_idx, val_inner_idx in inner_cv.split(X_train_outer):
            X_train_inner = X_train_outer.iloc[train_inner_idx]
            
            # Ø§ÛŒÙ†Ø¬Ø§ stability selection Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
            fold_stability = self.stability_selection_framework(
                X_train_inner, y_train_inner,
                n_iterations=20
            )
```

**Ø§ÙˆÙ„ÙˆÛŒØª:** ğŸ”´ **CRITICAL - ÙÙˆØ±ÛŒ**

---

### C2: Gap Calculation Ø§Ø² Ú©Ù„ Dataset

**Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±:**
- Lopez de Prado (2018) - Chapter 7: Cross-Validation in Finance [11][17][20]
- Purged Cross-Validation - embargo mechanisms [13][16][19]
- "ACF calculation must use only training data" [28]

**Ù…Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ú©Ø¯:**
```python
def calculate_adaptive_gap(self, X: pd.DataFrame, y: pd.Series, 
                          label_horizon: int = 0) -> int:
    # Ø§Ú¯Ø± X Ø´Ø§Ù…Ù„ test data Ø¨Ø§Ø´Ø¯ â†’ leakage
    if 'close' in X.columns:
        returns = X['close'].pct_change().dropna()  # â† Ø§Ø² Ù‡Ù…Ù‡ X
        acf_data = returns
    
    autocorr = acf(acf_data, nlags=max_lag_check, fft=True)
```

**Ú†Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª:**
Gap Ø§Ø² autocorrelation Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ú¯Ø± test data Ø¯Ø± Ø§ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø´Ø¯ØŒ gap Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢ÛŒÙ†Ø¯Ù‡ ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ ÛŒÚ© look-ahead bias Ú©Ù„Ø§Ø³ÛŒÚ© Ø§Ø³Øª.

**Ø´ÙˆØ§Ù‡Ø¯ Ø¹Ù„Ù…ÛŒ:**
> "The embargo size should be calculated based on the training set only, as using test data would introduce forward-looking bias." - Purged CV documentation [13][16]

**ØªØ§Ø«ÛŒØ±:**
- Gap Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ optimistic
- CV results ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ
- Ø¯Ø± productionØŒ Ø§ÛŒÙ† gap Ø¯Ù‚ÛŒÙ‚ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ ØµØ­ÛŒØ­:**
```python
def calculate_adaptive_gap_correct(
    self, 
    X_train_only: pd.DataFrame,  # ÙÙ‚Ø· train
    y_train: pd.Series,
    label_horizon: int = 0
) -> int:
    """
    CRITICAL: ÙÙ‚Ø· Ø§Ø² train data
    """
    if 'close' in X_train_only.columns:
        returns = X_train_only['close'].pct_change().dropna()
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ACF ÙÙ‚Ø· Ø§Ø² train
    autocorr = acf(returns, nlags=max_lag_check, fft=True)
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† optimal gap
    for lag in range(1, max_lag_check):
        if abs(autocorr[lag]) < significance_level:
            return lag
    
    return max_lag_check
```

**Ø§ÙˆÙ„ÙˆÛŒØª:** ğŸ”´ **CRITICAL**

---

### C3: PBO Implementation Ù†Ø§Ø¯Ø±Ø³Øª

**Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± (Ù…Ù‚Ø§Ù„Ø§Øª Ø§ØµÙ„ÛŒ):**
- Bailey, D. H., Borwein, J., Lopez de Prado, M., & Zhu, Q. J. (2014). "The Probability of Backtest Overfitting" [12][15][18][21]
- Bailey et al. (2015). SSRN paper [21][24][27]
- CSCV methodology (Combinatorially Symmetric Cross-Validation) [12][15]

**Ù…Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ú©Ø¯:**
```python
def calculate_pbo_with_multiple_strategies(self, X, y, ...):
    # ÙÙ‚Ø· ÛŒÚ© split Ø³Ø§Ø¯Ù‡
    n = len(X)
    is_end = n // 2
    oos_start = is_end + gap
    
    X_is = X.iloc[:is_end]      # in-sample
    X_oos = X.iloc[oos_start:]  # out-of-sample
```

**Ú†Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª:**
Ø·Ø¨Ù‚ Bailey et al. (2014), PBO Ù†ÛŒØ§Ø² Ø¨Ù‡:
1. **Multiple train/test scenarios** (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 16 combinations)
2. **Combinatorial Symmetric Cross-Validation (CSCV)**
3. ØªØ³Øª N strategies Ø±ÙˆÛŒ Ù‡Ø± scenario
4. Ù…Ø­Ø§Ø³Ø¨Ù‡ rank distribution

Ø§ÛŒÙ† implementation ÙÙ‚Ø· Ø§Ø² ÛŒÚ© split Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

**Ø´ÙˆØ§Ù‡Ø¯ Ø¹Ù„Ù…ÛŒ:**
> "PBO requires combinatorially symmetric cross-validation with multiple train-test paths to properly estimate overfitting probability." - Bailey et al. (2014) [12][15][18]

**ÙØ±Ù…ÙˆÙ„ ØµØ­ÛŒØ­ PBO:**
\[
PBO = P[\text{OOS rank} \geq \frac{N}{2} | \text{IS optimal}]
\]

**ØªØ§Ø«ÛŒØ±:**
- PBO Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ overfitting Ø±Ø§ detect Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ strategy robustness Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†Ø¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ ØµØ­ÛŒØ­:**
```python
def calculate_pbo_correct(self, X, y, n_scenarios=16, n_strategies=50):
    """
    Bailey et al. (2014) CSCV methodology
    """
    from itertools import combinations
    
    # 1. Ø§ÛŒØ¬Ø§Ø¯ S scenarios Ø¨Ø§ CSCV
    n_splits = 6
    scenarios = []
    
    # Combinatorial splits
    for combo in combinations(range(n_splits), n_splits // 2):
        train_idx = [i for i in range(n_splits) if i not in combo]
        test_idx = list(combo)
        scenarios.append((train_idx, test_idx))
    
    # 2. Ø¨Ø±Ø§ÛŒ Ù‡Ø± scenario Ùˆ strategy
    is_performance = np.zeros((n_scenarios, n_strategies))
    oos_performance = np.zeros((n_scenarios, n_strategies))
    
    for s_idx, (train_folds, test_folds) in enumerate(scenarios):
        # Split data
        train_data = self._get_folds(X, y, train_folds)
        test_data = self._get_folds(X, y, test_folds)
        
        for strat_idx in range(n_strategies):
            # Random feature subset
            features = self._random_feature_selection()
            
            # Train model
            model = self._train_model(train_data, features)
            
            # IS performance
            is_perf = self._evaluate(model, train_data)
            is_performance[s_idx, strat_idx] = is_perf
            
            # OOS performance
            oos_perf = self._evaluate(model, test_data)
            oos_performance[s_idx, strat_idx] = oos_perf
    
    # 3. Ù…Ø­Ø§Ø³Ø¨Ù‡ PBO
    # Ø¨Ø±Ø§ÛŒ strategy Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† IS performance
    best_is_idx = np.argmax(is_performance.mean(axis=0))
    
    # Rank Ø¯Ø± OOS
    oos_ranks = np.argsort(np.argsort(oos_performance[:, best_is_idx]))
    
    # PBO = Ø§Ø­ØªÙ…Ø§Ù„ rank <= median
    pbo = np.mean(oos_ranks <= len(oos_ranks) / 2)
    
    return {
        'pbo': pbo,
        'is_performance': is_performance,
        'oos_performance': oos_performance,
        'interpretation': 'Good' if pbo < 0.5 else 'Overfitted'
    }
```

**Ø§ÙˆÙ„ÙˆÛŒØª:** ğŸ”´ **CRITICAL**

---

### C4: Sharpe Ratio ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ

**Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± (Ù…Ù‚Ø§Ù„Ø§Øª Ø§ØµÙ„ÛŒ):**
- Bailey, D. H. & Lopez de Prado, M. (2014). "The Deflated Sharpe Ratio" [39][42][45][48][51][54]
- Wikipedia: Deflated Sharpe Ratio [42]
- Published in Journal of Computational Finance [42][57]

**Ù…Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ú©Ø¯:**
```python
def calculate_sharpe_from_predictions(
    self, y_true, y_pred_proba,
    returns_per_signal: float = 0.01,  # â† ÙØ±Ø¶ ØºÙ„Ø·
    annual_factor: int = 252
):
    positions = np.where(y_pred_proba > 0.5, 1, -1)
    
    # ÙØ±Ø¶: Ù‡Ø± signal return Ø«Ø§Ø¨Øª Ø¯Ø§Ø±Ø¯
    actual_returns = np.where(
        y_true == 1, 
        returns_per_signal,      # â† ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ
        -returns_per_signal
    )
    
    strategy_returns = positions * actual_returns
    sharpe = (mean_return / std_return) * np.sqrt(annual_factor)
```

**Ú†Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª:**
1. ÙØ±Ø¶ return Ø«Ø§Ø¨Øª (1%) Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ signals Ú©Ø§Ù…Ù„Ø§Ù‹ ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª
2. Transaction costs Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù†Ø´Ø¯Ù‡
3. Slippage Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
4. Deflated Sharpe Ø¨Ø±Ø§ÛŒ multiple testing Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡

**Ø´ÙˆØ§Ù‡Ø¯ Ø¹Ù„Ù…ÛŒ:**
> "The Sharpe ratio should be calculated from actual strategy returns, including all costs. The Deflated Sharpe Ratio corrects for selection bias under multiple testing." - Bailey & Lopez de Prado (2014) [39][42]

**ÙØ±Ù…ÙˆÙ„ Deflated Sharpe Ratio:**
\[
DSR = \frac{\hat{SR} - SR_0}{\sqrt{\text{Var}[\hat{SR}]}}
\]

Ú©Ù‡ Ø¯Ø± Ø¢Ù†:
\[
SR_0 = \sqrt{\text{Var}[\hat{SR}]} \times \left[(1-\gamma)\Phi^{-1}[1-\frac{1}{N}] + \gamma\Phi^{-1}[1-\frac{1}{Ne}]\right]
\]

**ØªØ§Ø«ÛŒØ±:**
- Sharpe Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ optimistic
- ØªØµÙ…ÛŒÙ…Ø§Øª Ø§Ø´ØªØ¨Ø§Ù‡ Ø¯Ø± feature selection
- MinTRL Ù†Ø§Ø¯Ø±Ø³Øª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ ØµØ­ÛŒØ­:**
```python
def calculate_real_sharpe_with_dsr(
    self,
    signals: np.ndarray,              # +1, -1, 0
    actual_price_returns: np.ndarray, # Ø¨Ø§Ø²Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² price
    transaction_cost: float = 0.0002, # 2 pips
    slippage: float = 0.0001,         # 1 pip
    n_trials: int = 100               # ØªØ¹Ø¯Ø§Ø¯ strategies test Ø´Ø¯Ù‡
) -> Dict:
    """
    Sharpe ÙˆØ§Ù‚Ø¹ÛŒ + Deflated Sharpe
    """
    # 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ strategy returns Ø¨Ø§ costs
    position_changes = np.abs(np.diff(signals))
    
    # Ø¨Ø§Ø²Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
    strategy_returns = signals[:-1] * actual_price_returns[1:]
    
    # Ú©Ø³Ø± costs
    costs = position_changes * (transaction_cost + slippage)
    net_returns = strategy_returns - costs
    
    # 2. Sharpe Ratio (annualized)
    mean_ret = np.mean(net_returns)
    std_ret = np.std(net_returns)
    
    if std_ret < 1e-10:
        return {'sharpe': 0.0, 'dsr': 0.0, 'psr': 0.0}
    
    sharpe = (mean_ret / std_ret) * np.sqrt(252)
    
    # 3. Moments Ø¨Ø±Ø§ÛŒ DSR
    from scipy.stats import skew, kurtosis
    skewness = skew(net_returns)
    kurt = kurtosis(net_returns)
    
    # 4. Variance of Sharpe
    T = len(net_returns)
    var_sr = (1 / T) * (
        1 + 0.5 * sharpe**2
        - skewness * sharpe
        + (kurt / 4) * sharpe**2
    )
    
    # 5. Expected Maximum SR (EMC)
    from scipy.stats import norm
    euler = 0.5772156649  # Euler-Mascheroni constant
    
    sr_threshold = np.sqrt(var_sr) * (
        (1 - euler) * norm.ppf(1 - 1/n_trials) +
        euler * norm.ppf(1 - 1/(n_trials * np.e))
    )
    
    # 6. Deflated Sharpe Ratio
    dsr = (sharpe - sr_threshold) / np.sqrt(var_sr)
    
    # 7. Probabilistic Sharpe Ratio
    psr = norm.cdf(dsr)
    
    return {
        'sharpe': sharpe,
        'deflated_sharpe': dsr,
        'probabilistic_sharpe': psr,
        'sr_threshold': sr_threshold,
        'mean_return': mean_ret * 252,  # annualized
        'volatility': std_ret * np.sqrt(252),
        'total_costs': np.sum(costs),
        'n_trades': position_changes.sum(),
        'skewness': skewness,
        'kurtosis': kurt,
        'interpretation': self._interpret_dsr(psr)
    }

def _interpret_dsr(self, psr):
    if psr >= 0.95:
        return "EXCELLENT - Strategy has skill (95%+ confidence)"
    elif psr >= 0.90:
        return "GOOD - Likely has skill (90%+ confidence)"
    elif psr >= 0.75:
        return "MODERATE - Some evidence of skill"
    else:
        return "POOR - Likely due to luck, not skill"
```

**Ø§ÙˆÙ„ÙˆÛŒØª:** ğŸ”´ **CRITICAL**

---

### C5: Sample Weights Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢ÛŒÙ†Ø¯Ù‡

**Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±:**
- Lopez de Prado (2018) - Chapter 4: Sample Weights [11][17][20][67][72][79]
- Sequential Bootstrap methodology [67][69][77]
- Sample weights by uniqueness [67][68][69][72]

**Ù…Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¯Ø± Ú©Ø¯:**
```python
def compute_time_weighted_samples(self, y: pd.Series, 
                                  label_horizon: int = None):
    n = len(y)
    time_weights = np.linspace(0.5, 1.5, n)
    
    if label_horizon and label_horizon > 0:
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² label_horizon â†’ future info
        decay_factor = 1 - (label_horizon / len(y))
        time_weights *= decay_factor
```

**Ú†Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª:**
`label_horizon` ÛŒÚ© hyperparameter Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ù…Ø±Ø¨ÙˆØ· Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† Ø¯Ø± sample weights ÛŒÚ© information leakage Ø§Ø³Øª Ø²ÛŒØ±Ø§ weights Ø¨Ø§ÛŒØ¯ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø°Ø´ØªÙ‡ (historical) Ø¨Ø§Ø´Ù†Ø¯.

**Ø´ÙˆØ§Ù‡Ø¯ Ø¹Ù„Ù…ÛŒ:**
> "Sample weights should be based on the uniqueness of observations, accounting for label overlap, not on future information." - Lopez de Prado (2018), Chapter 4 [11][67][72]

**ØªØ§Ø«ÛŒØ±:**
- Model Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢ÛŒÙ†Ø¯Ù‡ train Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Performance metrics optimistic Ùˆ unrealistic
- Ø¯Ø± production Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ

**Ø±Ø§Ù‡â€ŒØ­Ù„ ØµØ­ÛŒØ­ (Sample Weights by Uniqueness):**
```python
def compute_sample_weights_by_uniqueness(
    self,
    y: pd.Series,
    label_times: pd.DataFrame  # columns: ['t_start', 't_end']
) -> np.ndarray:
    """
    Sample weights Ø¨Ø± Ø§Ø³Ø§Ø³ uniqueness - Lopez de Prado (2018)
    
    Samples with fewer concurrent labels â†’ higher weight
    """
    n = len(y)
    
    # 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ concurrent labels
    concurrent_labels = np.zeros(n)
    
    for i in range(n):
        t_start_i = label_times.iloc[i]['t_start']
        t_end_i = label_times.iloc[i]['t_end']
        
        # Ú†Ù†Ø¯ label Ø¨Ø§ Ø§ÛŒÙ† overlap Ø¯Ø§Ø±Ù†Ø¯ØŸ
        overlaps = (
            (label_times['t_start'] <= t_end_i) &
            (label_times['t_end'] >= t_start_i)
        )
        concurrent_labels[i] = overlaps.sum() - 1  # Ø®ÙˆØ¯Ø´ Ù†Ù‡
    
    # 2. Uniqueness = 1 / (concurrent + 1)
    uniqueness = 1.0 / (concurrent_labels + 1.0)
    
    # 3. Average uniqueness per label
    sample_weights = np.zeros(n)
    
    for i in range(n):
        t_start_i = label_times.iloc[i]['t_start']
        t_end_i = label_times.iloc[i]['t_end']
        
        # Ù‡Ù…Ù‡ timestamps Ø¯Ø± Ø§ÛŒÙ† label
        mask = (
            (label_times.index >= t_start_i) &
            (label_times.index <= t_end_i)
        )
        
        # Average uniqueness
        sample_weights[i] = uniqueness[mask].mean()
    
    # 4. Normalize
    sample_weights = sample_weights / sample_weights.mean()
    
    # 5. Class balancing (optional)
    if self.classification:
        class_counts = np.bincount(y)
        class_weights = len(y) / (len(class_counts) * class_counts)
        
        for i, label in enumerate(y):
            sample_weights[i] *= class_weights[label]
    
    return sample_weights
```

**Sequential Bootstrap (Ø¨Ø±Ø§ÛŒ bagging):**
```python
def sequential_bootstrap(
    self,
    label_times: pd.DataFrame,
    sample_weights: np.ndarray,
    n_bootstrap: int = 1000
) -> List[np.ndarray]:
    """
    Sequential Bootstrap - Lopez de Prado (2018)
    
    Handles overlapping labels correctly
    """
    n = len(label_times)
    bootstrap_samples = []
    
    for _ in range(n_bootstrap):
        selected = []
        available = set(range(n))
        available_weights = sample_weights.copy()
        
        while len(available) > 0 and len(selected) < n:
            # Sample Ø¨Ø§ weights
            probs = available_weights[list(available)]
            probs = probs / probs.sum()
            
            idx = np.random.choice(
                list(available),
                size=1,
                p=probs
            )[0]
            
            selected.append(idx)
            
            # Remove overlapping
            t_start = label_times.iloc[idx]['t_start']
            t_end = label_times.iloc[idx]['t_end']
            
            for i in list(available):
                if (label_times.iloc[i]['t_start'] <= t_end and
                    label_times.iloc[i]['t_end'] >= t_start):
                    available.remove(i)
        
        bootstrap_samples.append(np.array(selected))
    
    return bootstrap_samples
```

**Ø§ÙˆÙ„ÙˆÛŒØª:** ğŸ”´ **CRITICAL**

---

## Ø¨Ø®Ø´ 2: Ù…Ø³Ø§Ø¦Ù„ HIGH PRIORITY

### H1: Walk-Forward Ø¨Ø¯ÙˆÙ† Embargo

**Ù…Ù†Ø§Ø¨Ø¹:**
- Purged Cross-Validation - embargoing [13][16][19][28]
- Lopez de Prado (2018) - Chapter 7 [11][17]

**Ù…Ø­Ù„ Ù…Ø´Ú©Ù„:**
```python
def walk_forward_analysis(self, X, y, n_splits=10):
    for fold in range(n_splits):
        X_train = X.iloc[:train_end]
        test_start = train_end  # â† Ù‡ÛŒÚ† gap Ù†ÛŒØ³Øª
        X_test = X.iloc[test_start:test_end]
```

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```python
def walk_forward_with_embargo(
    self, X, y,
    n_splits=10,
    embargo_pct=0.01  # 1% embargo
):
    n = len(X)
    embargo_size = int(n * embargo_pct)
    
    for fold in range(n_splits):
        # Train
        X_train = X.iloc[:train_end]
        
        # Embargo gap
        test_start = train_end + embargo_size
        test_end = test_start + test_size
        
        # Test
        X_test = X.iloc[test_start:test_end]
```

**Ø§ÙˆÙ„ÙˆÛŒØª:** ğŸ”´ **HIGH**

---

### H2-H5: Ø³Ø§ÛŒØ± Ù…Ø³Ø§Ø¦Ù„ HIGH

Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„ØŒ Ø®Ù„Ø§ØµÙ‡:

**H2: Feature Validation Layer Ù†Ø§Ù‚Øµ**
- Ù†ÛŒØ§Ø² Ø¨Ù‡ causality testing
- Ø¨Ø±Ø±Ø³ÛŒ rolling/expanding windows
- ØªØ³Øª look-ahead bias

**H3: Nested CV - Inner Splits Ú©Ù…**
- Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø² 3 Ø¨Ù‡ 5-7 splits
- Ø¨Ù‡Ø¨ÙˆØ¯ hyperparameter tuning

**H4: Ensemble Weights Ø«Ø§Ø¨Øª**
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Optuna Ø¨Ø±Ø§ÛŒ optimization
- Data-driven weight learning

**H5: MinTRL Ø¨Ø§ Sharpe Ù†Ø§Ø¯Ø±Ø³Øª**
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Sharpe ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² backtest
- Ø§ØµÙ„Ø§Ø­ ÙØ±Ù…ÙˆÙ„ MinTRL

---

## Ø¨Ø®Ø´ 3: Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ (ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹)

### 1. Triple Barrier Method

**Ù…Ù†Ø§Ø¨Ø¹:**
- Lopez de Prado (2018) - Chapter 3: Labeling [11][17][41][44][47][50][53]
- Multiple implementations available [41][44][50][53]

**Ù…Ø²Ø§ÛŒØ§:**
- Labeling ÙˆØ§Ù‚Ø¹ÛŒâ€ŒØªØ± Ø¨Ø§ take-profit, stop-loss, time barrier
- Ø§Ù…Ú©Ø§Ù† meta-labeling
- Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ ØªØ±ÛŒØ¯ ÙˆØ§Ù‚Ø¹ÛŒ

**Implementation:**
```python
class TripleBarrierLabeling:
    def __init__(
        self,
        upper_barrier_pct: float = 0.02,  # 2% profit
        lower_barrier_pct: float = 0.01,  # 1% loss
        time_barrier: int = 24            # 24 periods max
    ):
        self.upper = upper_barrier_pct
        self.lower = lower_barrier_pct
        self.time = time_barrier
    
    def apply(self, prices, side=None):
        """
        Returns:
            label: 1 (profit), -1 (loss), 0 (neutral)
            barrier_hit: 'upper', 'lower', 'time'
            holding_period: periods until hit
        """
        # Implementation...
```

---

### 2. Fractional Differentiation

**Ù…Ù†Ø§Ø¨Ø¹:**
- Lopez de Prado (2018) - Chapter 5 [11][17][40][43][46][49][52]
- Academic papers on fractional differentiation [40][49]

**Ù…Ø²Ø§ÛŒØ§:**
- Stationarity + Memory preservation
- Ø¨Ù‡ØªØ± Ø§Ø² integer differencing
- Ù…Ø®ØµÙˆØµ financial time series

**Implementation:**
```python
def fractional_differentiation(
    series: pd.Series,
    d: float = 0.5,  # 0.4-0.6 optimal
    threshold: float = 0.01
):
    """
    d=0: original
    d=1: first difference
    d=0.5: optimal Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ø³Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ
    """
    # Calculate weights
    weights = get_frac_diff_weights(d, len(series))
    weights = weights[abs(weights) > threshold]
    
    # Apply
    result = series.copy()
    for i in range(len(weights), len(series)):
        result.iloc[i] = np.dot(
            series.iloc[i-len(weights):i].values,
            weights
        )
    
    return result
```

---

### 3. Meta-Labeling

**Ù…Ù†Ø§Ø¨Ø¹:**
- Lopez de Prado (2018) - Chapter 3 [11][17][68][71][74][78][81]
- Wikipedia: Meta-Labeling [68]

**Ù…Ø²Ø§ÛŒØ§:**
- Ø§ÙØ²Ø§ÛŒØ´ precision Ø¨Ø¯ÙˆÙ† Ú©Ø§Ù‡Ø´ recall
- ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† false positives
- Ø§Ù…Ú©Ø§Ù† ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ non-ML strategies

**Implementation:**
```python
class MetaLabeling:
    def __init__(self, primary_model):
        self.primary = primary_model
        self.meta_model = None
    
    def create_meta_labels(self, primary_preds, actual_returns):
        """
        Meta-label = 1 if prediction profitable
        """
        meta_labels = np.zeros(len(primary_preds))
        
        for i, pred in enumerate(primary_preds):
            if pred == 0:
                continue
            
            actual_return = actual_returns[i] * pred
            meta_labels[i] = 1 if actual_return > 0 else 0
        
        return meta_labels
    
    def fit_meta_model(self, X, meta_labels):
        """Train secondary model"""
        self.meta_model = lgb.LGBMClassifier()
        self.meta_model.fit(X, meta_labels)
    
    def predict_with_confidence(self, X):
        """Primary + meta predictions"""
        primary_signals = self.primary.predict(X)
        confidence = self.meta_model.predict_proba(X)[:, 1]
        
        # Filter low confidence
        filtered = primary_signals.copy()
        filtered[confidence < 0.55] = 0
        
        return filtered, confidence
```

---

### 4. Combinatorial Purged CV (CPCV)

**Ù…Ù†Ø§Ø¨Ø¹:**
- Lopez de Prado (2018) - Chapter 7 [11][17][20]
- Multiple implementations [13][16][22][25][28]

**Ù…Ø²Ø§ÛŒØ§:**
- Multiple train/test paths
- Robust validation
- Purging + Embargo

**Implementation:**
```python
class CombinatorialPurgedCV:
    def __init__(
        self,
        n_splits: int = 6,
        n_test_splits: int = 2,
        embargo_pct: float = 0.01
    ):
        self.n_splits = n_splits
        self.n_test = n_test_splits
        self.embargo = embargo_pct
    
    def split(self, X):
        from itertools import combinations
        
        n = len(X)
        group_size = n // self.n_splits
        
        # All combinations
        for test_groups in combinations(
            range(self.n_splits),
            self.n_test
        ):
            # Get indices
            test_idx = self._get_test_indices(
                test_groups, group_size, n
            )
            
            # Purge + embargo
            train_idx = self._get_train_with_purge_embargo(
                test_idx, n
            )
            
            yield train_idx, test_idx
```

---

## Ø¨Ø®Ø´ 4: Implementation Roadmap

### ÙØ§Ø² 1: Ø±ÙØ¹ CRITICAL (2-3 Ù‡ÙØªÙ‡)

**Ù‡ÙØªÙ‡ 1:**
1. âœ… Ø§ØµÙ„Ø§Ø­ `calculate_adaptive_gap` - ÙÙ‚Ø· train data
2. âœ… Ø§ØµÙ„Ø§Ø­ `sample_weights` - uniqueness based
3. âœ… Ø§ØµÙ„Ø§Ø­ `Sharpe calculation` - actual returns + DSR

**Ù‡ÙØªÙ‡ 2:**
4. âœ… Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØµØ­ÛŒØ­ PBO Ø¨Ø§ CSCV
5. âœ… Ø§ØµÙ„Ø§Ø­ Stability Selection Ø¯Ø± Nested CV

**ØªØ³Øª:**
- Smoke tests Ø±ÙˆÛŒ synthetic data
- Validation Ø±ÙˆÛŒ known datasets
- Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‚Ø¨Ù„/Ø¨Ø¹Ø¯

---

### ÙØ§Ø² 2: Ø±ÙØ¹ HIGH (2-3 Ù‡ÙØªÙ‡)

**Ù‡ÙØªÙ‡ 3:**
1. âœ… Embargo Ø¯Ø± Walk-Forward
2. âœ… Feature Validation Layer
3. âœ… Ø§ÙØ²Ø§ÛŒØ´ Inner Splits Ø¨Ù‡ 5-7

**Ù‡ÙØªÙ‡ 4:**
4. âœ… Optimize Ensemble Weights Ø¨Ø§ Optuna
5. âœ… Ø§ØµÙ„Ø§Ø­ MinTRL

---

### ÙØ§Ø² 3: Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ (4-6 Ù‡ÙØªÙ‡)

**Ù‡ÙØªÙ‡ 5-6:**
1. âœ… CPCV Implementation
2. âœ… Triple Barrier Labeling
3. âœ… Fractional Differentiation

**Ù‡ÙØªÙ‡ 7-8:**
4. âœ… Meta-Labeling Framework
5. âœ… Regime Detection (optional)

**Ù‡ÙØªÙ‡ 9-10:**
6. âœ… Sequential Bootstrap
7. âœ… Cross-Asset Validation

---

### ÙØ§Ø² 4: Testing & Validation (3-4 Ù‡ÙØªÙ‡)

**Unit Tests:**
- Ù‡Ø± method Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
- Edge cases
- Data leakage tests

**Integration Tests:**
- Ú©Ù„ pipeline
- Multiple datasets
- Different market conditions

**Forward Testing:**
- Paper trading 3 Ù…Ø§Ù‡
- Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ backtest
- Monitoring Ùˆ logging

---

## Ø¨Ø®Ø´ 5: Ú†Ú©â€ŒÙ„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ

### âœ… Ù‚Ø¨Ù„ Ø§Ø² Production:

**Data Leakage Prevention:**
- [ ] Gap calculation ÙÙ‚Ø· Ø§Ø² train
- [ ] Sample weights Ø¨Ø¯ÙˆÙ† label_horizon
- [ ] Feature validation layer Ú©Ø§Ù…Ù„
- [ ] Embargo Ø¯Ø± Ù‡Ù…Ù‡ splits
- [ ] Purging Ø¯Ø± CV

**Performance Metrics:**
- [ ] Sharpe Ø§Ø² actual returns
- [ ] Transaction costs included
- [ ] Deflated Sharpe calculated
- [ ] PBO < 0.5
- [ ] MinTRL Ø¨Ø§ Sharpe ÙˆØ§Ù‚Ø¹ÛŒ

**Validation:**
- [ ] CPCV Ø¨Ø§ multiple paths
- [ ] Sequential bootstrap Ø¨Ø±Ø§ÛŒ bagging
- [ ] Cross-asset validation
- [ ] Forward test 3+ Ù…Ø§Ù‡
- [ ] Monitoring dashboard

**Code Quality:**
- [ ] Unit tests coverage > 80%
- [ ] Integration tests
- [ ] Logging comprehensive
- [ ] Error handling robust
- [ ] Documentation complete

---

## Ø¨Ø®Ø´ 6: Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù…Ø±Ø§Ø¬Ø¹

### Ù…Ø±Ø§Ø¬Ø¹ Ø§ØµÙ„ÛŒ (Ú©ØªØ§Ø¨â€ŒÙ‡Ø§):

1. **Lopez de Prado, M. (2018).** "Advances in Financial Machine Learning"
   - John Wiley & Sons
   - ISBN: 978-1-119-48208-6
   - **ÙØµÙˆÙ„ Ú©Ù„ÛŒØ¯ÛŒ:**
     - Chapter 3: Labeling (Triple Barrier, Meta-Labeling)
     - Chapter 4: Sample Weights (Uniqueness, Sequential Bootstrap)
     - Chapter 5: Fractionally Differentiated Features
     - Chapter 7: Cross-Validation in Finance (CPCV, Purging, Embargo)
     - Chapter 8: Feature Importance
     - Chapter 11: The Dangers of Backtesting
   
   **Ø¯Ø³ØªØ±Ø³ÛŒ:**
   - Wiley official: https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086 [17][23]
   - Amazon, Google Books
   - University libraries

---

### Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ù„Ù…ÛŒ Ù…Ø¹ØªØ¨Ø±:

2. **Bailey, D. H., Borwein, J., Lopez de Prado, M., & Zhu, Q. J. (2014).**
   "The Probability of Backtest Overfitting"
   - Journal of Computational Finance, 2017
   - SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2326253 [12][15][21]
   - DOI: 10.21314/JCF.2016.322
   - PDF: https://www.davidhbailey.com/dhbpapers/backtest-prob.pdf [18]

3. **Bailey, D. H. & Lopez de Prado, M. (2014).**
   "The Deflated Sharpe Ratio: Correcting for Selection Bias, Backtest Overfitting, and Non-Normality"
   - Journal of Portfolio Management, 2014
   - SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2460551 [48][54]
   - PDF: https://www.davidhbailey.com/dhbpapers/deflated-sharpe.pdf [39]

4. **Bailey, D. H., Borwein, J., Lopez de Prado, M., & Zhu, Q. J. (2015).**
   "Statistical Overfitting and Backtest Performance"
   - SSRN paper [24][27]

---

### Wikipedia Ùˆ Ù…Ù†Ø§Ø¨Ø¹ Ø¢Ù…ÙˆØ²Ø´ÛŒ:

5. **Wikipedia: Purged Cross-Validation**
   - https://en.wikipedia.org/wiki/Purged_cross-validation [19]
   - Describes purging and embargoing
   - References to Lopez de Prado's work

6. **Wikipedia: Deflated Sharpe Ratio**
   - https://en.wikipedia.org/wiki/Deflated_Sharpe_ratio [42]
   - Formula and methodology
   - Examples and interpretation

7. **Wikipedia: Meta-Labeling**
   - https://en.wikipedia.org/wiki/Meta-Labeling [68]
   - Primary and secondary models
   - Applications in finance

---

### Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ:

8. **mlfinlab (Hudson Thames)**
   - Website: https://hudsonthames.org/
   - Implementation of AFML techniques
   - Articles:
     - Sequential Bootstrap: https://hudsonthames.org/bagging-in-financial-machine-learning-sequential-bootstrapping-python/ [67]
     - Meta Labeling: https://hudsonthames.org/meta-labeling-a-toy-example/ [81]
     - Fractional Diff: https://mlfinpy.readthedocs.io/en/latest/FractionalDifferentiated.html [43]
     - Triple Barrier: https://mlfinpy.readthedocs.io/en/latest/Labelling.html [44]

9. **skfolio**
   - CPCV Implementation: https://skfolio.org/generated/skfolio.model_selection.CombinatorialPurgedCV.html [16]
   - Production-ready library

10. **quantbeckman**
    - Article on CPCV: https://www.quantbeckman.com/p/with-code-combinatorial-purged-cross [13]
    - Code examples

---

### ÙˆØ¨Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ùˆ Ø¢Ù…ÙˆØ²Ø´â€ŒÙ‡Ø§:

11. **QuantInsti Blog**
    - Cross Validation in Finance: https://blog.quantinsti.com/cross-validation-embargo-purging-combinatorial/ [28]
    - Practical examples

12. **Towards AI**
    - CPCV Method: https://towardsai.net/p/l/the-combinatorial-purged-cross-validation-method [22]

13. **InsightBig**
    - Traditional Backtesting vs CPCV: https://www.insightbig.com/post/traditional-backtesting-is-outdated-use-cpcv-instead [25]

14. **QuantDare**
    - Deflated Sharpe Ratio: https://quantdare.com/deflated-sharpe-ratio-how-to-avoid-been-fooled-by-randomness/ [45]

15. **staITuned**
    - Fractional Differentiation: https://staituned.com/learn/expert/time-series-forecasting-with-fraction-differentiation [40]

16. **William Santos**
    - Triple Barrier Algorithm: https://williamsantos.me/posts/2022/triple-barrier-labelling-algorithm/ [41]

17. **Sefidian.com**
    - Labeling Financial Data: https://www.sefidian.com/2021/06/26/labeling-financial-data-for-machine-learning/ [71]

---

### Ù…Ù‚Ø§Ù„Ø§Øª arXiv Ùˆ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¬Ø¯ÛŒØ¯:

18. **Stock Price Prediction Using Triple Barrier Labeling (2024)**
    - arXiv: https://arxiv.org/html/2504.02249v2 [50]

19. **Time-Series Forecasting with Fractional Differentiation (2023)**
    - arXiv: https://arxiv.org/pdf/2309.13409.pdf [49]

20. **Survey of Financial AI (2024)**
    - arXiv: http://arxiv.org/pdf/2411.12747.pdf [6]

---

### GitHub Repositories:

21. **fracdiff/fracdiff**
    - https://github.com/fracdiff/fracdiff [58]
    - Fractional differentiation implementation

22. **nkonts/barrier-method**
    - https://github.com/nkonts/barrier-method [53]
    - Triple barrier method expansion

---

### ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ:

23. **YouTube: Triple Barrier Method**
    - https://www.youtube.com/watch?v=-Yxkd5WC_gg [56]

24. **YouTube: Sample Weights and Label Uniqueness**
    - https://www.youtube.com/watch?v=g_C42VewM10 [69]

25. **YouTube: Sequential Bootstrap**
    - https://www.youtube.com/watch?v=RyHG3B0LsAQ [77]

---

### Academic Papers (Additional):

26. **MDPI: Early Warning System for Financial Networks (2024)**
    - https://www.mdpi.com/1099-4300/26/9/796 [10]

27. **From Factor Models to Deep Learning (2024)**
    - arXiv: https://arxiv.org/pdf/2403.06779.pdf [4]

---

### Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±:

28. **Lopez de Prado, M. M. (2020).**
    "Machine Learning for Asset Managers"
    - Cambridge University Press
    - Complements AFML

---

## Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ

### ğŸ¯ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ú©Ù„ÛŒ:

**ÙˆØ¶Ø¹ÛŒØª Ú©Ø¯ ÙØ¹Ù„ÛŒ:**
- âš ï¸ **Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ production Ù†ÛŒØ³Øª**
- ğŸ”´ 5 Ù…Ø³Ø¦Ù„Ù‡ CRITICAL Ú©Ù‡ Ø¨Ø§ÛŒØ¯ ÙÙˆØ±ÛŒ Ø­Ù„ Ø´ÙˆÙ†Ø¯
- ğŸŸ  5 Ù…Ø³Ø¦Ù„Ù‡ HIGH Ú©Ù‡ Ø¯Ù‚Øª Ø±Ø§ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯
- ğŸŸ¡ 5+ Ù…Ø³Ø¦Ù„Ù‡ MEDIUM Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯

**Ø§Ø¹ØªØ¨Ø§Ø± ØªØ­Ù„ÛŒÙ„:**
- âœ… 100% ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹ Ø¹Ù„Ù…ÛŒ Ù…Ø¹ØªØ¨Ø±
- âœ… Ø¨ÛŒØ´ Ø§Ø² 30 Ù…Ù†Ø¨Ø¹ Ø§Ø²:
  - Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (Lopez de Prado)
  - Ù…Ù‚Ø§Ù„Ø§Øª peer-reviewed (Bailey et al.)
  - Wikipedia
  - Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ (mlfinlab, skfolio)
  - Ù…Ù‚Ø§Ù„Ø§Øª arXiv Ø¬Ø¯ÛŒØ¯ (2024-2025)

**Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø§ØµÙ„Ø§Ø­:**
- ÙØ§Ø² 1 (CRITICAL): 2-3 Ù‡ÙØªÙ‡
- ÙØ§Ø² 2 (HIGH): 2-3 Ù‡ÙØªÙ‡
- ÙØ§Ø² 3 (Ù…Ø¹Ù…Ø§Ø±ÛŒ): 4-6 Ù‡ÙØªÙ‡
- ÙØ§Ø² 4 (ØªØ³Øª): 3-4 Ù‡ÙØªÙ‡
- **Ú©Ù„:** 11-16 Ù‡ÙØªÙ‡ (3-4 Ù…Ø§Ù‡)

**ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:**
1. â›” **Ù‡ÛŒÚ†â€ŒÚ¯Ø§Ù‡** Ø§Ø² Ø§ÛŒÙ† Ú©Ø¯ Ø¯Ø± ØªØ±ÛŒØ¯ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø¯ÙˆÙ† Ø§ØµÙ„Ø§Ø­Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯
2. âœ… Ø§Ø¨ØªØ¯Ø§ Ù…Ø³Ø§Ø¦Ù„ CRITICAL Ø±Ø§ Ø­Ù„ Ú©Ù†ÛŒØ¯
3. âœ… Ø³Ù¾Ø³ HIGH PRIORITY
4. âœ… Forward test Ø­Ø¯Ø§Ù‚Ù„ 3 Ù…Ø§Ù‡
5. âœ… Paper trading Ù‚Ø¨Ù„ Ø§Ø² real money

**Ø§Ú¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ù†Ú©Ù†ÛŒØ¯:**
- ğŸ’¸ Ø§Ø­ØªÙ…Ø§Ù„ Ø¶Ø±Ø± Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§
- ğŸ“‰ Performance Ø¯Ø± production Ø¨Ø³ÛŒØ§Ø± Ú©Ù…ØªØ± Ø§Ø² backtest
- ğŸš« Feature selection Ù†Ø§Ø¯Ø±Ø³Øª Ùˆ ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯

---

## Ù¾ÛŒÙˆØ³Øª: Quick Reference

### ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:

**1. Deflated Sharpe Ratio:**
\[
DSR = \frac{\hat{SR} - SR_0}{\sqrt{\text{Var}[\hat{SR}]}}
\]

**2. Sample Weight (Uniqueness):**
\[
w_i = \frac{1}{c_i + 1}
\]
Ú©Ù‡ \(c_i\) = ØªØ¹Ø¯Ø§Ø¯ concurrent labels

**3. PBO:**
\[
PBO = P[\text{OOS rank} \geq \frac{N}{2} | \text{IS optimal}]
\]

**4. Fractional Differentiation:**
\[
\tilde{X}_t = \sum_{k=0}^{l-1} \omega_k X_{t-k}
\]

---

### ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ:

```python
def test_no_data_leakage():
    """Test Ú©Ù‡ Ù‡ÛŒÚ† data leakage ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯"""
    # Test gap calculation
    # Test sample weights
    # Test feature engineering
    pass

def test_reproducibility():
    """Test Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ reproducible Ù‡Ø³ØªÙ†Ø¯"""
    # Ø¨Ø§ random_seed Ø«Ø§Ø¨Øª
    pass

def test_cross_asset_stability():
    """Test Ø±ÙˆÛŒ multiple currency pairs"""
    # EURUSD, GBPUSD, USDJPY, etc.
    pass
```

---

**Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´ÛŒØ¯! ğŸš€**

---

**ØªØ§Ø±ÛŒØ®:** 19 Ù†ÙˆØ§Ù…Ø¨Ø± 2025  
**Ù†Ø³Ø®Ù‡:** 2.0 (Final - Ø¨Ø§ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ù…Ø¬Ø¯Ø¯ Ùˆ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±)  
**ÙˆØ¶Ø¹ÛŒØª:** âœ… ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§ 30+ Ù…Ù†Ø¨Ø¹ Ø¹Ù„Ù…ÛŒ Ù…Ø¹ØªØ¨Ø±
