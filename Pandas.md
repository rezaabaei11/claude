# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Pandas Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø±Ø¨Ø§Øª ØªØ±ÛŒØ¯Ø± ÙØ§Ø±Ú©Ø³ Ø¨Ø§ LightGBM
## Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§ÛŒ 2025 - Ø§Ø² Ù…Ø¨ØªØ¯ÛŒ ØªØ§ Ù¾ÛŒØ´Ø±ÙØªÙ‡

**ØªØ§Ø±ÛŒØ® ØªÙ‡ÛŒÙ‡**: Ø§Ú©ØªØ¨Ø± 2025  
**Ù†Ø³Ø®Ù‡**: 2.0  
**Ù…Ø®ØµÙˆØµ**: Ø³Ø§Ø®Øª Ø±Ø¨Ø§Øª ØªØ±ÛŒØ¯Ø± Ø§ØªÙˆÙ…Ø§Øª Ø¨Ø§ Pandas + LightGBM + Scikit-learn + NumPy + SHAP + Optuna

---

## ğŸ“‹ ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨

1. [Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø­ÛŒØ· ØªÙˆØ³Ø¹Ù‡ (2025 Setup)](#Ø¨Ø®Ø´-0-Ø±Ø§Ù‡-Ø§Ù†Ø¯Ø§Ø²ÛŒ-Ù…Ø­ÛŒØ·-ØªÙˆØ³Ø¹Ù‡-2025)
2. [Ù…Ø¨Ø§Ù†ÛŒ Pandas Ø¨Ø±Ø§ÛŒ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯](#Ø¨Ø®Ø´-1-Ù…Ø¨Ø§Ù†ÛŒ-pandas-Ø¨Ø±Ø§ÛŒ-ØªØ±ÛŒØ¯ÛŒÙ†Ú¯)
3. [Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Time Series ÙØ§Ø±Ú©Ø³](#Ø¨Ø®Ø´-2-Ù¾Ø±Ø¯Ø§Ø²Ø´-Ø¯Ø§Ø¯Ù‡-Ù‡Ø§ÛŒ-time-series-ÙØ§Ø±Ú©Ø³)
4. [Feature Engineering Ø¨Ø±Ø§ÛŒ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯](#Ø¨Ø®Ø´-3-feature-engineering-Ø¨Ø±Ø§ÛŒ-ØªØ±ÛŒØ¯ÛŒÙ†Ú¯)
5. [ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ LightGBM](#Ø¨Ø®Ø´-4-ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡-Ø³Ø§Ø²ÛŒ-Ø¨Ø§-lightgbm)
6. [Pipeline Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„](#Ø¨Ø®Ø´-5-pipeline-Ø³Ø§Ø®Øª-Ù…Ø¯Ù„-Ú©Ø§Ù…Ù„)
7. [Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØ³Øª](#Ø¨Ø®Ø´-6-Ø¨Ù‡ÛŒÙ†Ù‡-Ø³Ø§Ø²ÛŒ-Ùˆ-ØªØ³Øª)
8. [Ø¯ÛŒÙ¾Ù„ÙˆÛŒÙ…Ù†Øª Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù†Ø¯Ù‡](#Ø¨Ø®Ø´-7-Ø¯ÛŒÙ¾Ù„ÙˆÛŒÙ…Ù†Øª-Ùˆ-Ø§Ø¬Ø±Ø§ÛŒ-Ø²Ù†Ø¯Ù‡)

---

## Ø¨Ø®Ø´ 0: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø­ÛŒØ· ØªÙˆØ³Ø¹Ù‡ (2025)

### Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§

```python
# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
pip install pandas==2.2.0  # Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ø§ CoW
pip install numpy>=1.26.0
pip install lightgbm>=4.5.0
pip install scikit-learn>=1.5.0
pip install optuna>=3.6.0
pip install shap>=0.45.0

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ú©Ø³
pip install pandas-market-calendars>=4.4.0
pip install pandas-ta>=0.3.14b  # Ø¨Ø±Ø§ÛŒ Technical Indicators
pip install pyarrow>=15.0.0  # Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§ Ùˆ API
pip install yfinance>=0.2.40
pip install requests>=2.31.0
```

### ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§ÛŒ 2025

```python
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
import optuna
import shap
import warnings

# ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Copy-on-Write (Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ 2025)
pd.options.mode.copy_on_write = True

# ØªÙ†Ø¸ÛŒÙ… PyArrow Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
pd.options.mode.dtype_backend = 'pyarrow'

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´
pd.set_option('display.max_columns', None)
pd.set_option('display.precision', 6)
pd.set_option('display.float_format', '{:.6f}'.format)

warnings.filterwarnings('ignore')

print(f"Pandas Version: {pd.__version__}")
print(f"NumPy Version: {np.__version__}")
print(f"LightGBM Version: {lgb.__version__}")
```

---

## Ø¨Ø®Ø´ 1: Ù…Ø¨Ø§Ù†ÛŒ Pandas Ø¨Ø±Ø§ÛŒ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯

### 1.1 Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ú©Ø³

```python
# âœ… Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ OHLCV Ø§Ø² CSV
def load_forex_data(filepath, pair='EURUSD'):
    """
    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ú©Ø³ Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
    """
    df = pd.read_csv(
        filepath,
        parse_dates=['datetime'],
        index_col='datetime',
        dtype_backend='pyarrow',  # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ 2025
        usecols=['datetime', 'open', 'high', 'low', 'close', 'volume']
    )
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ±ØªÛŒØ¨ Ø²Ù…Ø§Ù†ÛŒ
    df = df.sort_index()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Missing Values
    print(f"Missing Values:\n{df.isnull().sum()}")
    print(f"Data Shape: {df.shape}")
    print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    return df

# Ù…Ø«Ø§Ù„: Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡
df_forex = load_forex_data('data/EURUSD_1H.csv')
print(df_forex.head())
```

### 1.2 Ù…Ø¯ÛŒØ±ÛŒØª Timezone Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ú©Ø³ (24/7 Market)

```python
import pandas_market_calendars as mcal
from pytz import timezone

def prepare_forex_timezone(df, tz='UTC'):
    """
    Ù…Ø¯ÛŒØ±ÛŒØª timezone Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± ÙØ§Ø±Ú©Ø³ Ú©Ù‡ 24/7 Ø§Ø³Øª
    """
    # ØªÙ†Ø¸ÛŒÙ… timezone
    if df.index.tz is None:
        df.index = df.index.tz_localize('UTC')
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ timezone Ø¯Ù„Ø®ÙˆØ§Ù‡
    df.index = df.index.tz_convert(tz)
    
    # Ø§ÙØ²ÙˆØ¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
    df['hour'] = df.index.hour
    df['day_of_week'] = df.index.dayofweek
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    
    # Ø¬Ù„Ø³Ø§Øª Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ ÙØ§Ø±Ú©Ø³
    df['session'] = pd.cut(
        df['hour'],
        bins=[0, 7, 15, 21, 24],
        labels=['Asian', 'London', 'NY', 'Pacific'],
        include_lowest=True
    )
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex = prepare_forex_timezone(df_forex, tz='America/New_York')
print(df_forex[['open', 'close', 'hour', 'session']].head())
```

### 1.3 Ù…Ø¯ÛŒØ±ÛŒØª Missing Data Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ú©Ø³

```python
def handle_forex_missing_data(df, method='ffill'):
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Missing Data Ø¯Ø± ÙØ§Ø±Ú©Ø³
    
    Parameters:
    -----------
    method : 'ffill', 'interpolate', 'drop'
    """
    print(f"Missing data before: {df.isnull().sum().sum()}")
    
    if method == 'ffill':
        # Forward Fill - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§
        df = df.ffill(limit=5)  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ§Ù¾ÛŒ
        
    elif method == 'interpolate':
        # Interpolation - Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].interpolate(
            method='time',  # Ø¨Ø±Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
            limit_direction='forward',
            limit=5
        )
        
    elif method == 'drop':
        # Ø­Ø°Ù - Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…ØªØ± Ø§Ø² 1%
        df = df.dropna()
    
    print(f"Missing data after: {df.isnull().sum().sum()}")
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex = handle_forex_missing_data(df_forex, method='interpolate')
```

---

## Ø¨Ø®Ø´ 2: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Time Series ÙØ§Ø±Ú©Ø³

### 2.1 Resampling Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (ØªØ¨Ø¯ÛŒÙ„ Timeframe)

```python
def resample_forex_data(df, timeframe='4H', method='ohlc'):
    """
    ØªØ¨Ø¯ÛŒÙ„ timeframe Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ú©Ø³
    
    Timeframes: '1T', '5T', '15T', '1H', '4H', 'D', 'W', 'M'
    """
    
    if method == 'ohlc':
        # OHLC Resampling
        resampled = df['close'].resample(timeframe).ohlc()
        resampled['volume'] = df['volume'].resample(timeframe).sum()
        
    elif method == 'last':
        # Ø¢Ø®Ø±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø±
        resampled = df.resample(timeframe).last()
        
    elif method == 'mean':
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
        resampled = df.resample(timeframe).mean()
    
    # Ø­Ø°Ù NaN
    resampled = resampled.dropna()
    
    print(f"Original shape: {df.shape}")
    print(f"Resampled shape: {resampled.shape}")
    
    return resampled

# Ù…Ø«Ø§Ù„: ØªØ¨Ø¯ÛŒÙ„ Ø§Ø² 1H Ø¨Ù‡ 4H
df_4h = resample_forex_data(df_forex, timeframe='4H')
print(df_4h.head())
```

### 2.2 Rolling Windows Ø¨Ø±Ø§ÛŒ Technical Indicators

```python
def calculate_rolling_features(df, windows=[5, 10, 20, 50, 200]):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Rolling Window
    """
    df = df.copy()
    
    for window in windows:
        # Moving Averages
        df[f'sma_{window}'] = df['close'].rolling(
            window=window, 
            min_periods=1
        ).mean()
        
        df[f'ema_{window}'] = df['close'].ewm(
            span=window, 
            adjust=False
        ).mean()
        
        # Volatility
        df[f'std_{window}'] = df['close'].rolling(
            window=window,
            min_periods=1
        ).std()
        
        # High-Low Range
        df[f'range_{window}'] = (
            df['high'].rolling(window).max() - 
            df['low'].rolling(window).min()
        )
        
        # Volume Moving Average
        df[f'volume_ma_{window}'] = df['volume'].rolling(
            window=window
        ).mean()
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex = calculate_rolling_features(df_forex)
print(df_forex[['close', 'sma_20', 'ema_20', 'std_20']].tail())
```

### 2.3 Shift & Lag Features (ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ§Ø®ÛŒØ±ÛŒ)

```python
def create_lag_features(df, lags=[1, 2, 3, 5, 10], columns=['close', 'volume']):
    """
    Ø³Ø§Ø®Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Lag Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ML
    """
    df = df.copy()
    
    for col in columns:
        for lag in lags:
            # Lag features
            df[f'{col}_lag_{lag}'] = df[col].shift(lag)
            
            # Percentage change
            df[f'{col}_pct_change_{lag}'] = df[col].pct_change(lag)
            
            # Difference
            df[f'{col}_diff_{lag}'] = df[col].diff(lag)
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex = create_lag_features(
    df_forex, 
    lags=[1, 2, 3, 5, 10, 20],
    columns=['close', 'volume']
)
```

### 2.4 Expanding Window Features

```python
def calculate_expanding_features(df):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Expanding (ØªØ¬Ù…Ø¹ÛŒ)
    """
    df = df.copy()
    
    # Expanding Statistics
    df['cumsum_return'] = df['close'].pct_change().expanding().sum()
    df['cummax'] = df['close'].expanding().max()
    df['cummin'] = df['close'].expanding().min()
    
    # Drawdown
    df['drawdown'] = (df['close'] - df['cummax']) / df['cummax']
    df['max_drawdown'] = df['drawdown'].expanding().min()
    
    # Expanding Volatility
    df['expanding_vol'] = df['close'].pct_change().expanding().std()
    
    return df

df_forex = calculate_expanding_features(df_forex)
```

---

## Ø¨Ø®Ø´ 3: Feature Engineering Ø¨Ø±Ø§ÛŒ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯

### 3.1 Technical Indicators Ø¨Ø§ pandas_ta

```python
import pandas_ta as ta

def add_technical_indicators(df):
    """
    Ø§ÙØ²ÙˆØ¯Ù† Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ pandas_ta
    """
    df = df.copy()
    
    # Trend Indicators
    df.ta.sma(length=20, append=True)
    df.ta.ema(length=12, append=True)
    df.ta.ema(length=26, append=True)
    
    # Momentum Indicators
    df.ta.rsi(length=14, append=True)
    df.ta.macd(fast=12, slow=26, signal=9, append=True)
    df.ta.stoch(append=True)  # Stochastic
    
    # Volatility Indicators
    df.ta.bbands(length=20, std=2, append=True)  # Bollinger Bands
    df.ta.atr(length=14, append=True)  # Average True Range
    
    # Volume Indicators
    df.ta.obv(append=True)  # On-Balance Volume
    df.ta.ad(append=True)   # Accumulation/Distribution
    
    # Support/Resistance
    df.ta.pivots(append=True)
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex = add_technical_indicators(df_forex)
print(df_forex.columns.tolist())
```

### 3.2 Custom Technical Indicators

```python
def calculate_custom_indicators(df):
    """
    Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø³ÙØ§Ø±Ø´ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ú©Ø³
    """
    df = df.copy()
    
    # 1. Price Action Features
    df['body'] = abs(df['close'] - df['open'])
    df['upper_shadow'] = df['high'] - df[['open', 'close']].max(axis=1)
    df['lower_shadow'] = df[['open', 'close']].min(axis=1) - df['low']
    
    df['is_bullish'] = (df['close'] > df['open']).astype(int)
    df['is_doji'] = (df['body'] < (df['high'] - df['low']) * 0.1).astype(int)
    
    # 2. Momentum Features
    df['momentum_5'] = df['close'] - df['close'].shift(5)
    df['momentum_10'] = df['close'] - df['close'].shift(10)
    df['momentum_20'] = df['close'] - df['close'].shift(20)
    
    # 3. Volatility Ratio
    df['volatility_ratio'] = (
        df['close'].rolling(5).std() / 
        df['close'].rolling(20).std()
    )
    
    # 4. Volume Profile
    df['volume_ratio'] = (
        df['volume'] / 
        df['volume'].rolling(20).mean()
    )
    
    # 5. Price Distance from MA
    df['distance_sma20'] = (df['close'] - df['sma_20']) / df['sma_20']
    df['distance_ema50'] = (df['close'] - df['ema_50']) / df['ema_50']
    
    # 6. Trend Strength
    df['trend_strength'] = (
        df['close'].rolling(20).apply(
            lambda x: np.polyfit(range(len(x)), x, 1)[0]
        )
    )
    
    return df

df_forex = calculate_custom_indicators(df_forex)
```

### 3.3 Multi-Timeframe Features

```python
def create_multitimeframe_features(df_1h, timeframes=['4H', 'D']):
    """
    Ø³Ø§Ø®Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…
    """
    df = df_1h.copy()
    
    for tf in timeframes:
        # Resample Ø¨Ù‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø¨Ø§Ù„Ø§ØªØ±
        df_higher = resample_forex_data(df, timeframe=tf)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§
        df_higher['rsi'] = ta.rsi(df_higher['close'], length=14)
        df_higher['macd'] = ta.macd(df_higher['close'])['MACD_12_26_9']
        df_higher['trend'] = np.where(
            df_higher['close'] > df_higher['close'].rolling(20).mean(),
            1, -1
        )
        
        # Merge Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ
        df_higher = df_higher.add_suffix(f'_{tf}')
        df = df.join(df_higher, how='left', rsuffix=f'_{tf}')
        
        # Forward fill
        df = df.ffill()
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex = create_multitimeframe_features(df_forex, timeframes=['4H', 'D'])
```

### 3.4 Target Variable (Label) Ø³Ø§Ø®Øª

```python
def create_target_variable(df, method='classification', horizon=5, threshold=0.001):
    """
    Ø³Ø§Ø®Øª Ù…ØªØºÛŒØ± Ù‡Ø¯Ù Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ML
    
    Parameters:
    -----------
    method : 'classification', 'regression'
    horizon : ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡
    threshold : Ø¢Ø³ØªØ§Ù†Ù‡ ØªØºÛŒÛŒØ± Ù‚ÛŒÙ…Øª (Ø¨Ø±Ø§ÛŒ classification)
    """
    df = df.copy()
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ø¯Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡
    df['future_return'] = df['close'].pct_change(horizon).shift(-horizon)
    
    if method == 'classification':
        # Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ: BUY (1), SELL (-1), HOLD (0)
        df['target'] = np.where(
            df['future_return'] > threshold, 1,
            np.where(df['future_return'] < -threshold, -1, 0)
        )
        
        print(f"Target Distribution:\n{df['target'].value_counts()}")
        
    elif method == 'regression':
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§Ø²Ø¯Ù‡
        df['target'] = df['future_return']
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Classification
df_forex = create_target_variable(
    df_forex, 
    method='classification',
    horizon=5,
    threshold=0.0015  # 0.15% Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ú©Ø³
)
```

### 3.5 Feature Selection Ø¨Ø§ Pandas

```python
def select_features(df, target_col='target', method='correlation', threshold=0.05):
    """
    Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
    """
    # Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = numeric_cols.drop([target_col], errors='ignore')
    
    if method == 'correlation':
        # Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§ target
        correlations = df[numeric_cols].corrwith(df[target_col]).abs()
        selected_features = correlations[correlations > threshold].index.tolist()
        
        print(f"Selected {len(selected_features)} features")
        print(f"Top 10 correlations:\n{correlations.nlargest(10)}")
        
    elif method == 'variance':
        # Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ú©Ù…
        from sklearn.feature_selection import VarianceThreshold
        
        selector = VarianceThreshold(threshold=threshold)
        selector.fit(df[numeric_cols])
        
        selected_features = numeric_cols[selector.get_support()].tolist()
        print(f"Selected {len(selected_features)} features")
    
    return selected_features

# Ø§Ø³ØªÙØ§Ø¯Ù‡
selected_features = select_features(df_forex, target_col='target', threshold=0.02)
```

---

## Ø¨Ø®Ø´ 4: ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ LightGBM

### 4.1 Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ LightGBM

```python
def prepare_data_for_lgbm(df, target_col='target', test_size=0.2):
    """
    Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ LightGBM
    """
    # Ø­Ø°Ù NaN
    df = df.dropna()
    
    # Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Features Ùˆ Target
    feature_cols = [col for col in df.columns if col not in [
        target_col, 'future_return', 'open', 'high', 'low', 'close', 'volume'
    ]]
    
    X = df[feature_cols]
    y = df[target_col]
    
    # Split Train/Test Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù† (Ù…Ù‡Ù…!)
    split_index = int(len(df) * (1 - test_size))
    
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]
    
    print(f"Train shape: {X_train.shape}")
    print(f"Test shape: {X_test.shape}")
    print(f"Features: {len(feature_cols)}")
    
    return X_train, X_test, y_train, y_test, feature_cols

# Ø§Ø³ØªÙØ§Ø¯Ù‡
X_train, X_test, y_train, y_test, features = prepare_data_for_lgbm(df_forex)
```

### 4.2 Ø³Ø§Ø®Øª Ù…Ø¯Ù„ LightGBM Ù¾Ø§ÛŒÙ‡

```python
def train_lgbm_baseline(X_train, y_train, X_test, y_test, task='classification'):
    """
    Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LightGBM Ù¾Ø§ÛŒÙ‡
    """
    if task == 'classification':
        params = {
            'objective': 'multiclass',
            'num_class': 3,  # BUY, SELL, HOLD
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'max_depth': 7,
            'min_data_in_leaf': 50,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'verbose': -1,
            'force_row_wise': True,  # Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±
        }
        
    elif task == 'regression':
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }
    
    # Ø³Ø§Ø®Øª Dataset
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # Ø¢Ù…ÙˆØ²Ø´
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[train_data, valid_data],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=100)
        ]
    )
    
    return model

# Ø¢Ù…ÙˆØ²Ø´
model = train_lgbm_baseline(X_train, y_train, X_test, y_test)
```

### 4.3 Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Hyperparameter Ø¨Ø§ Optuna

```python
def optimize_lgbm_with_optuna(X_train, y_train, X_test, y_test, n_trials=100):
    """
    Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ LightGBM Ø¨Ø§ Optuna
    """
    def objective(trial):
        # ÙØ¶Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        params = {
            'objective': 'multiclass',
            'num_class': 3,
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 10, 100),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'verbose': -1
        }
        
        # Dataset
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
        
        # Ø¢Ù…ÙˆØ²Ø´
        model = lgb.train(
            params,
            train_data,
            num_boost_round=500,
            valid_sets=[valid_data],
            valid_names=['valid'],
            callbacks=[lgb.early_stopping(stopping_rounds=30)]
        )
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        y_pred = model.predict(X_test)
        y_pred_class = np.argmax(y_pred, axis=1)
        
        from sklearn.metrics import accuracy_score
        accuracy = accuracy_score(y_test, y_pred_class)
        
        return accuracy
    
    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
    
    print("Best trial:")
    print(f"  Accuracy: {study.best_value:.4f}")
    print(f"  Params: {study.best_params}")
    
    return study.best_params

# Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
best_params = optimize_lgbm_with_optuna(
    X_train, y_train, X_test, y_test, 
    n_trials=50
)
```

### 4.4 ØªÙØ³ÛŒØ± Ù…Ø¯Ù„ Ø¨Ø§ SHAP

```python
def explain_model_with_shap(model, X_test, feature_names, max_display=20):
    """
    ØªÙØ³ÛŒØ± Ù…Ø¯Ù„ LightGBM Ø¨Ø§ SHAP
    """
    import matplotlib.pyplot as plt
    
    # Ø³Ø§Ø®Øª Explainer
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test)
    
    # Summary Plot
    plt.figure(figsize=(12, 8))
    shap.summary_plot(
        shap_values, 
        X_test, 
        feature_names=feature_names,
        max_display=max_display,
        show=False
    )
    plt.tight_layout()
    plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Feature Importance
    shap_importance = np.abs(shap_values).mean(axis=0).mean(axis=0)
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': shap_importance
    }).sort_values('importance', ascending=False)
    
    print("Top 20 Important Features (SHAP):")
    print(feature_importance_df.head(20))
    
    return feature_importance_df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
shap_importance = explain_model_with_shap(model, X_test, features)
```

---

## Ø¨Ø®Ø´ 5: Pipeline Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„

### 5.1 Ø³Ø§Ø®Øª Pipeline Ø¨Ø§ Scikit-learn

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.compose import ColumnTransformer

def create_trading_pipeline(numeric_features, categorical_features=None):
    """
    Ø³Ø§Ø®Øª Pipeline Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ
    """
    # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
    numeric_transformer = Pipeline(steps=[
        ('scaler', RobustScaler())  # Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø¨Ø±Ø§ÛŒ outliers
    ])
    
    # ColumnTransformer
    if categorical_features:
        from sklearn.preprocessing import OneHotEncoder
        
        categorical_transformer = Pipeline(steps=[
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ])
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ]
        )
    else:
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features)
            ]
        )
    
    # Pipeline Ú©Ø§Ù…Ù„
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor)
    ])
    
    # ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ set_output Ø¨Ø±Ø§ÛŒ Pandas (2025 feature)
    pipeline.set_output(transform="pandas")
    
    return pipeline

# Ø§Ø³ØªÙØ§Ø¯Ù‡
numeric_features = [col for col in features if 'session' not in col]
pipeline = create_trading_pipeline(numeric_features)

# Transform
X_train_scaled = pipeline.fit_transform(X_train)
X_test_scaled = pipeline.transform(X_test)
```

### 5.2 Walk-Forward Optimization

```python
def walk_forward_optimization(df, feature_cols, target_col, 
                               train_window=1000, test_window=200, 
                               step=100):
    """
    Walk-Forward Optimization Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙˆØ§Ù‚Ø¹ÛŒâ€ŒØªØ±
    """
    results = []
    models = []
    
    # ØªØ¹Ø¯Ø§Ø¯ splits
    n_splits = (len(df) - train_window) // step
    
    for i in range(n_splits):
        # ØªØ¹Ø±ÛŒÙ Ø¨Ø§Ø²Ù‡â€ŒÙ‡Ø§ÛŒ train Ùˆ test
        train_start = i * step
        train_end = train_start + train_window
        test_start = train_end
        test_end = min(test_start + test_window, len(df))
        
        if test_end - test_start < 50:  # Ø­Ø¯Ø§Ù‚Ù„ 50 Ù†Ù…ÙˆÙ†Ù‡ test
            break
        
        # Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
        X_train_wf = df.iloc[train_start:train_end][feature_cols]
        y_train_wf = df.iloc[train_start:train_end][target_col]
        X_test_wf = df.iloc[test_start:test_end][feature_cols]
        y_test_wf = df.iloc[test_start:test_end][target_col]
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
        params = {
            'objective': 'multiclass',
            'num_class': 3,
            'num_leaves': 31,
            'learning_rate': 0.05,
            'verbose': -1
        }
        
        train_data = lgb.Dataset(X_train_wf, label=y_train_wf)
        model_wf = lgb.train(params, train_data, num_boost_round=200)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        y_pred_wf = model_wf.predict(X_test_wf)
        y_pred_class = np.argmax(y_pred_wf, axis=1)
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        from sklearn.metrics import accuracy_score, precision_score, recall_score
        
        accuracy = accuracy_score(y_test_wf, y_pred_class)
        precision = precision_score(y_test_wf, y_pred_class, average='weighted')
        recall = recall_score(y_test_wf, y_pred_class, average='weighted')
        
        results.append({
            'fold': i,
            'train_end': train_end,
            'test_end': test_end,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall
        })
        
        models.append(model_wf)
        
        print(f"Fold {i}: Accuracy={accuracy:.4f}, Precision={precision:.4f}")
    
    # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
    results_df = pd.DataFrame(results)
    print("\nWalk-Forward Results Summary:")
    print(results_df.describe())
    
    return results_df, models

# Ø§Ø³ØªÙØ§Ø¯Ù‡
wf_results, wf_models = walk_forward_optimization(
    df_forex, features, 'target',
    train_window=5000, test_window=1000, step=500
)
```

### 5.3 Ensemble Models

```python
def create_ensemble_model(models, X_test):
    """
    Ø³Ø§Ø®Øª Ensemble Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„ LightGBM
    """
    predictions = []
    
    for model in models:
        pred = model.predict(X_test)
        predictions.append(pred)
    
    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
    ensemble_pred = np.mean(predictions, axis=0)
    ensemble_class = np.argmax(ensemble_pred, axis=1)
    
    return ensemble_class, ensemble_pred

# Ø§Ø³ØªÙØ§Ø¯Ù‡
ensemble_predictions, ensemble_probs = create_ensemble_model(wf_models[:5], X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, ensemble_predictions))
```

---

## Ø¨Ø®Ø´ 6: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØ³Øª

### 6.1 Backtesting Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ

```python
def backtest_trading_strategy(df, predictions, initial_capital=10000, 
                               pip_value=10, spread=2):
    """
    Backtesting Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯
    
    Parameters:
    -----------
    predictions : array Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯ (1=BUY, -1=SELL, 0=HOLD)
    pip_value : Ø§Ø±Ø²Ø´ Ù‡Ø± pip
    spread : Ø§Ø³Ù¾Ø±Ø¯ (Ø¨Ù‡ pip)
    """
    df = df.copy()
    df['signal'] = predictions
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³ÙˆØ¯/Ø²ÛŒØ§Ù†
    df['returns'] = df['close'].pct_change()
    df['strategy_returns'] = df['signal'].shift(1) * df['returns']
    
    # Ú©Ø§Ù‡Ø´ spread
    df['strategy_returns'] = df['strategy_returns'] - (spread * 0.0001)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Equity
    df['cumulative_returns'] = (1 + df['strategy_returns']).cumprod()
    df['equity'] = initial_capital * df['cumulative_returns']
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Drawdown
    df['cummax_equity'] = df['equity'].cummax()
    df['drawdown'] = (df['equity'] - df['cummax_equity']) / df['cummax_equity']
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Metrics
    total_return = (df['equity'].iloc[-1] - initial_capital) / initial_capital
    max_drawdown = df['drawdown'].min()
    sharpe_ratio = df['strategy_returns'].mean() / df['strategy_returns'].std() * np.sqrt(252 * 24)  # ÙØ§Ø±Ú©Ø³ 24/5
    
    # ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    df['position_change'] = df['signal'].diff()
    num_trades = (df['position_change'] != 0).sum()
    
    # Win Rate
    winning_trades = (df['strategy_returns'] > 0).sum()
    win_rate = winning_trades / num_trades if num_trades > 0 else 0
    
    print("="*50)
    print("Backtest Results:")
    print("="*50)
    print(f"Initial Capital: ${initial_capital:,.2f}")
    print(f"Final Equity: ${df['equity'].iloc[-1]:,.2f}")
    print(f"Total Return: {total_return:.2%}")
    print(f"Max Drawdown: {max_drawdown:.2%}")
    print(f"Sharpe Ratio: {sharpe_ratio:.2f}")
    print(f"Number of Trades: {num_trades}")
    print(f"Win Rate: {win_rate:.2%}")
    print("="*50)
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_backtest = backtest_trading_strategy(
    df_forex.loc[X_test.index], 
    ensemble_predictions,
    initial_capital=10000
)

# Ø±Ø³Ù… Equity Curve
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 7))
plt.plot(df_backtest.index, df_backtest['equity'], label='Strategy Equity')
plt.axhline(y=10000, color='r', linestyle='--', label='Initial Capital')
plt.title('Equity Curve - Trading Strategy')
plt.xlabel('Date')
plt.ylabel('Equity ($)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('equity_curve.png', dpi=300)
plt.show()
```

### 6.2 Risk Management

```python
def calculate_position_size(equity, risk_percent, stop_loss_pips, pip_value=10):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§ÛŒØ² Ù¾ÙˆØ²ÛŒØ´Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú©
    
    Parameters:
    -----------
    equity : Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø­Ø³Ø§Ø¨
    risk_percent : Ø¯Ø±ØµØ¯ Ø±ÛŒØ³Ú© (Ù…Ø¹Ù…ÙˆÙ„Ø§ 1-2%)
    stop_loss_pips : ØªØ¹Ø¯Ø§Ø¯ pip Ø§Ø³ØªØ§Ù¾ Ù„Ø§Ø³
    pip_value : Ø§Ø±Ø²Ø´ Ù‡Ø± pip
    """
    risk_amount = equity * (risk_percent / 100)
    position_size = risk_amount / (stop_loss_pips * pip_value)
    
    return position_size

def add_risk_management(df, predictions, equity=10000, risk_percent=1.5):
    """
    Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Stop Loss Ùˆ Take Profit
    """
    df = df.copy()
    df['signal'] = predictions
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ø¨Ø±Ø§ÛŒ Stop Loss Ù¾ÙˆÛŒØ§
    df['atr'] = df.ta.atr(length=14)
    
    # Stop Loss: 2 * ATR
    df['stop_loss_pips'] = df['atr'] * 10000 * 2  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ pip
    
    # Take Profit: 3 * ATR (Risk-Reward 1:1.5)
    df['take_profit_pips'] = df['atr'] * 10000 * 3
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Position Size
    df['position_size'] = df.apply(
        lambda row: calculate_position_size(
            equity, risk_percent, row['stop_loss_pips']
        ),
        axis=1
    )
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex_risk = add_risk_management(df_forex, ensemble_predictions)
print(df_forex_risk[['close', 'signal', 'atr', 'stop_loss_pips', 'position_size']].tail())
```

### 6.3 Performance Analysis

```python
def analyze_trading_performance(df):
    """
    ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯
    """
    results = {}
    
    # 1. Return Metrics
    results['total_return'] = df['strategy_returns'].sum()
    results['mean_return'] = df['strategy_returns'].mean()
    results['std_return'] = df['strategy_returns'].std()
    
    # 2. Risk Metrics
    results['sharpe_ratio'] = (
        results['mean_return'] / results['std_return'] * np.sqrt(252 * 24)
    )
    results['max_drawdown'] = df['drawdown'].min()
    results['calmar_ratio'] = (
        results['total_return'] / abs(results['max_drawdown'])
    )
    
    # 3. Trade Metrics
    df['trade'] = (df['signal'] != df['signal'].shift()).astype(int)
    results['num_trades'] = df['trade'].sum()
    
    # Winning/Losing Trades
    trades_returns = df[df['trade'] == 1]['strategy_returns']
    results['num_winning'] = (trades_returns > 0).sum()
    results['num_losing'] = (trades_returns < 0).sum()
    results['win_rate'] = results['num_winning'] / results['num_trades']
    
    # Average Win/Loss
    results['avg_win'] = trades_returns[trades_returns > 0].mean()
    results['avg_loss'] = trades_returns[trades_returns < 0].mean()
    results['profit_factor'] = abs(results['avg_win'] / results['avg_loss'])
    
    # 4. Exposure
    results['exposure'] = (df['signal'] != 0).sum() / len(df)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    print("\n" + "="*60)
    print("TRADING PERFORMANCE ANALYSIS")
    print("="*60)
    
    for key, value in results.items():
        if 'ratio' in key or 'rate' in key or 'factor' in key:
            print(f"{key.replace('_', ' ').title():<30}: {value:>10.4f}")
        elif 'return' in key:
            print(f"{key.replace('_', ' ').title():<30}: {value:>10.4%}")
        else:
            print(f"{key.replace('_', ' ').title():<30}: {value:>10.2f}")
    
    print("="*60 + "\n")
    
    return pd.Series(results)

# Ø§Ø³ØªÙØ§Ø¯Ù‡
performance_metrics = analyze_trading_performance(df_backtest)
```

---

## Ø¨Ø®Ø´ 7: Ø¯ÛŒÙ¾Ù„ÙˆÛŒÙ…Ù†Øª Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù†Ø¯Ù‡

### 7.1 Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„

```python
import joblib
import json

def save_trading_model(model, pipeline, feature_names, params, filepath='models/'):
    """
    Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    """
    import os
    os.makedirs(filepath, exist_ok=True)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ LightGBM
    model.save_model(f'{filepath}lgbm_model.txt')
    
    # Ø°Ø®ÛŒØ±Ù‡ Pipeline
    joblib.dump(pipeline, f'{filepath}pipeline.pkl')
    
    # Ø°Ø®ÛŒØ±Ù‡ Feature Names Ùˆ Params
    metadata = {
        'feature_names': feature_names,
        'params': params,
        'num_features': len(feature_names)
    }
    
    with open(f'{filepath}metadata.json', 'w') as f:
        json.dump(metadata, f, indent=4)
    
    print(f"Model saved to {filepath}")

def load_trading_model(filepath='models/'):
    """
    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    """
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
    model = lgb.Booster(model_file=f'{filepath}lgbm_model.txt')
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Pipeline
    pipeline = joblib.load(f'{filepath}pipeline.pkl')
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Metadata
    with open(f'{filepath}metadata.json', 'r') as f:
        metadata = json.load(f)
    
    print(f"Model loaded from {filepath}")
    print(f"Number of features: {metadata['num_features']}")
    
    return model, pipeline, metadata

# Ø°Ø®ÛŒØ±Ù‡
save_trading_model(model, pipeline, features, best_params)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
model_loaded, pipeline_loaded, metadata = load_trading_model()
```

### 7.2 Real-Time Prediction

```python
class ForexTradingBot:
    """
    Ø±Ø¨Ø§Øª ØªØ±ÛŒØ¯Ø± ÙØ§Ø±Ú©Ø³ Ø¨Ø§ LightGBM
    """
    def __init__(self, model_path='models/'):
        self.model, self.pipeline, self.metadata = load_trading_model(model_path)
        self.feature_names = self.metadata['feature_names']
        
    def preprocess_live_data(self, df_live):
        """
        Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ Ø²Ù†Ø¯Ù‡
        """
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ù…Ù‡ ÙÛŒÚ†Ø±Ù‡Ø§
        df = df_live.copy()
        df = calculate_rolling_features(df)
        df = add_technical_indicators(df)
        df = calculate_custom_indicators(df)
        df = create_lag_features(df)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¢Ø®Ø±ÛŒÙ† Ø±Ø¯ÛŒÙ
        df_latest = df[self.feature_names].iloc[[-1]]
        
        return df_latest
    
    def predict(self, df_live):
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯
        """
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        X = self.preprocess_live_data(df_live)
        
        # Transform Ø¨Ø§ Pipeline
        X_scaled = self.pipeline.transform(X)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        pred_proba = self.model.predict(X_scaled)
        pred_class = np.argmax(pred_proba, axis=1)[0]
        
        # Mapping
        signal_map = {0: 'SELL', 1: 'HOLD', 2: 'BUY'}
        signal = signal_map.get(pred_class, 'HOLD')
        
        confidence = pred_proba[0][pred_class]
        
        return {
            'signal': signal,
            'confidence': float(confidence),
            'probabilities': {
                'SELL': float(pred_proba[0][0]),
                'HOLD': float(pred_proba[0][1]),
                'BUY': float(pred_proba[0][2])
            }
        }
    
    def generate_trading_signal(self, df_live, min_confidence=0.6):
        """
        ØªÙˆÙ„ÛŒØ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯ Ø¨Ø§ ÙÛŒÙ„ØªØ± confidence
        """
        prediction = self.predict(df_live)
        
        if prediction['confidence'] >= min_confidence:
            return prediction
        else:
            return {
                'signal': 'HOLD',
                'confidence': prediction['confidence'],
                'reason': 'Low confidence'
            }

# Ø§Ø³ØªÙØ§Ø¯Ù‡
bot = ForexTradingBot(model_path='models/')

# ÙØ±Ø¶: Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø²Ù†Ø¯Ù‡
df_live = df_forex.iloc[-100:]  # 100 Ú©Ù†Ø¯Ù„ Ø¢Ø®Ø±

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
signal = bot.generate_trading_signal(df_live, min_confidence=0.65)
print(f"Signal: {signal['signal']}")
print(f"Confidence: {signal['confidence']:.2%}")
print(f"Probabilities: {signal.get('probabilities', {})}")
```

### 7.3 Live Trading Integration

```python
import time
from datetime import datetime

def live_trading_loop(bot, api_client, pair='EURUSD', 
                       timeframe='1H', check_interval=60):
    """
    Ø­Ù„Ù‚Ù‡ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯ Ø²Ù†Ø¯Ù‡
    
    Parameters:
    -----------
    api_client : Ú©Ù„Ø§ÛŒÙ†Øª API Ø¨Ø±ÙˆÚ©Ø± (Ù…Ø«Ù„Ø§ MetaTrader, OANDA)
    check_interval : ÙØ§ØµÙ„Ù‡ Ú†Ú© Ú©Ø±Ø¯Ù† (Ø«Ø§Ù†ÛŒÙ‡)
    """
    print(f"Starting live trading for {pair} - {timeframe}")
    print(f"Check interval: {check_interval}s")
    print("-" * 60)
    
    while True:
        try:
            # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø¢Ø®Ø±ÛŒÙ† 200 Ú©Ù†Ø¯Ù„
            df_live = api_client.get_historical_data(
                pair=pair,
                timeframe=timeframe,
                count=200
            )
            
            # ØªÙˆÙ„ÛŒØ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„
            signal = bot.generate_trading_signal(df_live, min_confidence=0.65)
            
            # Log
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"[{timestamp}] Signal: {signal['signal']} | "
                  f"Confidence: {signal['confidence']:.2%}")
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ù‡
            if signal['signal'] == 'BUY':
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Stop Loss Ùˆ Take Profit
                current_price = df_live['close'].iloc[-1]
                atr = df_live['atr'].iloc[-1]
                
                stop_loss = current_price - (2 * atr)
                take_profit = current_price + (3 * atr)
                
                # Ø§Ø±Ø³Ø§Ù„ Ø³ÙØ§Ø±Ø´
                order = api_client.place_order(
                    pair=pair,
                    side='BUY',
                    volume=0.01,  # Lot size
                    stop_loss=stop_loss,
                    take_profit=take_profit
                )
                
                print(f"  --> BUY order placed: {order['order_id']}")
                print(f"      SL: {stop_loss:.5f} | TP: {take_profit:.5f}")
                
            elif signal['signal'] == 'SELL':
                # Ù…Ø´Ø§Ø¨Ù‡ BUY
                current_price = df_live['close'].iloc[-1]
                atr = df_live['atr'].iloc[-1]
                
                stop_loss = current_price + (2 * atr)
                take_profit = current_price - (3 * atr)
                
                order = api_client.place_order(
                    pair=pair,
                    side='SELL',
                    volume=0.01,
                    stop_loss=stop_loss,
                    take_profit=take_profit
                )
                
                print(f"  --> SELL order placed: {order['order_id']}")
                print(f"      SL: {stop_loss:.5f} | TP: {take_profit:.5f}")
            
            # Ø§Ù†ØªØ¸Ø§Ø± ØªØ§ Ú†Ú© Ø¨Ø¹Ø¯ÛŒ
            time.sleep(check_interval)
            
        except KeyboardInterrupt:
            print("\nStopping live trading...")
            break
            
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(check_interval)

# ØªÙˆØ¬Ù‡: Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ api_client Ø¯Ø§Ø±ÛŒØ¯
# Ù…Ø«Ø§Ù„: MT5Client, OANDAClient, etc.
```

### 7.4 Monitoring Ùˆ Logging

```python
import logging
from datetime import datetime

def setup_trading_logger(log_file='logs/trading.log'):
    """
    Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Logger Ø¨Ø±Ø§ÛŒ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯
    """
    import os
    os.makedirs('logs', exist_ok=True)
    
    logger = logging.getLogger('TradingBot')
    logger.setLevel(logging.INFO)
    
    # File Handler
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.INFO)
    
    # Console Handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

# Ø§Ø³ØªÙØ§Ø¯Ù‡
logger = setup_trading_logger()

logger.info("Trading bot started")
logger.info(f"Model loaded: LightGBM with {len(features)} features")
logger.warning("Low confidence signal detected")
logger.error("API connection failed")
```

---

## Ø¨Ø®Ø´ 8: Ù†Ú©Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ùˆ Best Practices

### 8.1 Memory Optimization Ø¨Ø±Ø§ÛŒ Big Data

```python
def optimize_dataframe_memory(df):
    """
    Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ DataFrame
    """
    print(f"Memory before: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            
            # Integer
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                    
            # Float
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
        
        # Categorical
        else:
            if df[col].nunique() / len(df) < 0.5:
                df[col] = df[col].astype('category')
    
    print(f"Memory after: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    return df

# Ø§Ø³ØªÙØ§Ø¯Ù‡
df_forex = optimize_dataframe_memory(df_forex)
```

### 8.2 GPU Acceleration Ø¨Ø±Ø§ÛŒ LightGBM

```python
def train_lgbm_gpu(X_train, y_train, X_test, y_test):
    """
    Ø¢Ù…ÙˆØ²Ø´ LightGBM Ø¨Ø§ GPU
    """
    params = {
        'objective': 'multiclass',
        'num_class': 3,
        'device': 'gpu',  # ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ GPU
        'gpu_platform_id': 0,
        'gpu_device_id': 0,
        'num_leaves': 63,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1
    }
    
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test)
    
    # Ø¢Ù…ÙˆØ²Ø´
    import time
    start_time = time.time()
    
    model = lgb.train(
        params,
        train_data,
        num_boost_round=500,
        valid_sets=[valid_data],
        callbacks=[lgb.early_stopping(50)]
    )
    
    training_time = time.time() - start_time
    print(f"GPU Training time: {training_time:.2f}s")
    
    return model

# ØªÙˆØ¬Ù‡: Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù†ØµØ¨ LightGBM Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ GPU
```

### 8.3 Feature Store

```python
class FeatureStore:
    """
    Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª ÙÛŒÚ†Ø±Ù‡Ø§
    """
    def __init__(self, cache_dir='feature_cache/'):
        self.cache_dir = cache_dir
        import os
        os.makedirs(cache_dir, exist_ok=True)
    
    def save_features(self, df, name):
        """Ø°Ø®ÛŒØ±Ù‡ ÙÛŒÚ†Ø±Ù‡Ø§"""
        filepath = f"{self.cache_dir}{name}.parquet"
        df.to_parquet(filepath, engine='pyarrow', compression='snappy')
        print(f"Features saved: {filepath}")
    
    def load_features(self, name):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§"""
        filepath = f"{self.cache_dir}{name}.parquet"
        df = pd.read_parquet(filepath, engine='pyarrow')
        print(f"Features loaded: {filepath}")
        return df
    
    def update_features(self, df_new, name):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§"""
        try:
            df_old = self.load_features(name)
            df_combined = pd.concat([df_old, df_new]).drop_duplicates()
            self.save_features(df_combined, name)
        except FileNotFoundError:
            self.save_features(df_new, name)

# Ø§Ø³ØªÙØ§Ø¯Ù‡
feature_store = FeatureStore()
feature_store.save_features(df_forex, 'EURUSD_1H_features')
```

---

## Ø¨Ø®Ø´ 9: Ø®Ù„Ø§ØµÙ‡ Ùˆ Ú†Ú©â€ŒÙ„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ

### âœ… Ú†Ú©â€ŒÙ„ÛŒØ³Øª Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡

#### **Ù…Ø±Ø­Ù„Ù‡ 1: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ**
- [ ] Ù†ØµØ¨ ØªÙ…Ø§Ù… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ 2025
- [ ] ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Copy-on-Write Ø¯Ø± Pandas
- [ ] ØªÙ†Ø¸ÛŒÙ… PyArrow Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
- [ ] Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø­ÛŒØ· ØªÙˆØ³Ø¹Ù‡ (Jupyter / VS Code)

#### **Ù…Ø±Ø­Ù„Ù‡ 2: Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡**
- [ ] Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ ÙØ§Ø±Ú©Ø³ (Ø­Ø¯Ø§Ù‚Ù„ 2-3 Ø³Ø§Ù„)
- [ ] Ù…Ø¯ÛŒØ±ÛŒØª Timezone Ùˆ Ø³Ø§Ø¹Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ
- [ ] Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Missing Data
- [ ] Resampling Ø¨Ù‡ timeframe Ø¯Ù„Ø®ÙˆØ§Ù‡

#### **Ù…Ø±Ø­Ù„Ù‡ 3: Feature Engineering**
- [ ] Ù…Ø­Ø§Ø³Ø¨Ù‡ Technical Indicators (RSI, MACD, BB, ATR)
- [ ] Ø³Ø§Ø®Øª Rolling & Expanding Features
- [ ] Ø§ÛŒØ¬Ø§Ø¯ Lag Features (1-20 Ø¯ÙˆØ±Ù‡)
- [ ] Multi-Timeframe Features
- [ ] Custom Indicators
- [ ] Target Variable (Classification / Regression)

#### **Ù…Ø±Ø­Ù„Ù‡ 4: Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ**
- [ ] ØªÙ‚Ø³ÛŒÙ… Train/Test Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
- [ ] Ø³Ø§Ø®Øª Pipeline Ø¨Ø§ ColumnTransformer
- [ ] Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LightGBM Ù¾Ø§ÛŒÙ‡
- [ ] Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Hyperparameter Ø¨Ø§ Optuna (50+ trials)
- [ ] Feature Importance Ø¨Ø§ SHAP
- [ ] Walk-Forward Optimization

#### **Ù…Ø±Ø­Ù„Ù‡ 5: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ**
- [ ] Backtesting Ø¨Ø§ Ø³Ø±Ù…Ø§ÛŒÙ‡ ÙˆØ§Ù‚Ø¹ÛŒ
- [ ] Ù…Ø­Ø§Ø³Ø¨Ù‡ Metrics (Sharpe, Max DD, Win Rate)
- [ ] Risk Management (Stop Loss, Take Profit, Position Sizing)
- [ ] Ensemble Models
- [ ] Performance Analysis

#### **Ù…Ø±Ø­Ù„Ù‡ 6: Ø¯ÛŒÙ¾Ù„ÙˆÛŒÙ…Ù†Øª**
- [ ] Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ùˆ Pipeline
- [ ] Ø³Ø§Ø®Øª Ú©Ù„Ø§Ø³ Trading Bot
- [ ] Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Real-Time Prediction
- [ ] ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ API Ø¨Ø±ÙˆÚ©Ø±
- [ ] Logging Ùˆ Monitoring
- [ ] ØªØ³Øª Ø¯Ø± Ø­Ø³Ø§Ø¨ Demo

#### **Ù…Ø±Ø­Ù„Ù‡ 7: Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ**
- [ ] Re-training Ù…Ø¯Ù„ Ù‡Ø± 1-3 Ù…Ø§Ù‡
- [ ] Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Features
- [ ] Ø¨Ø±Ø±Ø³ÛŒ Performance
- [ ] Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ Optuna

---

## Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯

### ğŸ“š Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø±Ø³Ù…ÛŒ
- **Pandas**: https://pandas.pydata.org/docs/
- **LightGBM**: https://lightgbm.readthedocs.io/
- **Scikit-learn**: https://scikit-learn.org/stable/
- **Optuna**: https://optuna.readthedocs.io/
- **SHAP**: https://shap.readthedocs.io/

### ğŸ”— GitHub Repositories
- Pandas Source: https://github.com/pandas-dev/pandas
- LightGBM Examples: https://github.com/microsoft/LightGBM/tree/master/examples
- Trading Strategies: https://github.com/topics/algorithmic-trading

### ğŸ“– Ú©ØªØ§Ø¨â€ŒÙ‡Ø§
- "Advances in Financial Machine Learning" - Marcos LÃ³pez de Prado
- "Python for Finance" - Yves Hilpisch  
- "Machine Learning for Algorithmic Trading" - Stefan Jansen

### ğŸ“ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§
- QuantConnect - https://www.quantconnect.com/
- QuantInsti - https://www.quantinsti.com/
- DataCamp: Machine Learning for Finance

---

## Ù†Ú©Ø§Øª Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ âš ï¸

### âš ï¸ **Ø§Ø®Ø·Ø§Ø±Ù‡Ø§ÛŒ Ù…Ù‡Ù…**

1. **Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ ÙÙ‚Ø· Ø¬Ù†Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø§Ø±Ø¯**
   - Ù‡Ø±Ú¯ÙˆÙ†Ù‡ ØªØ±ÛŒØ¯ÛŒÙ†Ú¯ ÙˆØ§Ù‚Ø¹ÛŒ Ø±ÛŒØ³Ú© Ø¯Ø§Ø±Ø¯
   - Ø§Ø¨ØªØ¯Ø§ Ø¯Ø± Ø­Ø³Ø§Ø¨ Demo ØªØ³Øª Ú©Ù†ÛŒØ¯
   - Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ú©Ù‡ Ø­Ø§Ø¶Ø± Ø¨Ù‡ Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù†ÛŒØ³ØªÛŒØ¯ Ø±ÛŒØ³Ú© Ù†Ú©Ù†ÛŒØ¯

2. **Ù‡ÛŒÚ† Ù…Ø¯Ù„ ML Ø¨Ø¯ÙˆÙ† Ø±ÛŒØ³Ú© Ù†ÛŒØ³Øª**
   - Performance Ú¯Ø°Ø´ØªÙ‡ ØªØ¶Ù…ÛŒÙ† Ø¢ÛŒÙ†Ø¯Ù‡ Ù†ÛŒØ³Øª
   - Ù‡Ù…ÛŒØ´Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
   - Ø§Ø² Leverage Ø¨Ø§Ù„Ø§ Ø§Ø¬ØªÙ†Ø§Ø¨ Ú©Ù†ÛŒØ¯

3. **Overfitting Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ø®Ø·Ø± Ø§Ø³Øª**
   - Ø§Ø² Walk-Forward Optimization Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
   - Ù…Ø¯Ù„ Ø±Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Out-of-Sample ØªØ³Øª Ú©Ù†ÛŒØ¯
   - ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±Ù‡Ø§ Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†ÛŒØ¯

4. **Ø¨Ø§Ø²Ø§Ø± Ø¯Ø§Ø¦Ù…Ø§ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯**
   - Ù…Ø¯Ù„ Ø±Ø§ Ù…Ø±ØªØ¨Ø§ Re-train Ú©Ù†ÛŒØ¯
   - Performance Ø±Ø§ Monitor Ú©Ù†ÛŒØ¯
   - Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ‚Ù Ø±Ø¨Ø§Øª Ø¨Ø§Ø´ÛŒØ¯

### ğŸ’¡ **ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ**

1. **Ø´Ø±ÙˆØ¹ Ú©ÙˆÚ†Ú©**: Ø¨Ø§ ÛŒÚ© pair Ùˆ ÛŒÚ© timeframe Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯
2. **Logging Ú©Ø§Ù…Ù„**: ØªÙ…Ø§Ù… ØªØµÙ…ÛŒÙ…Ø§Øª Ùˆ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø±Ø§ Log Ú©Ù†ÛŒØ¯
3. **Diversification**: Ø±ÙˆÛŒ Ú†Ù†Ø¯ pair Ù…Ø®ØªÙ„Ù Ú©Ø§Ø± Ú©Ù†ÛŒØ¯
4. **Automation**: Ù‡Ø±Ú†Ù‡ Ø¨ÛŒØ´ØªØ± Ø®ÙˆØ¯Ú©Ø§Ø± Ú©Ù†ÛŒØ¯ ØªØ§ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø®ÛŒÙ„ Ù†Ø´ÙˆØ¯
5. **Continuous Learning**: Ù‡Ù…ÛŒØ´Ù‡ Ø¯Ø± Ø­Ø§Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§Ø´ÛŒØ¯

---

## Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ

Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ Ø¬Ø§Ù…Ø¹â€ŒØªØ±ÛŒÙ† Ù…Ù†Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø±Ø¨Ø§Øª ØªØ±ÛŒØ¯Ø± ÙØ§Ø±Ú©Ø³ Ø¨Ø§ Pandas Ùˆ LightGBM Ø§Ø³Øª. Ø¨Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯:

âœ… ÛŒÚ© Ø³ÛŒØ³ØªÙ… ØªØ±ÛŒØ¯ÛŒÙ†Ú¯ Ú©Ø§Ù…Ù„Ø§ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø³Ø§Ø²ÛŒØ¯  
âœ… Ø§Ø² Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ML Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯  
âœ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯ Ùˆ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯  
âœ… Ø±ÛŒØ³Ú© Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯  

**Ù…ÙˆÙÙ‚ Ùˆ Ù¾Ø±Ø³ÙˆØ¯ Ø¨Ø§Ø´ÛŒØ¯! ğŸš€ğŸ“ˆ**

---

**Ù†Ø³Ø®Ù‡**: 2.0  
**ØªØ§Ø±ÛŒØ®**: Ø§Ú©ØªØ¨Ø± 2025  
**Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§**: Pandas 2.2+, LightGBM 4.5+, Python 3.10+

---

*ØªÙˆØ¬Ù‡: Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ Ø¨Ø§ ØªØ­Ù‚ÛŒÙ‚ Ú¯Ø³ØªØ±Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ø´Ø§Ù…Ù„ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø±Ø³Ù…ÛŒ PandasØŒ LightGBMØŒ Scikit-learn Ùˆ Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ù„Ù…ÛŒ Ø³Ø§Ù„ 2025 ØªÙ‡ÛŒÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.*